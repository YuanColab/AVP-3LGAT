{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c931b0f5",
   "metadata": {},
   "source": [
    "# 1.定义函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6c1516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Step 1: Detailed Dataset Processing and Statistics\n",
      "================================================================================\n",
      "🚀 STEP 1: DETAILED PROCESSING OF INTERNAL AND EXTERNAL DATASETS\n",
      "================================================================================\n",
      "\n",
      "📊 PROCESSING INTERNAL AVP DATASETS\n",
      "================================================================================\n",
      "   🔹 Processing individual internal AVP datasets:\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing: DRAVP\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 1,986\n",
      "            Step 1 - After natural AA filter: 1,756 (removed: 230)\n",
      "            Step 2 - After length filter (5-50): 1,714\n",
      "                   ├─ Too short (<5): 17\n",
      "                   └─ Too long (>50): 25\n",
      "            Step 3 - After deduplication: 1,608 (removed duplicates: 106)\n",
      "            ✅ Verification: Expected 1,608, Got 1,608\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "      ✅ DRAVP completed: 1,986 → 1,608 sequences\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing: AVPdb\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 2,059\n",
      "            Step 1 - After natural AA filter: 2,058 (removed: 1)\n",
      "            Step 2 - After length filter (5-50): 2,024\n",
      "                   ├─ Too short (<5): 15\n",
      "                   └─ Too long (>50): 19\n",
      "            Step 3 - After deduplication: 1,784 (removed duplicates: 240)\n",
      "            ✅ Verification: Expected 1,784, Got 1,784\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "      ✅ AVPdb completed: 2,059 → 1,784 sequences\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing: ACovPepDB\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 518\n",
      "            Step 1 - After natural AA filter: 381 (removed: 137)\n",
      "            Step 2 - After length filter (5-50): 350\n",
      "                   ├─ Too short (<5): 0\n",
      "                   └─ Too long (>50): 31\n",
      "            Step 3 - After deduplication: 149 (removed duplicates: 201)\n",
      "            ✅ Verification: Expected 149, Got 149\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "      ✅ ACovPepDB completed: 518 → 149 sequences\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing: HIPdb\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 981\n",
      "            Step 1 - After natural AA filter: 979 (removed: 2)\n",
      "            Step 2 - After length filter (5-50): 950\n",
      "                   ├─ Too short (<5): 23\n",
      "                   └─ Too long (>50): 6\n",
      "            Step 3 - After deduplication: 858 (removed duplicates: 92)\n",
      "            ✅ Verification: Expected 858, Got 858\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "      ✅ HIPdb completed: 981 → 858 sequences\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing: CAMP\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 1,087\n",
      "            Step 1 - After natural AA filter: 951 (removed: 136)\n",
      "            Step 2 - After length filter (5-50): 905\n",
      "                   ├─ Too short (<5): 16\n",
      "                   └─ Too long (>50): 30\n",
      "            Step 3 - After deduplication: 898 (removed duplicates: 7)\n",
      "            ✅ Verification: Expected 898, Got 898\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "      ✅ CAMP completed: 1,087 → 898 sequences\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing: DBAASP\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 1,488\n",
      "            Step 1 - After natural AA filter: 1,263 (removed: 225)\n",
      "            Step 2 - After length filter (5-50): 1,220\n",
      "                   ├─ Too short (<5): 19\n",
      "                   └─ Too long (>50): 24\n",
      "            Step 3 - After deduplication: 1,142 (removed duplicates: 78)\n",
      "            ✅ Verification: Expected 1,142, Got 1,142\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "      ✅ DBAASP completed: 1,488 → 1,142 sequences\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing: DRAMP\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 2,004\n",
      "            Step 1 - After natural AA filter: 1,771 (removed: 233)\n",
      "            Step 2 - After length filter (5-50): 1,672\n",
      "                   ├─ Too short (<5): 17\n",
      "                   └─ Too long (>50): 82\n",
      "            Step 3 - After deduplication: 1,575 (removed duplicates: 97)\n",
      "            ✅ Verification: Expected 1,575, Got 1,575\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "      ✅ DRAMP completed: 2,004 → 1,575 sequences\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing: dbAMP_AntiHIV\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         📄 Parsing FASTA file: dbAMP_AntiHIV_2024.fasta\n",
      "            ✓ 1024 records, length range: 2-137\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 1,024\n",
      "            Step 1 - After natural AA filter: 1,024 (removed: 0)\n",
      "            Step 2 - After length filter (5-50): 972\n",
      "                   ├─ Too short (<5): 23\n",
      "                   └─ Too long (>50): 29\n",
      "            Step 3 - After deduplication: 970 (removed duplicates: 2)\n",
      "            ✅ Verification: Expected 970, Got 970\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "      ✅ dbAMP_AntiHIV completed: 1,024 → 970 sequences\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing: dbAMP_Antiviral\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         📄 Parsing FASTA file: dbAMP_Antiviral_2024.fasta\n",
      "            ✓ 2018 records, length range: 2-586\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 2,018\n",
      "            Step 1 - After natural AA filter: 1,891 (removed: 127)\n",
      "            Step 2 - After length filter (5-50): 1,691\n",
      "                   ├─ Too short (<5): 27\n",
      "                   └─ Too long (>50): 173\n",
      "            Step 3 - After deduplication: 1,691 (removed duplicates: 0)\n",
      "            ✅ Verification: Expected 1,691, Got 1,691\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "      ✅ dbAMP_Antiviral completed: 2,018 → 1,691 sequences\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing: Peptipedia\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         📄 Parsing FASTA file: Peptipedia_Antiviral.fasta\n",
      "            ✓ 5899 records, length range: 2-363\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 5,899\n",
      "            Step 1 - After natural AA filter: 5,177 (removed: 722)\n",
      "            Step 2 - After length filter (5-50): 4,856\n",
      "                   ├─ Too short (<5): 85\n",
      "                   └─ Too long (>50): 236\n",
      "            Step 3 - After deduplication: 4,856 (removed duplicates: 0)\n",
      "            ✅ Verification: Expected 4,856, Got 4,856\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "      ✅ Peptipedia completed: 5,899 → 4,856 sequences\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "   ──────────────────────────────────────────────────────────────────────\n",
      "   🔗 Merging internal AVP datasets\n",
      "   ──────────────────────────────────────────────────────────────────────\n",
      "      📈 Total sequences before merge: 15,531\n",
      "         └─ DRAVP: 1,608 sequences\n",
      "         └─ AVPdb: 1,784 sequences\n",
      "         └─ ACovPepDB: 149 sequences\n",
      "         └─ HIPdb: 858 sequences\n",
      "         └─ CAMP: 898 sequences\n",
      "         └─ DBAASP: 1,142 sequences\n",
      "         └─ DRAMP: 1,575 sequences\n",
      "         └─ dbAMP_AntiHIV: 970 sequences\n",
      "         └─ dbAMP_Antiviral: 1,691 sequences\n",
      "         └─ Peptipedia: 4,856 sequences\n",
      "      📉 After final deduplication: 4,993 sequences\n",
      "      🗑️ Final duplicates removed: 10,538\n",
      "   ──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "📊 PROCESSING INTERNAL non_AVP DATASETS\n",
      "================================================================================\n",
      "   🔹 Processing UniProt non_AVP dataset:\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing: UniProt\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         📄 Parsing FASTA file: uniprotkb_13124.fasta\n",
      "            ✓ 13124 records, length range: 5-50\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 13,124\n",
      "            Step 1 - After natural AA filter: 12,513 (removed: 611)\n",
      "            Step 2 - After length filter (5-50): 12,513\n",
      "                   ├─ Too short (<5): 0\n",
      "                   └─ Too long (>50): 0\n",
      "            Step 3 - After deduplication: 9,402 (removed duplicates: 3,111)\n",
      "            ✅ Verification: Expected 9,402, Got 9,402\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "      ✅ UniProt completed: 13,124 → 9,402 sequences\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📝 ID format examples:\n",
      "         sp|A0A068B6Q6|CA18_CONBE Conotoxin Bt1.8 (Fragment) OS=Conus betulinus OX=89764 PE=1 SV=1 → sp|A0A068B6Q6|\n",
      "         sp|A0A0C5B5G6|MOTSC_HUMAN Mitochondrial-derived peptide MOTS-c OS=Homo sapiens OX=9606 GN=MT-RNR1 PE=1 SV=1 → sp|A0A0C5B5G6|\n",
      "         sp|A0A2R8VHR8|DT3UO_MOUSE DDIT3 upstream open reading frame protein OS=Mus musculus OX=10090 GN=Ddit3 PE=2 SV=1 → sp|A0A2R8VHR8|\n",
      "         sp|A0A4V8GZX0|TXNA1_OMOSC Mu-theraphotoxin-Os1a OS=Omothymus schioedtei OX=1046902 PE=1 SV=2 → sp|A0A4V8GZX0|\n",
      "         sp|A5A616|MGTS_ECOLI Small protein MgtS OS=Escherichia coli (strain K12) OX=83333 GN=mgtS PE=1 SV=1 → sp|A5A616|\n",
      "      ✅ Processed 9402 UniProt IDs to short format\n",
      "\n",
      "📊 PROCESSING EXTERNAL DATASETS\n",
      "================================================================================\n",
      "   🔹 Processing external AVP datasets:\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing: Stack-AVP-TR_pos\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         📄 Parsing FASTA file: Stack-AVP-TR_pos.fasta\n",
      "            ✓ 2290 records, length range: 5-50\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 2,290\n",
      "            Step 1 - After natural AA filter: 2,290 (removed: 0)\n",
      "            Step 2 - After length filter (5-50): 2,290\n",
      "                   ├─ Too short (<5): 0\n",
      "                   └─ Too long (>50): 0\n",
      "            Step 3 - After deduplication: 2,290 (removed duplicates: 0)\n",
      "            ✅ Verification: Expected 2,290, Got 2,290\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "      ✅ Stack-AVP-TR_pos completed: 2,290 → 2,290 sequences\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "   🔹 Processing external non_AVP datasets:\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing: Stack-AVP-TR_neg\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         📄 Parsing FASTA file: Stack-AVP-TR_neg.fasta\n",
      "            ✓ 2311 records, length range: 6-50\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 2,311\n",
      "            Step 1 - After natural AA filter: 2,311 (removed: 0)\n",
      "            Step 2 - After length filter (5-50): 2,311\n",
      "                   ├─ Too short (<5): 0\n",
      "                   └─ Too long (>50): 0\n",
      "            Step 3 - After deduplication: 2,311 (removed duplicates: 0)\n",
      "            ✅ Verification: Expected 2,311, Got 2,311\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "      ✅ Stack-AVP-TR_neg completed: 2,311 → 2,311 sequences\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "   🔹 Processing mixed-label datasets:\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing mixed-label dataset: AVP-HNCL_non-AMP\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         📊 Original data: 4258 total sequences\n",
      "            ├─ AVP (positive): 2129\n",
      "            └─ non_AVP (negative): 2129\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 4,258\n",
      "            Step 1 - After natural AA filter: 4,258 (removed: 0)\n",
      "            Step 2 - After length filter (5-50): 2,232\n",
      "                   ├─ Too short (<5): 0\n",
      "                   └─ Too long (>50): 2,026\n",
      "            Step 3 - After deduplication: 2,232 (removed duplicates: 0)\n",
      "            ✅ Verification: Expected 2,232, Got 2,232\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         📊 After all filtering: 2232 total sequences\n",
      "            ├─ AVP: 2088 (lost: 41)\n",
      "            └─ non_AVP: 144 (lost: 1985)\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "      📊 Processing mixed-label dataset: AVP-HNCL_non-AVP\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "         📊 Original data: 4258 total sequences\n",
      "            ├─ AVP (positive): 2129\n",
      "            └─ non_AVP (negative): 2129\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         🔍 Detailed filtering process:\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "            Step 0 - Original sequences: 4,258\n",
      "            Step 1 - After natural AA filter: 4,258 (removed: 0)\n",
      "            Step 2 - After length filter (5-50): 3,692\n",
      "                   ├─ Too short (<5): 0\n",
      "                   └─ Too long (>50): 566\n",
      "            Step 3 - After deduplication: 3,692 (removed duplicates: 0)\n",
      "            ✅ Verification: Expected 3,692, Got 3,692\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄\n",
      "         📊 After all filtering: 3692 total sequences\n",
      "            ├─ AVP: 2088 (lost: 41)\n",
      "            └─ non_AVP: 1604 (lost: 525)\n",
      "      ──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "   ──────────────────────────────────────────────────────────────────────\n",
      "   🔗 Merging external AVP datasets\n",
      "   ──────────────────────────────────────────────────────────────────────\n",
      "      📈 Total sequences before merge: 6,466\n",
      "         └─ Stack-AVP-TR_pos: 2,290 sequences\n",
      "         └─ AVP-HNCL_non-AMP: 2,088 sequences\n",
      "         └─ AVP-HNCL_non-AVP: 2,088 sequences\n",
      "      📉 After final deduplication: 3,412 sequences\n",
      "      🗑️ Final duplicates removed: 3,054\n",
      "   ──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "   ──────────────────────────────────────────────────────────────────────\n",
      "   🔗 Merging external non_AVP datasets\n",
      "   ──────────────────────────────────────────────────────────────────────\n",
      "      📈 Total sequences before merge: 4,059\n",
      "         └─ Stack-AVP-TR_neg: 2,311 sequences\n",
      "         └─ AVP-HNCL_non-AMP: 144 sequences\n",
      "         └─ AVP-HNCL_non-AVP: 1,604 sequences\n",
      "      📉 After final deduplication: 4,049 sequences\n",
      "      🗑️ Final duplicates removed: 10\n",
      "   ──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "💾 SAVING PROCESSED DATASETS:\n",
      "================================================================================\n",
      "   ✅ External Avp: 3,412 sequences\n",
      "      └─ Saved to: 1_Data/Processed_data_set/Initial_merged_data_set/Initial_TR_AVP.csv\n",
      "      └─ Columns: ['Id', 'Sequence', 'Source', 'Length', 'Label', 'Type']\n",
      "   ✅ External Non Avp: 4,049 sequences\n",
      "      └─ Saved to: 1_Data/Processed_data_set/Initial_merged_data_set/Initial_TR_non_AVP.csv\n",
      "      └─ Columns: ['Id', 'Sequence', 'Source', 'Length', 'Label', 'Type']\n",
      "   ✅ Internal Avp: 4,993 sequences\n",
      "      └─ Saved to: 1_Data/Processed_data_set/Initial_merged_data_set/Initial_TS_AVP.csv\n",
      "      └─ Columns: ['Id', 'Sequence', 'Source', 'Length', 'Label', 'Type']\n",
      "   ✅ Internal Non Avp: 9,402 sequences\n",
      "      └─ Saved to: 1_Data/Processed_data_set/Initial_merged_data_set/Initial_TS_non_AVP.csv\n",
      "      └─ Columns: ['Id', 'Sequence', 'Source', 'Length', 'Label', 'Type']\n",
      "\n",
      "   📄 Processing log saved to: 2_Log/2.1_Training set_and_test_set_processing/step1_dataset_processing_log.json\n",
      "\n",
      "   📁 All files saved to: 1_Data/Processed_data_set/Initial_merged_data_set\n",
      "   📊 Total files saved: 4\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "📋 COMPREHENSIVE STATISTICS SUMMARY\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "🔹 INTERNAL DATASETS DETAILED BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "   📊 Individual AVP dataset filtering details:\n",
      "   ┌─────────────────────────────────────────────────────────────────┐\n",
      "   │  DRAVP                                                          │\n",
      "   ├─────────────────────────────────────────────────────────────────┤\n",
      "   │  Original:    1,986 sequences                                      │\n",
      "   │  Non-natural AA removed:      230                                    │\n",
      "   │  Too short removed:       17                                          │\n",
      "   │  Too long removed:       25                                           │\n",
      "   │  Duplicates removed:      106                                        │\n",
      "   │  Final:    1,608 sequences                                         │\n",
      "   └─────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "   ┌─────────────────────────────────────────────────────────────────┐\n",
      "   │  AVPdb                                                          │\n",
      "   ├─────────────────────────────────────────────────────────────────┤\n",
      "   │  Original:    2,059 sequences                                      │\n",
      "   │  Non-natural AA removed:        1                                      │\n",
      "   │  Too short removed:       15                                          │\n",
      "   │  Too long removed:       19                                           │\n",
      "   │  Duplicates removed:      240                                        │\n",
      "   │  Final:    1,784 sequences                                         │\n",
      "   └─────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "   ┌─────────────────────────────────────────────────────────────────┐\n",
      "   │  ACovPepDB                                                      │\n",
      "   ├─────────────────────────────────────────────────────────────────┤\n",
      "   │  Original:      518 sequences                                        │\n",
      "   │  Non-natural AA removed:      137                                    │\n",
      "   │  Too short removed:        0                                           │\n",
      "   │  Too long removed:       31                                           │\n",
      "   │  Duplicates removed:      201                                        │\n",
      "   │  Final:      149 sequences                                           │\n",
      "   └─────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "   ┌─────────────────────────────────────────────────────────────────┐\n",
      "   │  HIPdb                                                          │\n",
      "   ├─────────────────────────────────────────────────────────────────┤\n",
      "   │  Original:      981 sequences                                        │\n",
      "   │  Non-natural AA removed:        2                                      │\n",
      "   │  Too short removed:       23                                          │\n",
      "   │  Too long removed:        6                                            │\n",
      "   │  Duplicates removed:       92                                         │\n",
      "   │  Final:      858 sequences                                           │\n",
      "   └─────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "   ┌─────────────────────────────────────────────────────────────────┐\n",
      "   │  CAMP                                                           │\n",
      "   ├─────────────────────────────────────────────────────────────────┤\n",
      "   │  Original:    1,087 sequences                                      │\n",
      "   │  Non-natural AA removed:      136                                    │\n",
      "   │  Too short removed:       16                                          │\n",
      "   │  Too long removed:       30                                           │\n",
      "   │  Duplicates removed:        7                                          │\n",
      "   │  Final:      898 sequences                                           │\n",
      "   └─────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "   ┌─────────────────────────────────────────────────────────────────┐\n",
      "   │  DBAASP                                                         │\n",
      "   ├─────────────────────────────────────────────────────────────────┤\n",
      "   │  Original:    1,488 sequences                                      │\n",
      "   │  Non-natural AA removed:      225                                    │\n",
      "   │  Too short removed:       19                                          │\n",
      "   │  Too long removed:       24                                           │\n",
      "   │  Duplicates removed:       78                                         │\n",
      "   │  Final:    1,142 sequences                                         │\n",
      "   └─────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "   ┌─────────────────────────────────────────────────────────────────┐\n",
      "   │  DRAMP                                                          │\n",
      "   ├─────────────────────────────────────────────────────────────────┤\n",
      "   │  Original:    2,004 sequences                                      │\n",
      "   │  Non-natural AA removed:      233                                    │\n",
      "   │  Too short removed:       17                                          │\n",
      "   │  Too long removed:       82                                           │\n",
      "   │  Duplicates removed:       97                                         │\n",
      "   │  Final:    1,575 sequences                                         │\n",
      "   └─────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "   ┌─────────────────────────────────────────────────────────────────┐\n",
      "   │  dbAMP_AntiHIV                                                  │\n",
      "   ├─────────────────────────────────────────────────────────────────┤\n",
      "   │  Original:    1,024 sequences                                      │\n",
      "   │  Non-natural AA removed:        0                                      │\n",
      "   │  Too short removed:       23                                          │\n",
      "   │  Too long removed:       29                                           │\n",
      "   │  Duplicates removed:        2                                          │\n",
      "   │  Final:      970 sequences                                           │\n",
      "   └─────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "   ┌─────────────────────────────────────────────────────────────────┐\n",
      "   │  dbAMP_Antiviral                                                │\n",
      "   ├─────────────────────────────────────────────────────────────────┤\n",
      "   │  Original:    2,018 sequences                                      │\n",
      "   │  Non-natural AA removed:      127                                    │\n",
      "   │  Too short removed:       27                                          │\n",
      "   │  Too long removed:      173                                          │\n",
      "   │  Duplicates removed:        0                                          │\n",
      "   │  Final:    1,691 sequences                                         │\n",
      "   └─────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "   ┌─────────────────────────────────────────────────────────────────┐\n",
      "   │  Peptipedia                                                     │\n",
      "   ├─────────────────────────────────────────────────────────────────┤\n",
      "   │  Original:    5,899 sequences                                      │\n",
      "   │  Non-natural AA removed:      722                                    │\n",
      "   │  Too short removed:       85                                          │\n",
      "   │  Too long removed:      236                                          │\n",
      "   │  Duplicates removed:        0                                          │\n",
      "   │  Final:    4,856 sequences                                         │\n",
      "   └─────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "   📊 Internal non_AVP (UniProt) filtering details:\n",
      "   ┌─────────────────────────────────────────────────────────────────┐\n",
      "   │  UniProt non_AVP                                                │\n",
      "   ├─────────────────────────────────────────────────────────────────┤\n",
      "   │  Original:   13,124 sequences                                     │\n",
      "   │  Non-natural AA removed:      611                                    │\n",
      "   │  Too short removed:        0                                           │\n",
      "   │  Too long removed:        0                                            │\n",
      "   │  Duplicates removed:    3,111                                      │\n",
      "   │  Final:    9,402 sequences                                         │\n",
      "   └─────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "🔹 EXTERNAL DATASETS DETAILED BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "   ┌───────────────────────────────────────────────────────────────────────────┐\n",
      "   │  📊 Stack-AVP-TR_pos                                                       │\n",
      "   ├───────────────────────────────────────────────────────────────────────────┤\n",
      "   │  Original: 2,290 sequences (AVP)                                            │\n",
      "   │  Non-natural AA removed:        0                                                │\n",
      "   │  Too short removed:        0                                                     │\n",
      "   │  Too long removed:        0                                                      │\n",
      "   │  Duplicates removed:        0                                                    │\n",
      "   │  Final:    2,290 sequences                                                   │\n",
      "   └───────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "   ┌───────────────────────────────────────────────────────────────────────────┐\n",
      "   │  📊 Stack-AVP-TR_neg                                                       │\n",
      "   ├───────────────────────────────────────────────────────────────────────────┤\n",
      "   │  Original: 2,311 sequences (non_AVP)                                        │\n",
      "   │  Non-natural AA removed:        0                                                │\n",
      "   │  Too short removed:        0                                                     │\n",
      "   │  Too long removed:        0                                                      │\n",
      "   │  Duplicates removed:        0                                                    │\n",
      "   │  Final:    2,311 sequences                                                   │\n",
      "   └───────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "   ┌───────────────────────────────────────────────────────────────────────────┐\n",
      "   │  📊 AVP-HNCL_non-AMP                                                       │\n",
      "   ├───────────────────────────────────────────────────────────────────────────┤\n",
      "   │  Original: 4,258 (AVP: 2129, non_AVP: 2129)                                 │\n",
      "   │  Non-natural AA removed:        0                                                │\n",
      "   │  Too short removed:        0                                                     │\n",
      "   │  Too long removed:    2,026                                                  │\n",
      "   │  Duplicates removed:        0                                                    │\n",
      "   │  Final: 2,232 (AVP: 2088, non_AVP: 144)                                     │\n",
      "   └───────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "   ┌───────────────────────────────────────────────────────────────────────────┐\n",
      "   │  📊 AVP-HNCL_non-AVP                                                       │\n",
      "   ├───────────────────────────────────────────────────────────────────────────┤\n",
      "   │  Original: 4,258 (AVP: 2129, non_AVP: 2129)                                 │\n",
      "   │  Non-natural AA removed:        0                                                │\n",
      "   │  Too short removed:        0                                                     │\n",
      "   │  Too long removed:      566                                                    │\n",
      "   │  Duplicates removed:        0                                                    │\n",
      "   │  Final: 3,692 (AVP: 2088, non_AVP: 1604)                                    │\n",
      "   └───────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "🔹 FINAL DATASET SUMMARY:\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "   📈 External AVP: 3,412 sequences (Initial_TR_AVP.csv)\n",
      "   📈 External non_AVP: 4,049 sequences (Initial_TR_non_AVP.csv)\n",
      "   📈 Internal AVP: 4,993 sequences (Initial_TS_AVP.csv)\n",
      "   📈 Internal non_AVP: 9,402 sequences (Initial_TS_non_AVP.csv)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "   📊 Grand Total: 21,856 sequences\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "📄 Complete Step 1 summary saved to: 2_Log/2.1_Training set_and_test_set_processing/step1_complete_processing_summary.json\n",
      "\n",
      "✅ Step 1 completed successfully with detailed statistics!\n",
      "   📁 Results stored in step1_detailed_results variable\n",
      "   💾 CSV files saved in: 1_Data/Processed_data_set/Initial_merged_data_set/\n",
      "   📊 Ready for Step 2: Overlap analysis\n",
      "\n",
      "🎯 Step 1 Complete - Detailed filtering statistics generated for all datasets!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 基础配置\n",
    "# ============================================================================\n",
    "MIN_SEQUENCE_LENGTH = 5\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "# ============================================================================\n",
    "# 通用工具函数\n",
    "# ============================================================================\n",
    "def filter_peptides_detailed(peptides_data, sequence_col='Sequence', min_length=MIN_SEQUENCE_LENGTH, max_length=MAX_SEQUENCE_LENGTH):\n",
    "    \"\"\"详细的过滤步骤：筛选5-50的天然氨基酸长度包括大小写的不重复的氨基酸序列，返回每步详细统计\"\"\"\n",
    "    natural_aa = set('ACDEFGHIKLMNPQRSTVWYacdefghiklmnpqrstvwy')\n",
    "    \n",
    "    # 初始化统计\n",
    "    stats = {\n",
    "        'step0_original': len(peptides_data),\n",
    "        'step1_after_natural_aa': 0,\n",
    "        'step2_after_length': 0,\n",
    "        'step3_final': 0,\n",
    "        'removed_non_natural_aa': 0,\n",
    "        'removed_too_short': 0,\n",
    "        'removed_too_long': 0,\n",
    "        'removed_duplicates': 0\n",
    "    }\n",
    "    \n",
    "    print(f\"         {'┄' * 60}\")\n",
    "    print(f\"         🔍 Detailed filtering process:\")\n",
    "    print(f\"         {'┄' * 60}\")\n",
    "    print(f\"            Step 0 - Original sequences: {stats['step0_original']:,}\")\n",
    "    \n",
    "    # 步骤1: 检查天然氨基酸\n",
    "    filtered_data = peptides_data.copy()\n",
    "    mask = filtered_data[sequence_col].apply(lambda seq: all(aa in natural_aa for aa in seq))\n",
    "    stats['removed_non_natural_aa'] = len(filtered_data) - mask.sum()\n",
    "    filtered_data = filtered_data[mask]\n",
    "    stats['step1_after_natural_aa'] = len(filtered_data)\n",
    "    print(f\"            Step 1 - After natural AA filter: {stats['step1_after_natural_aa']:,} (removed: {stats['removed_non_natural_aa']:,})\")\n",
    "    \n",
    "    # 步骤2: 过滤长度\n",
    "    filtered_data['temp_length'] = filtered_data[sequence_col].apply(len)\n",
    "    \n",
    "    # 先过滤太短的\n",
    "    too_short_mask = filtered_data['temp_length'] >= min_length\n",
    "    stats['removed_too_short'] = len(filtered_data) - too_short_mask.sum()\n",
    "    filtered_data = filtered_data[too_short_mask]\n",
    "    \n",
    "    # 再过滤太长的\n",
    "    too_long_mask = filtered_data['temp_length'] <= max_length\n",
    "    stats['removed_too_long'] = len(filtered_data) - too_long_mask.sum()\n",
    "    filtered_data = filtered_data[too_long_mask]\n",
    "    \n",
    "    stats['step2_after_length'] = len(filtered_data)\n",
    "    print(f\"            Step 2 - After length filter ({min_length}-{max_length}): {stats['step2_after_length']:,}\")\n",
    "    print(f\"                   ├─ Too short (<{min_length}): {stats['removed_too_short']:,}\")\n",
    "    print(f\"                   └─ Too long (>{max_length}): {stats['removed_too_long']:,}\")\n",
    "    \n",
    "    # 步骤3: 去除重复（不区分大小写）\n",
    "    filtered_data['temp_upper'] = filtered_data[sequence_col].str.upper()\n",
    "    before_dedup = len(filtered_data)\n",
    "    filtered_data = filtered_data.drop_duplicates(subset=['temp_upper'])\n",
    "    stats['removed_duplicates'] = before_dedup - len(filtered_data)\n",
    "    stats['step3_final'] = len(filtered_data)\n",
    "    print(f\"            Step 3 - After deduplication: {stats['step3_final']:,} (removed duplicates: {stats['removed_duplicates']:,})\")\n",
    "    \n",
    "    # 转换为大写并清理临时列\n",
    "    filtered_data[sequence_col] = filtered_data[sequence_col].str.upper()\n",
    "    filtered_data = filtered_data.drop(['temp_length', 'temp_upper'], axis=1, errors='ignore')\n",
    "    \n",
    "    # 验证总数\n",
    "    total_removed = stats['removed_non_natural_aa'] + stats['removed_too_short'] + stats['removed_too_long'] + stats['removed_duplicates']\n",
    "    expected_final = stats['step0_original'] - total_removed\n",
    "    print(f\"            ✅ Verification: Expected {expected_final:,}, Got {stats['step3_final']:,}\")\n",
    "    print(f\"         {'┄' * 60}\")\n",
    "    \n",
    "    return filtered_data, stats\n",
    "\n",
    "def parse_fasta_to_df(fasta_file, verbose=True):\n",
    "    \"\"\"将FASTA文件解析为pandas DataFrame\"\"\"\n",
    "    if verbose:\n",
    "        print(f\"         📄 Parsing FASTA file: {os.path.basename(fasta_file)}\")\n",
    "    \n",
    "    records = []\n",
    "    encodings_to_try = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1', 'ascii']\n",
    "    \n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            with open(fasta_file, 'r', encoding=encoding, errors='ignore') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            current_id = None\n",
    "            current_seq = []\n",
    "            \n",
    "            for line in content.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                    \n",
    "                if line.startswith('>'):\n",
    "                    if current_id and current_seq:\n",
    "                        records.append({'Id': current_id, 'Sequence': ''.join(current_seq)})\n",
    "                    current_id = line[1:]\n",
    "                    current_seq = []\n",
    "                else:\n",
    "                    current_seq.append(line)\n",
    "            \n",
    "            if current_id and current_seq:\n",
    "                records.append({'Id': current_id, 'Sequence': ''.join(current_seq)})\n",
    "            \n",
    "            if verbose and records:\n",
    "                lengths = [len(rec['Sequence']) for rec in records]\n",
    "                print(f\"            ✓ {len(records)} records, length range: {min(lengths)}-{max(lengths)}\")\n",
    "            break\n",
    "            \n",
    "        except (UnicodeDecodeError, UnicodeError):\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(records) if records else pd.DataFrame(columns=['Id', 'Sequence'])\n",
    "\n",
    "def process_dataset_detailed(file_path, sequence_col, id_col=None, source_name=\"\", file_type=\"csv\"):\n",
    "    \"\"\"详细的数据集处理函数\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"      ❌ File {file_path} does not exist\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        print(f\"      {'─' * 70}\")\n",
    "        print(f\"      📊 Processing: {source_name}\")\n",
    "        print(f\"      {'─' * 70}\")\n",
    "        \n",
    "        # 读取数据\n",
    "        if file_type == \"csv\":\n",
    "            data = pd.read_csv(file_path)\n",
    "        elif file_type == \"excel\":\n",
    "            data = pd.read_excel(file_path)\n",
    "        elif file_type == \"fasta\":\n",
    "            data = parse_fasta_to_df(file_path, verbose=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type\")\n",
    "        \n",
    "        if sequence_col not in data.columns:\n",
    "            print(f\"      ❌ Sequence column '{sequence_col}' not found\")\n",
    "            return None, None\n",
    "        \n",
    "        # 详细过滤数据\n",
    "        filtered_data, stats = filter_peptides_detailed(data, sequence_col=sequence_col)\n",
    "        \n",
    "        # 标准化列\n",
    "        if id_col and id_col in filtered_data.columns:\n",
    "            filtered_data = filtered_data[[id_col, sequence_col]].copy()\n",
    "            filtered_data = filtered_data.rename(columns={id_col: 'Id', sequence_col: 'Sequence'})\n",
    "        else:\n",
    "            filtered_data = filtered_data[[sequence_col]].copy()\n",
    "            filtered_data = filtered_data.rename(columns={sequence_col: 'Sequence'})\n",
    "            filtered_data['Id'] = [f\"{source_name}_{i+1}\" for i in range(len(filtered_data))]\n",
    "            filtered_data = filtered_data[['Id', 'Sequence']]\n",
    "        \n",
    "        filtered_data['Source'] = source_name\n",
    "        filtered_data['Length'] = filtered_data['Sequence'].apply(len)\n",
    "        \n",
    "        print(f\"      ✅ {source_name} completed: {stats['step0_original']:,} → {stats['step3_final']:,} sequences\")\n",
    "        print(f\"      {'─' * 70}\")\n",
    "        \n",
    "        return filtered_data, stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ❌ Error processing {source_name}: {str(e)}\")\n",
    "        print(f\"      {'─' * 70}\")\n",
    "        return None, None\n",
    "\n",
    "def merge_datasets_with_priority_detailed(datasets_list, priority_order, dataset_type=\"\"):\n",
    "    \"\"\"按优先级合并数据集并去重，显示详细统计\"\"\"\n",
    "    valid_datasets = [ds for ds in datasets_list if ds is not None]\n",
    "    \n",
    "    if not valid_datasets:\n",
    "        return pd.DataFrame(columns=['Id', 'Sequence', 'Source', 'Length'])\n",
    "    \n",
    "    print(f\"\\n   {'─' * 70}\")\n",
    "    print(f\"   🔗 Merging {dataset_type} datasets\")\n",
    "    print(f\"   {'─' * 70}\")\n",
    "    \n",
    "    # 计算合并前的统计\n",
    "    total_before = sum(len(ds) for ds in valid_datasets)\n",
    "    print(f\"      📈 Total sequences before merge: {total_before:,}\")\n",
    "    \n",
    "    # 显示各数据集贡献\n",
    "    for i, ds in enumerate(valid_datasets):\n",
    "        source = ds['Source'].iloc[0] if len(ds) > 0 else f\"Dataset_{i+1}\"\n",
    "        print(f\"         └─ {source}: {len(ds):,} sequences\")\n",
    "    \n",
    "    combined_df = pd.concat(valid_datasets, ignore_index=True)\n",
    "    combined_df['Length'] = combined_df['Sequence'].apply(len)\n",
    "    \n",
    "    # 按优先级排序\n",
    "    source_priority = {source: i+1 for i, source in enumerate(priority_order)}\n",
    "    combined_df['Priority'] = combined_df['Source'].map(source_priority)\n",
    "    combined_df = combined_df.sort_values('Priority')\n",
    "    \n",
    "    # 去重\n",
    "    before_final_dedup = len(combined_df)\n",
    "    final_dataset = combined_df.drop_duplicates(subset=['Sequence'], keep='first')\n",
    "    final_dataset = final_dataset.drop('Priority', axis=1).reset_index(drop=True)\n",
    "    \n",
    "    final_dedup_removed = before_final_dedup - len(final_dataset)\n",
    "    print(f\"      📉 After final deduplication: {len(final_dataset):,} sequences\")\n",
    "    print(f\"      🗑️ Final duplicates removed: {final_dedup_removed:,}\")\n",
    "    print(f\"   {'─' * 70}\")\n",
    "    \n",
    "    return final_dataset\n",
    "\n",
    "def save_datasets_to_csv(datasets_dict, output_dir):\n",
    "    \"\"\"保存数据集为CSV文件\"\"\"\n",
    "    print(f\"\\n💾 SAVING PROCESSED DATASETS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 创建输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # ✅ 添加：创建日志目录并保存处理信息\n",
    "    log_dir = \"2_Log/2.1_Training set_and_test_set_processing\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # 定义文件名映射\n",
    "    filename_mapping = {\n",
    "        'external_avp': 'Initial_TR_AVP.csv',\n",
    "        'external_non_avp': 'Initial_TR_non_AVP.csv',\n",
    "        'internal_avp': 'Initial_TS_AVP.csv',\n",
    "        'internal_non_avp': 'Initial_TS_non_AVP.csv'\n",
    "    }\n",
    "    \n",
    "    # 保存每个数据集\n",
    "    saved_files = {}\n",
    "    processing_log = []  # ✅ 添加：记录处理日志\n",
    "    \n",
    "    for key, filename in filename_mapping.items():\n",
    "        if key in datasets_dict and datasets_dict[key] is not None:\n",
    "            dataset = datasets_dict[key]\n",
    "            if len(dataset) > 0:\n",
    "                # 确保列顺序：Id, Sequence, Source, Length, Label, Type\n",
    "                columns_order = ['Id', 'Sequence', 'Source', 'Length', 'Label', 'Type']\n",
    "                \n",
    "                # 检查并重新排列列\n",
    "                available_columns = [col for col in columns_order if col in dataset.columns]\n",
    "                dataset_to_save = dataset[available_columns].copy()\n",
    "                \n",
    "                file_path = os.path.join(output_dir, filename)\n",
    "                dataset_to_save.to_csv(file_path, index=False)\n",
    "                saved_files[key] = file_path\n",
    "                \n",
    "                # ✅ 添加：记录处理信息\n",
    "                log_entry = {\n",
    "                    'dataset': key.replace('_', ' ').title(),\n",
    "                    'sequences': len(dataset),\n",
    "                    'file_path': file_path,\n",
    "                    'columns': list(dataset_to_save.columns)\n",
    "                }\n",
    "                processing_log.append(log_entry)\n",
    "                \n",
    "                print(f\"   ✅ {key.replace('_', ' ').title()}: {len(dataset):,} sequences\")\n",
    "                print(f\"      └─ Saved to: {file_path}\")\n",
    "                print(f\"      └─ Columns: {list(dataset_to_save.columns)}\")\n",
    "            else:\n",
    "                print(f\"   ⚠️  {key.replace('_', ' ').title()}: No data to save\")\n",
    "        else:\n",
    "            print(f\"   ❌ {key.replace('_', ' ').title()}: Dataset not found\")\n",
    "    \n",
    "    # ✅ 添加：保存处理日志到指定目录\n",
    "    if processing_log:\n",
    "        import json\n",
    "        \n",
    "        log_data = {\n",
    "            'step': 'Step 1 - Dataset Processing',\n",
    "            'total_files_saved': len(saved_files),\n",
    "            'datasets': processing_log\n",
    "        }\n",
    "        \n",
    "        log_file = os.path.join(log_dir, \"step1_dataset_processing_log.json\")\n",
    "        with open(log_file, 'w') as f:\n",
    "            json.dump(log_data, f, indent=2)\n",
    "        print(f\"\\n   📄 Processing log saved to: {log_file}\")\n",
    "    \n",
    "    print(f\"\\n   📁 All files saved to: {output_dir}\")\n",
    "    print(f\"   📊 Total files saved: {len(saved_files)}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "# ============================================================================\n",
    "# 第一步：处理内部和外部数据集（带详细统计）\n",
    "# ============================================================================\n",
    "\n",
    "def step1_process_datasets_detailed():\n",
    "    \"\"\"第一步：详细处理并统计内部和外部数据集\"\"\"\n",
    "    print(\"🚀 STEP 1: DETAILED PROCESSING OF INTERNAL AND EXTERNAL DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ✅ 添加：创建日志目录\n",
    "    log_dir = \"2_Log/2.1_Training set_and_test_set_processing\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # ========== 处理内部AVP数据集 ==========\n",
    "    print(\"\\n📊 PROCESSING INTERNAL AVP DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 配置所有数据源\n",
    "    avp_configs = [\n",
    "        {'file_path': '1_Data/Raw_data/AVPdataset/dravp_antiviral_peptides.xlsx', 'sequence_col': 'Sequence', 'id_col': 'DRAVP_ID', 'source_name': 'DRAVP', 'file_type': 'excel'},\n",
    "        {'file_path': '1_Data/Raw_data/AVPdataset/AVPdb_data.xls', 'sequence_col': 'Sequence', 'id_col': 'Id', 'source_name': 'AVPdb', 'file_type': 'excel'},\n",
    "        {'file_path': '1_Data/Raw_data/AVPdataset/ACovPepDB_Data_Entirety.csv', 'sequence_col': 'Sequence', 'id_col': 'ACovPid', 'source_name': 'ACovPepDB', 'file_type': 'csv'},\n",
    "        {'file_path': '1_Data/Raw_data/AVPdataset/HIPdb_data.xls', 'sequence_col': 'SEQUENCE', 'id_col': 'ID', 'source_name': 'HIPdb', 'file_type': 'excel'}\n",
    "    ]\n",
    "    \n",
    "    amp_configs = [\n",
    "        {'file_path': '1_Data/Raw_data/AMPdataset/CAMP.xlsx', 'sequence_col': 'Seqence', 'id_col': 'Camp_ID', 'source_name': 'CAMP', 'file_type': 'excel'},\n",
    "        {'file_path': '1_Data/Raw_data/AMPdataset/dbaasp_peptides.xlsx', 'sequence_col': 'SEQUENCE', 'id_col': 'ID', 'source_name': 'DBAASP', 'file_type': 'excel'},\n",
    "        {'file_path': '1_Data/Raw_data/AMPdataset/dramp_general_avps.xlsx', 'sequence_col': 'Sequence', 'id_col': 'DRAMP_ID', 'source_name': 'DRAMP', 'file_type': 'excel'},\n",
    "        {'file_path': '1_Data/Raw_data/AMPdataset/dbAMP_AntiHIV_2024.fasta', 'sequence_col': 'Sequence', 'id_col': 'Id', 'source_name': 'dbAMP_AntiHIV', 'file_type': 'fasta'},\n",
    "        {'file_path': '1_Data/Raw_data/AMPdataset/dbAMP_Antiviral_2024.fasta', 'sequence_col': 'Sequence', 'id_col': 'Id', 'source_name': 'dbAMP_Antiviral', 'file_type': 'fasta'}\n",
    "    ]\n",
    "    \n",
    "    other_configs = [\n",
    "        {'file_path': '1_Data/Raw_data/Peptipedia_Antiviral.fasta', 'sequence_col': 'Sequence', 'id_col': 'Id', 'source_name': 'Peptipedia', 'file_type': 'fasta'}\n",
    "    ]\n",
    "    \n",
    "    # 处理所有内部AVP数据源\n",
    "    all_internal_avp_datasets = []\n",
    "    internal_avp_detailed_stats = {}\n",
    "    \n",
    "    print(\"   🔹 Processing individual internal AVP datasets:\")\n",
    "    for config in avp_configs + amp_configs + other_configs:\n",
    "        result = process_dataset_detailed(**config)\n",
    "        if result and result[0] is not None:\n",
    "            dataset, stats = result\n",
    "            all_internal_avp_datasets.append(dataset)\n",
    "            internal_avp_detailed_stats[config['source_name']] = stats\n",
    "    \n",
    "    # 合并内部AVP数据集\n",
    "    priority_order = ['DRAVP', 'AVPdb', 'ACovPepDB', 'HIPdb', 'CAMP', 'DBAASP', 'DRAMP', 'dbAMP_AntiHIV', 'dbAMP_Antiviral', 'Peptipedia']\n",
    "    internal_avp_dataset = merge_datasets_with_priority_detailed(all_internal_avp_datasets, priority_order, \"internal AVP\")\n",
    "    internal_avp_dataset['Label'] = 1\n",
    "    internal_avp_dataset['Type'] = 'AVP'\n",
    "    \n",
    "    # ========== 处理内部non_AVP数据集 ==========\n",
    "    print(f\"\\n📊 PROCESSING INTERNAL non_AVP DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    file_path = '1_Data/Raw_data/non_AVP/uniprotkb_13124.fasta'\n",
    "    print(\"   🔹 Processing UniProt non_AVP dataset:\")\n",
    "\n",
    "    result = process_dataset_detailed(file_path, 'Sequence', 'Id', 'UniProt', 'fasta')\n",
    "    if result[0] is not None:\n",
    "        internal_non_avp_dataset, non_avp_detailed_stats = result\n",
    "        \n",
    "        # 处理UniProt ID格式\n",
    "        def process_uniprot_id(uniprot_id):\n",
    "            \"\"\"\n",
    "            处理UniProt ID，只保留前两个竖杠的部分\n",
    "            例如: sp|A5A616|YXXX_HUMAN -> sp|A5A616|\n",
    "            \"\"\"\n",
    "            if pd.isna(uniprot_id):\n",
    "                return uniprot_id\n",
    "            \n",
    "            id_str = str(uniprot_id).strip()\n",
    "            \n",
    "            # 查找所有竖杠的位置\n",
    "            pipe_positions = [i for i, char in enumerate(id_str) if char == '|']\n",
    "            \n",
    "            if len(pipe_positions) >= 2:\n",
    "                # 保留到第二个竖杠之后（包含第二个竖杠）\n",
    "                return id_str[:pipe_positions[1] + 1]\n",
    "            elif len(pipe_positions) == 1:\n",
    "                # 如果只有一个竖杠，保留到第一个竖杠之后\n",
    "                return id_str[:pipe_positions[0] + 1]\n",
    "            else:\n",
    "                # 如果没有竖杠，返回原ID\n",
    "                return id_str\n",
    "        \n",
    "        # 应用ID处理\n",
    "        original_ids = internal_non_avp_dataset['Id'].copy()\n",
    "        internal_non_avp_dataset['Id'] = internal_non_avp_dataset['Id'].apply(process_uniprot_id)\n",
    "        \n",
    "        # 显示ID处理示例\n",
    "        print(f\"      📝 ID format examples:\")\n",
    "        for i in range(min(5, len(original_ids))):\n",
    "            old_id = original_ids.iloc[i]\n",
    "            new_id = internal_non_avp_dataset['Id'].iloc[i]\n",
    "            print(f\"         {old_id} → {new_id}\")\n",
    "        \n",
    "        print(f\"      ✅ Processed {len(internal_non_avp_dataset)} UniProt IDs to short format\")\n",
    "        \n",
    "        internal_non_avp_dataset['Label'] = 0\n",
    "        internal_non_avp_dataset['Type'] = 'non_AVP'\n",
    "    else:\n",
    "        print(\"      ❌ Failed to process non_AVP dataset\")\n",
    "        return None\n",
    "    \n",
    "    # ========== 处理外部数据集 ==========\n",
    "    print(f\"\\n📊 PROCESSING EXTERNAL DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    external_files = {\n",
    "        'AVP': {\n",
    "            'Stack-AVP-TR_pos': '1_Data/Raw_data/External_dataset/Stack-AVP-TR_pos.fasta'\n",
    "        },\n",
    "        'non_AVP': {\n",
    "            'Stack-AVP-TR_neg': '1_Data/Raw_data/External_dataset/Stack-AVP-TR_neg.fasta'\n",
    "        },\n",
    "        'mixed': {\n",
    "            'AVP-HNCL_non-AMP': '1_Data/Raw_data/External_dataset/AVP-HNCL_non-AMP TR dataset.txt',\n",
    "            'AVP-HNCL_non-AVP': '1_Data/Raw_data/External_dataset/AVP-HNCL_non-AVP TR dataset.txt'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    external_individual_datasets = {'AVP': [], 'non_AVP': []}\n",
    "    external_detailed_stats = {}\n",
    "    \n",
    "    # 处理纯AVP文件\n",
    "    print(\"   🔹 Processing external AVP datasets:\")\n",
    "    for name, file_path in external_files['AVP'].items():\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"      ⚠️ File not found: {name}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            result = process_dataset_detailed(file_path, 'Sequence', 'Id', name, 'fasta')\n",
    "            if result and result[0] is not None:\n",
    "                filtered_df, stats = result\n",
    "                filtered_df['Label'] = 1  # AVP\n",
    "                external_individual_datasets['AVP'].append(filtered_df)\n",
    "                external_detailed_stats[name] = {**stats, 'type': 'AVP'}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ❌ Error processing {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # 处理纯non_AVP文件\n",
    "    print(\"   🔹 Processing external non_AVP datasets:\")\n",
    "    for name, file_path in external_files['non_AVP'].items():\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"      ⚠️ File not found: {name}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            result = process_dataset_detailed(file_path, 'Sequence', 'Id', name, 'fasta')\n",
    "            if result and result[0] is not None:\n",
    "                filtered_df, stats = result\n",
    "                filtered_df['Label'] = 0  # non_AVP\n",
    "                external_individual_datasets['non_AVP'].append(filtered_df)\n",
    "                external_detailed_stats[name] = {**stats, 'type': 'non_AVP'}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ❌ Error processing {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # 处理混合标签的txt文件\n",
    "    print(\"   🔹 Processing mixed-label datasets:\")\n",
    "    for name, file_path in external_files['mixed'].items():\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"      ⚠️ File not found: {name}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            sequences_data = []\n",
    "            encodings = ['utf-8', 'latin-1', 'cp1252', 'gbk']\n",
    "            \n",
    "            print(f\"      {'─' * 70}\")\n",
    "            print(f\"      📊 Processing mixed-label dataset: {name}\")\n",
    "            print(f\"      {'─' * 70}\")\n",
    "            \n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding=encoding) as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    # 首先检查是否是FASTA格式\n",
    "                    if '>' in content:\n",
    "                        # FASTA格式处理\n",
    "                        current_id = None\n",
    "                        current_seq = []\n",
    "                        \n",
    "                        for line in content.split('\\n'):\n",
    "                            line = line.strip()\n",
    "                            if not line:\n",
    "                                continue\n",
    "                                \n",
    "                            if line.startswith('>'):\n",
    "                                # 保存前一个序列\n",
    "                                if current_id and current_seq:\n",
    "                                    sequence = ''.join(current_seq)\n",
    "                                    # 从ID中推断标签\n",
    "                                    header_lower = current_id.lower()\n",
    "                                    if any(pos_indicator in header_lower for pos_indicator in ['pos']):\n",
    "                                        label = 1  # AVP\n",
    "                                    elif any(neg_indicator in header_lower for neg_indicator in ['neg']):\n",
    "                                        label = 0  # non_AVP\n",
    "                                    else:\n",
    "                                        # 根据文件名推断\n",
    "                                        if 'non-AMP' in name or 'non-AVP' in name:\n",
    "                                            label = 0  # non_AVP\n",
    "                                        else:\n",
    "                                            label = 1  # AVP (默认)\n",
    "                                    \n",
    "                                    sequences_data.append({\n",
    "                                        'Id': current_id,\n",
    "                                        'Sequence': sequence,\n",
    "                                        'Label': label\n",
    "                                    })\n",
    "                                \n",
    "                                current_id = line[1:]  # 去掉 '>'\n",
    "                                current_seq = []\n",
    "                            else:\n",
    "                                current_seq.append(line)\n",
    "                        \n",
    "                        # 保存最后一个序列\n",
    "                        if current_id and current_seq:\n",
    "                            sequence = ''.join(current_seq)\n",
    "                            # 从ID中推断标签\n",
    "                            header_lower = current_id.lower()\n",
    "                            if any(pos_indicator in header_lower for pos_indicator in ['pos']):\n",
    "                                label = 1  # AVP\n",
    "                            elif any(neg_indicator in header_lower for neg_indicator in ['neg']):\n",
    "                                label = 0  # non_AVP\n",
    "                            else:\n",
    "                                # 根据文件名推断\n",
    "                                if 'non-AMP' in name or 'non-AVP' in name:\n",
    "                                    label = 0  # non_AVP\n",
    "                                else:\n",
    "                                    label = 1  # AVP (默认)\n",
    "                            \n",
    "                            sequences_data.append({\n",
    "                                'Id': current_id,\n",
    "                                'Sequence': sequence,\n",
    "                                'Label': label\n",
    "                            })\n",
    "                    \n",
    "                    else:\n",
    "                        # 非FASTA格式，按行处理\n",
    "                        for line_num, line in enumerate(content.split('\\n'), 1):\n",
    "                            line = line.strip()\n",
    "                            if not line or line.startswith('#'):\n",
    "                                continue\n",
    "                            \n",
    "                            # 尝试不同的分隔符\n",
    "                            parts = None\n",
    "                            for sep in ['\\t', ' ', ',', ';']:\n",
    "                                if sep in line:\n",
    "                                    parts = [p.strip() for p in line.split(sep) if p.strip()]\n",
    "                                    break\n",
    "                            \n",
    "                            if parts is None:\n",
    "                                parts = [line.strip()]\n",
    "                            \n",
    "                            if len(parts) == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            # 解析序列和标签\n",
    "                            sequence = None\n",
    "                            label = None\n",
    "                            \n",
    "                            for part in parts:\n",
    "                                # 检查是否为标签\n",
    "                                if part.lower() in ['pos', 'positive', '1']:\n",
    "                                    label = 1  # AVP\n",
    "                                elif part.lower() in ['neg', 'negative', '0']:\n",
    "                                    label = 0  # non_AVP\n",
    "                                else:\n",
    "                                    # 检查是否为序列（包含氨基酸字符）\n",
    "                                    if len(part) > 3 and all(c.upper() in 'ACDEFGHIKLMNPQRSTVWY' for c in part if c.isalpha()):\n",
    "                                        sequence = part\n",
    "                            \n",
    "                            # 如果没有找到明确的序列，使用第一个非标签部分\n",
    "                            if sequence is None:\n",
    "                                for part in parts:\n",
    "                                    if part.lower() not in ['pos', 'negative', 'positive', 'neg', '1', '0']:\n",
    "                                        if len(part) > 3:  # 序列长度至少为4\n",
    "                                            sequence = part\n",
    "                                            break\n",
    "                            \n",
    "                            # 如果没有找到标签，根据文件名推断\n",
    "                            if label is None:\n",
    "                                if 'non-AMP' in name or 'non-AVP' in name:\n",
    "                                    label = 0  # non_AVP\n",
    "                                else:\n",
    "                                    label = 1  # AVP (默认)\n",
    "                            \n",
    "                            if sequence:\n",
    "                                sequences_data.append({\n",
    "                                    'Id': f\"{name}_{line_num}\",\n",
    "                                    'Sequence': sequence,\n",
    "                                    'Label': label\n",
    "                                })\n",
    "                    \n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            if sequences_data:\n",
    "                df = pd.DataFrame(sequences_data)\n",
    "                original_count = len(df)\n",
    "                \n",
    "                # 统计原始标签分布\n",
    "                original_avp_count = len(df[df['Label'] == 1])\n",
    "                original_non_avp_count = len(df[df['Label'] == 0])\n",
    "                \n",
    "                print(f\"         📊 Original data: {original_count} total sequences\")\n",
    "                print(f\"            ├─ AVP (positive): {original_avp_count}\")\n",
    "                print(f\"            └─ non_AVP (negative): {original_non_avp_count}\")\n",
    "                print(f\"         {'┄' * 50}\")\n",
    "                \n",
    "                # 应用详细过滤\n",
    "                filtered_df, detailed_stats = filter_peptides_detailed(df, sequence_col='Sequence')\n",
    "                \n",
    "                if len(filtered_df) > 0:\n",
    "                    filtered_df['Source'] = name\n",
    "                    filtered_df['Length'] = filtered_df['Sequence'].apply(len)\n",
    "                    \n",
    "                    # 按标签分组\n",
    "                    avp_df = filtered_df[filtered_df['Label'] == 1].copy()\n",
    "                    non_avp_df = filtered_df[filtered_df['Label'] == 0].copy()\n",
    "                    \n",
    "                    print(f\"         {'┄' * 50}\")\n",
    "                    print(f\"         📊 After all filtering: {len(filtered_df)} total sequences\")\n",
    "                    print(f\"            ├─ AVP: {len(avp_df)} (lost: {original_avp_count - len(avp_df)})\")\n",
    "                    print(f\"            └─ non_AVP: {len(non_avp_df)} (lost: {original_non_avp_count - len(non_avp_df)})\")\n",
    "                    \n",
    "                    if len(avp_df) > 0:\n",
    "                        external_individual_datasets['AVP'].append(avp_df)\n",
    "                    if len(non_avp_df) > 0:\n",
    "                        external_individual_datasets['non_AVP'].append(non_avp_df)\n",
    "                    \n",
    "                    external_detailed_stats[name] = {\n",
    "                        **detailed_stats,\n",
    "                        'original_avp_count': original_avp_count,\n",
    "                        'original_non_avp_count': original_non_avp_count,\n",
    "                        'filtered_avp_count': len(avp_df),\n",
    "                        'filtered_non_avp_count': len(non_avp_df)\n",
    "                    }\n",
    "                else:\n",
    "                    print(f\"         ⚠️ No sequences passed filtering\")\n",
    "            else:\n",
    "                print(f\"         ⚠️ No sequences found in file\")\n",
    "            \n",
    "            print(f\"      {'─' * 70}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ❌ Error processing {name}: {str(e)}\")\n",
    "            print(f\"      {'─' * 70}\")\n",
    "            continue\n",
    "    \n",
    "    # 合并外部数据集\n",
    "    external_avp_dataset = merge_datasets_with_priority_detailed(\n",
    "        external_individual_datasets['AVP'], \n",
    "        ['Stack-AVP-TR_pos', 'AVP-HNCL_non-AMP', 'AVP-HNCL_non-AVP'], \n",
    "        \"external AVP\"\n",
    "    )\n",
    "    if len(external_avp_dataset) > 0:\n",
    "        external_avp_dataset['Type'] = 'AVP'\n",
    "    \n",
    "    external_non_avp_dataset = merge_datasets_with_priority_detailed(\n",
    "        external_individual_datasets['non_AVP'],\n",
    "        ['Stack-AVP-TR_neg', 'AVP-HNCL_non-AMP', 'AVP-HNCL_non-AVP'],\n",
    "        \"external non_AVP\"\n",
    "    )\n",
    "    if len(external_non_avp_dataset) > 0:\n",
    "        external_non_avp_dataset['Type'] = 'non_AVP'\n",
    "    \n",
    "    # ========== 保存处理后的数据集 ==========\n",
    "    datasets_to_save = {\n",
    "        'internal_avp': internal_avp_dataset,\n",
    "        'internal_non_avp': internal_non_avp_dataset,\n",
    "        'external_avp': external_avp_dataset,\n",
    "        'external_non_avp': external_non_avp_dataset\n",
    "    }\n",
    "    \n",
    "    output_dir = \"1_Data/Processed_data_set/Initial_merged_data_set\"\n",
    "    saved_files = save_datasets_to_csv(datasets_to_save, output_dir)\n",
    "    \n",
    "    # ========== 打印详细统计汇总 ==========\n",
    "    print(f\"\\n{'═' * 80}\")\n",
    "    print(f\"📋 COMPREHENSIVE STATISTICS SUMMARY\")\n",
    "    print(f\"{'═' * 80}\")\n",
    "    \n",
    "    print(f\"\\n🔹 INTERNAL DATASETS DETAILED BREAKDOWN:\")\n",
    "    print(f\"{'─' * 80}\")\n",
    "    print(f\"   📊 Individual AVP dataset filtering details:\")\n",
    "    for source, stats in internal_avp_detailed_stats.items():\n",
    "        print(f\"   {'┌' + '─' * 65 + '┐'}\")\n",
    "        print(f\"   │  {source:<61s}  │\")\n",
    "        print(f\"   {'├' + '─' * 65 + '┤'}\")\n",
    "        print(f\"   │  Original: {stats['step0_original']:>8,} sequences{' ' * (65 - len(f'Original: {stats['step0_original']:,} sequences') - 2)}│\")\n",
    "        print(f\"   │  Non-natural AA removed: {stats['removed_non_natural_aa']:>8,}{' ' * (65 - len(f'Non-natural AA removed: {stats['removed_non_natural_aa']:,}') - 2)}│\")\n",
    "        print(f\"   │  Too short removed: {stats['removed_too_short']:>8,}{' ' * (65 - len(f'Too short removed: {stats['removed_too_short']:,}') - 2)}│\")\n",
    "        print(f\"   │  Too long removed: {stats['removed_too_long']:>8,}{' ' * (65 - len(f'Too long removed: {stats['removed_too_long']:,}') - 2)}│\")\n",
    "        print(f\"   │  Duplicates removed: {stats['removed_duplicates']:>8,}{' ' * (65 - len(f'Duplicates removed: {stats['removed_duplicates']:,}') - 2)}│\")\n",
    "        print(f\"   │  Final: {stats['step3_final']:>8,} sequences{' ' * (65 - len(f'Final: {stats['step3_final']:,} sequences') - 2)}│\")\n",
    "        print(f\"   {'└' + '─' * 65 + '┘'}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"{'─' * 80}\")\n",
    "    print(f\"   📊 Internal non_AVP (UniProt) filtering details:\")\n",
    "    stats = non_avp_detailed_stats\n",
    "    print(f\"   {'┌' + '─' * 65 + '┐'}\")\n",
    "    print(f\"   │  {'UniProt non_AVP':<61s}  │\")\n",
    "    print(f\"   {'├' + '─' * 65 + '┤'}\")\n",
    "    print(f\"   │  Original: {stats['step0_original']:>8,} sequences{' ' * (65 - len(f'Original: {stats['step0_original']:,} sequences') - 2)}│\")\n",
    "    print(f\"   │  Non-natural AA removed: {stats['removed_non_natural_aa']:>8,}{' ' * (65 - len(f'Non-natural AA removed: {stats['removed_non_natural_aa']:,}') - 2)}│\")\n",
    "    print(f\"   │  Too short removed: {stats['removed_too_short']:>8,}{' ' * (65 - len(f'Too short removed: {stats['removed_too_short']:,}') - 2)}│\")\n",
    "    print(f\"   │  Too long removed: {stats['removed_too_long']:>8,}{' ' * (65 - len(f'Too long removed: {stats['removed_too_long']:,}') - 2)}│\")\n",
    "    print(f\"   │  Duplicates removed: {stats['removed_duplicates']:>8,}{' ' * (65 - len(f'Duplicates removed: {stats['removed_duplicates']:,}') - 2)}│\")\n",
    "    print(f\"   │  Final: {stats['step3_final']:>8,} sequences{' ' * (65 - len(f'Final: {stats['step3_final']:,} sequences') - 2)}│\")\n",
    "    print(f\"   {'└' + '─' * 65 + '┘'}\")\n",
    "    \n",
    "    print(f\"\\n🔹 EXTERNAL DATASETS DETAILED BREAKDOWN:\")\n",
    "    print(f\"{'─' * 80}\")\n",
    "    for source, stats in external_detailed_stats.items():\n",
    "        print(f\"   {'┌' + '─' * 75 + '┐'}\")\n",
    "        print(f\"   │  📊 {source:<69s}  │\")\n",
    "        print(f\"   {'├' + '─' * 75 + '┤'}\")\n",
    "        \n",
    "        if 'original_avp_count' in stats:\n",
    "            # 混合标签数据集\n",
    "            orig_text = f\"Original: {stats['step0_original']:,} (AVP: {stats['original_avp_count']}, non_AVP: {stats['original_non_avp_count']})\"\n",
    "            print(f\"   │  {orig_text:<73s}  │\")\n",
    "            print(f\"   │  Non-natural AA removed: {stats['removed_non_natural_aa']:>8,}{' ' * (75 - len(f'Non-natural AA removed: {stats['removed_non_natural_aa']:,}') - 2)}│\")\n",
    "            print(f\"   │  Too short removed: {stats['removed_too_short']:>8,}{' ' * (75 - len(f'Too short removed: {stats['removed_too_short']:,}') - 2)}│\")\n",
    "            print(f\"   │  Too long removed: {stats['removed_too_long']:>8,}{' ' * (75 - len(f'Too long removed: {stats['removed_too_long']:,}') - 2)}│\")\n",
    "            print(f\"   │  Duplicates removed: {stats['removed_duplicates']:>8,}{' ' * (75 - len(f'Duplicates removed: {stats['removed_duplicates']:,}') - 2)}│\")\n",
    "            final_text = f\"Final: {stats['step3_final']:,} (AVP: {stats['filtered_avp_count']}, non_AVP: {stats['filtered_non_avp_count']})\"\n",
    "            print(f\"   │  {final_text:<73s}  │\")\n",
    "        else:\n",
    "            # 单一标签数据集\n",
    "            orig_text = f\"Original: {stats['step0_original']:,} sequences ({stats.get('type', 'Unknown')})\"\n",
    "            print(f\"   │  {orig_text:<73s}  │\")\n",
    "            print(f\"   │  Non-natural AA removed: {stats['removed_non_natural_aa']:>8,}{' ' * (75 - len(f'Non-natural AA removed: {stats['removed_non_natural_aa']:,}') - 2)}│\")\n",
    "            print(f\"   │  Too short removed: {stats['removed_too_short']:>8,}{' ' * (75 - len(f'Too short removed: {stats['removed_too_short']:,}') - 2)}│\")\n",
    "            print(f\"   │  Too long removed: {stats['removed_too_long']:>8,}{' ' * (75 - len(f'Too long removed: {stats['removed_too_long']:,}') - 2)}│\")\n",
    "            print(f\"   │  Duplicates removed: {stats['removed_duplicates']:>8,}{' ' * (75 - len(f'Duplicates removed: {stats['removed_duplicates']:,}') - 2)}│\")\n",
    "            print(f\"   │  Final: {stats['step3_final']:>8,} sequences{' ' * (75 - len(f'Final: {stats['step3_final']:,} sequences') - 2)}│\")\n",
    "        \n",
    "        print(f\"   {'└' + '─' * 75 + '┘'}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"{'═' * 80}\")\n",
    "    print(f\"🔹 FINAL DATASET SUMMARY:\")\n",
    "    print(f\"{'═' * 80}\")\n",
    "    print(f\"   📈 External AVP: {len(external_avp_dataset):,} sequences (Initial_TR_AVP.csv)\")\n",
    "    print(f\"   📈 External non_AVP: {len(external_non_avp_dataset):,} sequences (Initial_TR_non_AVP.csv)\")\n",
    "    print(f\"   📈 Internal AVP: {len(internal_avp_dataset):,} sequences (Initial_TS_AVP.csv)\")\n",
    "    print(f\"   📈 Internal non_AVP: {len(internal_non_avp_dataset):,} sequences (Initial_TS_non_AVP.csv)\") \n",
    "\n",
    "    print(f\"{'─' * 80}\")\n",
    "    grand_total = len(internal_avp_dataset) + len(internal_non_avp_dataset) + len(external_avp_dataset) + len(external_non_avp_dataset)\n",
    "    print(f\"   📊 Grand Total: {grand_total:,} sequences\")\n",
    "    print(f\"{'═' * 80}\")\n",
    "    \n",
    "    # ✅ 添加：保存Step 1完整摘要日志\n",
    "    import json\n",
    "    \n",
    "    overall_log = {\n",
    "        'step': 'Complete Step 1 Processing',\n",
    "        'internal_avp_stats': internal_avp_detailed_stats,\n",
    "        'internal_non_avp_stats': non_avp_detailed_stats,\n",
    "        'external_stats': external_detailed_stats,\n",
    "        'final_summary': {\n",
    "            'internal_avp_count': len(internal_avp_dataset),\n",
    "            'internal_non_avp_count': len(internal_non_avp_dataset),\n",
    "            'external_avp_count': len(external_avp_dataset),\n",
    "            'external_non_avp_count': len(external_non_avp_dataset),\n",
    "            'grand_total': grand_total\n",
    "        },\n",
    "        'saved_files': saved_files\n",
    "    }\n",
    "    \n",
    "    overall_log_file = os.path.join(log_dir, \"step1_complete_processing_summary.json\")\n",
    "    with open(overall_log_file, 'w') as f:\n",
    "        json.dump(overall_log, f, indent=2, default=str)\n",
    "    print(f\"\\n📄 Complete Step 1 summary saved to: {overall_log_file}\")\n",
    "    \n",
    "    return {\n",
    "        'internal_avp': internal_avp_dataset,\n",
    "        'internal_non_avp': internal_non_avp_dataset,\n",
    "        'external_avp': external_avp_dataset,\n",
    "        'external_non_avp': external_non_avp_dataset,\n",
    "        'internal_avp_stats': internal_avp_detailed_stats,\n",
    "        'internal_non_avp_stats': non_avp_detailed_stats,\n",
    "        'external_stats': external_detailed_stats,\n",
    "        'saved_files': saved_files\n",
    "    }\n",
    "\n",
    "# 运行详细的第一步处理\n",
    "print(\"🚀 Starting Step 1: Detailed Dataset Processing and Statistics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    step1_detailed_results = step1_process_datasets_detailed()\n",
    "    \n",
    "    if step1_detailed_results:\n",
    "        print(\"\\n✅ Step 1 completed successfully with detailed statistics!\")\n",
    "        print(f\"   📁 Results stored in step1_detailed_results variable\")\n",
    "        print(f\"   💾 CSV files saved in: 1_Data/Processed_data_set/Initial_merged_data_set/\")\n",
    "        print(f\"   📊 Ready for Step 2: Overlap analysis\")\n",
    "    else:\n",
    "        print(\"❌ Step 1 failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during Step 1: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n🎯 Step 1 Complete - Detailed filtering statistics generated for all datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48a311",
   "metadata": {},
   "source": [
    "# 2.数据集独立化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69ff0b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting Step 2: Enhanced Overlap Analysis and Dataset Independence\n",
      "================================================================================\n",
      "\n",
      "🚀 STEP 2: ENHANCED OVERLAP ANALYSIS AND DATASET INDEPENDENCE\n",
      "================================================================================\n",
      "\n",
      "📊 INITIAL DATASET OVERVIEW:\n",
      "   📈 Initial_TR_AVP: 3,412 sequences\n",
      "   📈 Initial_TR_non_AVP: 4,049 sequences\n",
      "   📈 Initial_TS_AVP: 4,993 sequences\n",
      "   📈 Initial_TS_non_AVP: 9,402 sequences\n",
      "   📊 Total: 21,856 sequences\n",
      "\n",
      "🔍 PHASE 1: LABEL CONFLICT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "   🔎 Analyzing label conflicts across datasets...\n",
      "   ⚠️  Found 521 sequences with label conflicts\n",
      "   📊 Total conflict records: 521\n",
      "   ✅ Label conflicts saved to: 1_Data/Processed_data_set/Final_merged_data_set/Overlap_data.csv\n",
      "\n",
      "🧹 PHASE 2: REMOVING CONFLICT SEQUENCES FROM DATASETS\n",
      "================================================================================\n",
      "   📊 Initial_TS_AVP:\n",
      "      Original: 4,993 sequences\n",
      "      Cleaned: 4,474 sequences\n",
      "      Removed: 519 sequences (10.39%)\n",
      "   📊 Initial_TS_non_AVP:\n",
      "      Original: 9,402 sequences\n",
      "      Cleaned: 9,172 sequences\n",
      "      Removed: 230 sequences (2.45%)\n",
      "   📊 Initial_TR_AVP:\n",
      "      Original: 3,412 sequences\n",
      "      Cleaned: 3,091 sequences\n",
      "      Removed: 321 sequences (9.41%)\n",
      "   📊 Initial_TR_non_AVP:\n",
      "      Original: 4,049 sequences\n",
      "      Cleaned: 3,719 sequences\n",
      "      Removed: 330 sequences (8.15%)\n",
      "\n",
      "🔄 PHASE 3: REMOVING DUPLICATE SEQUENCES\n",
      "================================================================================\n",
      "   🔎 Comparing datasets for duplicates...\n",
      "\n",
      "   📈 Duplicate removal results:\n",
      "      AVP datasets overlap: 3,071 sequences\n",
      "      non_AVP datasets overlap: 1,278 sequences\n",
      "      TS_AVP (unique custom): 1,403 sequences\n",
      "      TS_non_AVP (unique custom): 7,894 sequences\n",
      "      TR_AVP (external): 3,091 sequences\n",
      "      TR_non_AVP (external): 3,719 sequences\n",
      "\n",
      "💾 PHASE 4: SAVING FINAL DATASETS\n",
      "================================================================================\n",
      "   ✅ TS_AVP: 1,403 sequences → TS_AVP.csv\n",
      "   ✅ TS_non_AVP: 7,894 sequences → TS_non_AVP.csv\n",
      "   ✅ TR_AVP: 3,091 sequences → TR_AVP.csv\n",
      "   ✅ TR_non_AVP: 3,719 sequences → TR_non_AVP.csv\n",
      "\n",
      "✅ PHASE 5: INDEPENDENCE VERIFICATION\n",
      "================================================================================\n",
      "   ✅ TS_AVP ↔ TS_non_AVP: Independent\n",
      "   ✅ TS_AVP ↔ TR_AVP: Independent\n",
      "   ✅ TS_AVP ↔ TR_non_AVP: Independent\n",
      "   ✅ TS_non_AVP ↔ TR_AVP: Independent\n",
      "   ✅ TS_non_AVP ↔ TR_non_AVP: Independent\n",
      "   ✅ TR_AVP ↔ TR_non_AVP: Independent\n",
      "\n",
      "   🎉 ALL DATASETS ARE COMPLETELY INDEPENDENT!\n",
      "\n",
      "📋 COMPREHENSIVE FINAL REPORT\n",
      "================================================================================\n",
      "\n",
      "🔹 LABEL CONFLICT ANALYSIS:\n",
      "   ⚠️  Conflicting sequences: 521\n",
      "   📄 Overlap data file: Overlap_data.csv\n",
      "\n",
      "🔹 DATASET TRANSFORMATION:\n",
      "   Initial_TS_AVP           :  4,993 →  4,474 (-519)\n",
      "   Initial_TS_non_AVP       :  9,402 →  9,172 (-230)\n",
      "   Initial_TR_AVP           :  3,412 →  3,091 (-321)\n",
      "   Initial_TR_non_AVP       :  4,049 →  3,719 (-330)\n",
      "\n",
      "🔹 FINAL INDEPENDENT DATASETS:\n",
      "   TS_AVP         :  1,403 sequences\n",
      "   TS_non_AVP     :  7,894 sequences\n",
      "   TR_AVP         :  3,091 sequences\n",
      "   TR_non_AVP     :  3,719 sequences\n",
      "   ────────────────────────────────────────\n",
      "   Total Final    : 16,107 sequences\n",
      "\n",
      "🔹 DATA REDUCTION SUMMARY:\n",
      "   Initial total: 21,856 sequences\n",
      "   Final total: 16,107 sequences\n",
      "   Reduction: 5,749 sequences (26.30%)\n",
      "\n",
      "   📄 Statistics saved to: 2_Log/2.1_Training set_and_test_set_processing/step2_overlap_analysis_statistics.json\n",
      "\n",
      "✅ Step 2 completed successfully!\n",
      "   📁 Results stored in step2_results variable\n",
      "   🎉 All final datasets are completely independent!\n",
      "   💾 Final datasets saved to: 1_Data/Processed_data_set/Final_merged_data_set/\n",
      "   📊 Files created:\n",
      "      - TS_AVP.csv\n",
      "      - TS_non_AVP.csv\n",
      "      - TR_AVP.csv\n",
      "      - TR_non_AVP.csv\n",
      "      - Overlap_data.csv (label conflicts)\n",
      "      - processing_statistics.json (detailed stats)\n",
      "\n",
      "🎯 Step 2 Complete - Final independent datasets ready for model training!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 第二步：重叠分析和数据集独立化\n",
    "# ============================================================================\n",
    "\n",
    "def step2_overlap_analysis_enhanced(step1_results):\n",
    "    \"\"\"第二步：增强版重叠分析和数据集独立化\"\"\"\n",
    "    print(\"\\n🚀 STEP 2: ENHANCED OVERLAP ANALYSIS AND DATASET INDEPENDENCE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 获取四个数据集\n",
    "    initial_custom_avp = step1_results['internal_avp'].copy()\n",
    "    initial_custom_non_avp = step1_results['internal_non_avp'].copy()\n",
    "    initial_tr_avp = step1_results['external_avp'].copy()\n",
    "    initial_tr_non_avp = step1_results['external_non_avp'].copy()\n",
    "    \n",
    "    print(f\"\\n📊 INITIAL DATASET OVERVIEW:\")\n",
    "    print(f\"   📈 Initial_TR_AVP: {len(initial_tr_avp):,} sequences\")\n",
    "    print(f\"   📈 Initial_TR_non_AVP: {len(initial_tr_non_avp):,} sequences\")\n",
    "    print(f\"   📈 Initial_TS_AVP: {len(initial_custom_avp):,} sequences\")\n",
    "    print(f\"   📈 Initial_TS_non_AVP: {len(initial_custom_non_avp):,} sequences\")\n",
    "\n",
    "    print(f\"   📊 Total: {len(initial_custom_avp) + len(initial_custom_non_avp) + len(initial_tr_avp) + len(initial_tr_non_avp):,} sequences\")\n",
    "    \n",
    "    # 创建输出目录\n",
    "    output_dir = \"1_Data/Processed_data_set/Final_merged_data_set\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # ✅ 添加：创建日志目录\n",
    "    log_dir = \"2_Log/2.1_Training set_and_test_set_processing\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # ========== 第一阶段：标签冲突分析 ==========\n",
    "    print(f\"\\n🔍 PHASE 1: LABEL CONFLICT ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 创建数据集字典，便于分析\n",
    "    datasets = {\n",
    "        'Initial_TS_AVP': {'data': initial_custom_avp, 'expected_label': 1},        # ✅ 修改\n",
    "        'Initial_TS_non_AVP': {'data': initial_custom_non_avp, 'expected_label': 0}, # ✅ 修改\n",
    "        'Initial_TR_AVP': {'data': initial_tr_avp, 'expected_label': 1},\n",
    "        'Initial_TR_non_AVP': {'data': initial_tr_non_avp, 'expected_label': 0}\n",
    "    }\n",
    "    \n",
    "    # 收集所有序列及其来源和标签\n",
    "    sequence_sources = {}\n",
    "    \n",
    "    for dataset_name, dataset_info in datasets.items():\n",
    "        data = dataset_info['data']\n",
    "        expected_label = dataset_info['expected_label']\n",
    "        \n",
    "        for _, row in data.iterrows():\n",
    "            sequence = row['Sequence'].upper()\n",
    "            actual_label = row['Label']\n",
    "            \n",
    "            if sequence not in sequence_sources:\n",
    "                sequence_sources[sequence] = []\n",
    "            \n",
    "            sequence_sources[sequence].append({\n",
    "                'dataset': dataset_name,\n",
    "                'expected_label': expected_label,\n",
    "                'actual_label': actual_label,\n",
    "                'source': row['Source'] if 'Source' in row else 'Unknown',\n",
    "                'id': row['Id'] if 'Id' in row else 'Unknown'\n",
    "            })\n",
    "    \n",
    "    # 识别标签冲突\n",
    "    label_conflicts = []\n",
    "    conflict_sequences = set()\n",
    "    \n",
    "    print(f\"\\n   🔎 Analyzing label conflicts across datasets...\")\n",
    "    \n",
    "    for sequence, sources in sequence_sources.items():\n",
    "        if len(sources) > 1:  # 序列出现在多个数据集中\n",
    "            labels = [source['expected_label'] for source in sources]\n",
    "            if len(set(labels)) > 1:  # 标签不一致\n",
    "                conflict_sequences.add(sequence)\n",
    "                \n",
    "                # 创建冲突记录\n",
    "                conflict_record = {\n",
    "                    'Sequence': sequence,\n",
    "                    'Length': len(sequence),\n",
    "                    'Conflict_Type': 'Label_Mismatch',\n",
    "                    'Dataset_Count': len(sources)\n",
    "                }\n",
    "                \n",
    "                # 添加每个数据集的信息\n",
    "                for i, source in enumerate(sources):\n",
    "                    conflict_record[f'Dataset_{i+1}'] = source['dataset']\n",
    "                    conflict_record[f'Label_{i+1}'] = source['expected_label']\n",
    "                    conflict_record[f'Source_{i+1}'] = source['source']\n",
    "                    conflict_record[f'ID_{i+1}'] = source['id']\n",
    "                \n",
    "                # 填充空列（如果某些序列出现次数少于最大出现次数）\n",
    "                max_occurrences = max(len(sources) for sources in sequence_sources.values() if len(sources) > 1)\n",
    "                for i in range(len(sources), max_occurrences):\n",
    "                    conflict_record[f'Dataset_{i+1}'] = ''\n",
    "                    conflict_record[f'Label_{i+1}'] = ''\n",
    "                    conflict_record[f'Source_{i+1}'] = ''\n",
    "                    conflict_record[f'ID_{i+1}'] = ''\n",
    "                \n",
    "                label_conflicts.append(conflict_record)\n",
    "    \n",
    "    print(f\"   ⚠️  Found {len(conflict_sequences):,} sequences with label conflicts\")\n",
    "    print(f\"   📊 Total conflict records: {len(label_conflicts):,}\")\n",
    "    \n",
    "    # 保存标签冲突数据\n",
    "    if label_conflicts:\n",
    "        conflicts_df = pd.DataFrame(label_conflicts)\n",
    "        conflicts_file = os.path.join(output_dir, \"Overlap_data.csv\")\n",
    "        conflicts_df.to_csv(conflicts_file, index=False)\n",
    "        print(f\"   ✅ Label conflicts saved to: {conflicts_file}\")\n",
    "    else:\n",
    "        print(f\"   🎉 No label conflicts found!\")\n",
    "    \n",
    "    # ========== 第二阶段：从数据集中移除标签冲突序列 ==========\n",
    "    print(f\"\\n🧹 PHASE 2: REMOVING CONFLICT SEQUENCES FROM DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 移除冲突序列\n",
    "    cleaned_datasets = {}\n",
    "    removal_stats = {}\n",
    "    \n",
    "    for dataset_name, dataset_info in datasets.items():\n",
    "        original_data = dataset_info['data']\n",
    "        original_count = len(original_data)\n",
    "        \n",
    "        # 移除冲突序列\n",
    "        cleaned_data = original_data[~original_data['Sequence'].str.upper().isin(conflict_sequences)].copy()\n",
    "        cleaned_count = len(cleaned_data)\n",
    "        removed_count = original_count - cleaned_count\n",
    "        \n",
    "        cleaned_datasets[dataset_name] = cleaned_data\n",
    "        removal_stats[dataset_name] = {\n",
    "            'original': original_count,\n",
    "            'cleaned': cleaned_count,\n",
    "            'removed': removed_count,\n",
    "            'removal_rate': (removed_count / original_count * 100) if original_count > 0 else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"   📊 {dataset_name}:\")\n",
    "        print(f\"      Original: {original_count:,} sequences\")\n",
    "        print(f\"      Cleaned: {cleaned_count:,} sequences\")\n",
    "        print(f\"      Removed: {removed_count:,} sequences ({removal_stats[dataset_name]['removal_rate']:.2f}%)\")\n",
    "    \n",
    "    # ========== 第三阶段：去除重复序列 ==========\n",
    "    print(f\"\\n🔄 PHASE 3: REMOVING DUPLICATE SEQUENCES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 提取清理后的数据集\n",
    "    cleaned_tr_avp = cleaned_datasets['Initial_TR_AVP']\n",
    "    cleaned_tr_non_avp = cleaned_datasets['Initial_TR_non_AVP']\n",
    "    cleaned_custom_avp = cleaned_datasets['Initial_TS_AVP']\n",
    "    cleaned_custom_non_avp = cleaned_datasets['Initial_TS_non_AVP']\n",
    "   \n",
    "    # TR数据集的序列集合\n",
    "    tr_avp_sequences = set(cleaned_tr_avp['Sequence'].str.upper())\n",
    "    tr_non_avp_sequences = set(cleaned_tr_non_avp['Sequence'].str.upper())\n",
    "    \n",
    "    print(f\"   🔎 Comparing datasets for duplicates...\")\n",
    "    \n",
    "    # TS_AVP: Initial_TS_AVP中与Initial_TR_AVP不重复的序列\n",
    "    custom_avp_sequences = set(cleaned_custom_avp['Sequence'].str.upper())\n",
    "    ts_avp_sequences = custom_avp_sequences - tr_avp_sequences\n",
    "    ts_avp_data = cleaned_custom_avp[cleaned_custom_avp['Sequence'].str.upper().isin(ts_avp_sequences)].copy()\n",
    "    \n",
    "    # TS_non_AVP: Initial_TS_non_AVP中与Initial_TR_non_AVP不重复的序列\n",
    "    custom_non_avp_sequences = set(cleaned_custom_non_avp['Sequence'].str.upper())\n",
    "    ts_non_avp_sequences = custom_non_avp_sequences - tr_non_avp_sequences\n",
    "    ts_non_avp_data = cleaned_custom_non_avp[cleaned_custom_non_avp['Sequence'].str.upper().isin(ts_non_avp_sequences)].copy()\n",
    "    \n",
    "    # TR数据集保持不变（已清理冲突）\n",
    "    tr_avp_data = cleaned_tr_avp.copy()\n",
    "    tr_non_avp_data = cleaned_tr_non_avp.copy()\n",
    "    \n",
    "    # 统计去重结果\n",
    "    avp_overlap = len(custom_avp_sequences.intersection(tr_avp_sequences))\n",
    "    non_avp_overlap = len(custom_non_avp_sequences.intersection(tr_non_avp_sequences))\n",
    "    \n",
    "    print(f\"\\n   📈 Duplicate removal results:\")\n",
    "    print(f\"      AVP datasets overlap: {avp_overlap:,} sequences\")\n",
    "    print(f\"      non_AVP datasets overlap: {non_avp_overlap:,} sequences\")\n",
    "    print(f\"      TS_AVP (unique custom): {len(ts_avp_data):,} sequences\")\n",
    "    print(f\"      TS_non_AVP (unique custom): {len(ts_non_avp_data):,} sequences\")\n",
    "    print(f\"      TR_AVP (external): {len(tr_avp_data):,} sequences\")\n",
    "    print(f\"      TR_non_AVP (external): {len(tr_non_avp_data):,} sequences\")\n",
    "    \n",
    "    # ========== 第四阶段：保存最终数据集 ==========\n",
    "    print(f\"\\n💾 PHASE 4: SAVING FINAL DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    final_datasets = {\n",
    "        'TS_AVP': ts_avp_data,\n",
    "        'TS_non_AVP': ts_non_avp_data,\n",
    "        'TR_AVP': tr_avp_data,\n",
    "        'TR_non_AVP': tr_non_avp_data\n",
    "    }\n",
    "    \n",
    "    saved_files = {}\n",
    "    columns_order = ['Id', 'Sequence', 'Source', 'Length', 'Label', 'Type']\n",
    "    \n",
    "    for dataset_name, data in final_datasets.items():\n",
    "        if len(data) > 0:\n",
    "            # 确保列顺序\n",
    "            available_columns = [col for col in columns_order if col in data.columns]\n",
    "            data_to_save = data[available_columns].copy()\n",
    "            \n",
    "            filename = f\"{dataset_name}.csv\"\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            data_to_save.to_csv(file_path, index=False)\n",
    "            saved_files[dataset_name] = file_path\n",
    "            \n",
    "            print(f\"   ✅ {dataset_name}: {len(data):,} sequences → {filename}\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  {dataset_name}: No data to save\")\n",
    "    \n",
    "    # ========== 第五阶段：独立性验证 ==========\n",
    "    print(f\"\\n✅ PHASE 5: INDEPENDENCE VERIFICATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 创建最终数据集的序列集合\n",
    "    final_sequences = {}\n",
    "    for name, data in final_datasets.items():\n",
    "        if len(data) > 0:\n",
    "            final_sequences[name] = set(data['Sequence'].str.upper())\n",
    "        else:\n",
    "            final_sequences[name] = set()\n",
    "    \n",
    "    # 验证独立性\n",
    "    verification_passed = True\n",
    "    overlaps_found = []\n",
    "    \n",
    "    dataset_names = list(final_sequences.keys())\n",
    "    for i, name1 in enumerate(dataset_names):\n",
    "        for j, name2 in enumerate(dataset_names):\n",
    "            if i < j:  # 避免重复检查\n",
    "                overlap = final_sequences[name1].intersection(final_sequences[name2])\n",
    "                if len(overlap) > 0:\n",
    "                    verification_passed = False\n",
    "                    overlaps_found.append((name1, name2, len(overlap)))\n",
    "                    print(f\"   ❌ OVERLAP FOUND: {name1} ↔ {name2}: {len(overlap):,} sequences\")\n",
    "                else:\n",
    "                    print(f\"   ✅ {name1} ↔ {name2}: Independent\")\n",
    "    \n",
    "    if verification_passed:\n",
    "        print(f\"\\n   🎉 ALL DATASETS ARE COMPLETELY INDEPENDENT!\")\n",
    "    else:\n",
    "        print(f\"\\n   ⚠️  WARNING: {len(overlaps_found)} overlaps found between final datasets\")\n",
    "    \n",
    "    # ========== 最终统计报告 ==========\n",
    "    print(f\"\\n📋 COMPREHENSIVE FINAL REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\n🔹 LABEL CONFLICT ANALYSIS:\")\n",
    "    print(f\"   ⚠️  Conflicting sequences: {len(conflict_sequences):,}\")\n",
    "    print(f\"   📄 Overlap data file: Overlap_data.csv\")\n",
    "    \n",
    "    print(f\"\\n🔹 DATASET TRANSFORMATION:\")\n",
    "    for name, stats in removal_stats.items():\n",
    "        print(f\"   {name:25s}: {stats['original']:>6,} → {stats['cleaned']:>6,} (-{stats['removed']:,})\")\n",
    "    \n",
    "    print(f\"\\n🔹 FINAL INDEPENDENT DATASETS:\")\n",
    "    total_final = 0\n",
    "    for name, data in final_datasets.items():\n",
    "        count = len(data)\n",
    "        total_final += count\n",
    "        print(f\"   {name:15s}: {count:>6,} sequences\")\n",
    "    \n",
    "    print(f\"   {'─' * 40}\")\n",
    "    print(f\"   {'Total Final':15s}: {total_final:>6,} sequences\")\n",
    "    \n",
    "    print(f\"\\n🔹 DATA REDUCTION SUMMARY:\")\n",
    "    total_initial = sum(len(dataset['data']) for dataset in datasets.values())\n",
    "    reduction = total_initial - total_final\n",
    "    reduction_rate = (reduction / total_initial * 100) if total_initial > 0 else 0\n",
    "    print(f\"   Initial total: {total_initial:,} sequences\")\n",
    "    print(f\"   Final total: {total_final:,} sequences\")\n",
    "    print(f\"   Reduction: {reduction:,} sequences ({reduction_rate:.2f}%)\")\n",
    "    \n",
    "    final_stats = {\n",
    "        'label_conflicts': {\n",
    "            'conflict_sequences_count': len(conflict_sequences),\n",
    "            'conflict_records_count': len(label_conflicts)\n",
    "        },\n",
    "        'removal_stats': removal_stats,\n",
    "        'final_datasets': {name: len(data) for name, data in final_datasets.items()},\n",
    "        'independence_verification': {\n",
    "            'passed': verification_passed,\n",
    "            'overlaps_found': overlaps_found\n",
    "        },\n",
    "        'summary': {\n",
    "            'initial_total': total_initial,\n",
    "            'final_total': total_final,\n",
    "            'reduction': reduction,\n",
    "            'reduction_rate': reduction_rate\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    # ✅ 修改：保存到日志目录\n",
    "    stats_file = os.path.join(log_dir, \"step2_overlap_analysis_statistics.json\")\n",
    "    \n",
    "    # ✅ 添加：更详细的日志信息\n",
    "    detailed_log = {\n",
    "        'step': 'Step 2 - Overlap Analysis and Dataset Independence',\n",
    "        'statistics': final_stats,\n",
    "        'saved_files': saved_files,\n",
    "        'verification_passed': verification_passed\n",
    "    }\n",
    "    \n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(detailed_log, f, indent=2, default=str)\n",
    "    print(f\"\\n   📄 Statistics saved to: {stats_file}\")\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'final_datasets': final_datasets,\n",
    "        'label_conflicts': label_conflicts,\n",
    "        'conflict_sequences': conflict_sequences,\n",
    "        'removal_stats': removal_stats,\n",
    "        'saved_files': saved_files,\n",
    "        'verification_passed': verification_passed,\n",
    "        'final_stats': final_stats\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# 运行第二步分析\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n🚀 Starting Step 2: Enhanced Overlap Analysis and Dataset Independence\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # 确保第一步已完成\n",
    "    if 'step1_detailed_results' not in locals():\n",
    "        print(\"❌ Error: Step 1 results not found. Please run Step 1 first.\")\n",
    "    else:\n",
    "        step2_results = step2_overlap_analysis_enhanced(step1_detailed_results)\n",
    "        \n",
    "        if step2_results:\n",
    "            print(\"\\n✅ Step 2 completed successfully!\")\n",
    "            print(f\"   📁 Results stored in step2_results variable\")\n",
    "            \n",
    "            if step2_results['verification_passed']:\n",
    "                print(f\"   🎉 All final datasets are completely independent!\")\n",
    "            else:\n",
    "                print(f\"   ⚠️  Warning: Some overlaps detected in final datasets\")\n",
    "            \n",
    "            print(f\"   💾 Final datasets saved to: 1_Data/Processed_data_set/Final_merged_data_set/\")\n",
    "            print(f\"   📊 Files created:\")\n",
    "            for dataset_name, file_path in step2_results['saved_files'].items():\n",
    "                print(f\"      - {dataset_name}.csv\")\n",
    "            print(f\"      - Overlap_data.csv (label conflicts)\")\n",
    "            print(f\"      - processing_statistics.json (detailed stats)\")\n",
    "        else:\n",
    "            print(\"❌ Step 2 failed\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during Step 2: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n🎯 Step 2 Complete - Final independent datasets ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17607b26",
   "metadata": {},
   "source": [
    "# 3.csv to fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4140caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Converting TR datasets to CSV and FASTA formats\n",
      "======================================================================\n",
      "📊 Reading CSV files:\n",
      "--------------------------------------------------\n",
      "   ✅ TR_AVP.csv: 3,091 sequences\n",
      "   ✅ TR_non_AVP.csv: 3,719 sequences\n",
      "\n",
      "📊 Combined dataset: 6,810 sequences\n",
      "\n",
      "🔧 ID Duplicate Check and Fix:\n",
      "--------------------------------------------------\n",
      "🔍 Checking for duplicate IDs...\n",
      "   ⚠️  Found 76 duplicate ID groups affecting 152 sequences\n",
      "   📋 Duplicate ID examples:\n",
      "      'neg1786': 2 occurrences\n",
      "      'neg1762': 2 occurrences\n",
      "      'neg1761': 2 occurrences\n",
      "      'neg1740': 2 occurrences\n",
      "      'neg1703': 2 occurrences\n",
      "      ... and 71 more\n",
      "   ✅ Applied 76 ID fixes\n",
      "   📝 ID fix examples:\n",
      "      neg2124 → neg2124_1\n",
      "      neg105 → neg105_1\n",
      "      neg171 → neg171_1\n",
      "      neg299 → neg299_1\n",
      "      neg560 → neg560_1\n",
      "      ... and 71 more fixes\n",
      "\n",
      "🔀 Shuffling data with random seed 42...\n",
      "\n",
      "💾 Saving CSV file: 1_Data/Processed_data_set/Final_merged_data_set/TR.csv\n",
      "💾 Saving FASTA file: 1_Data/Processed_data_set/Final_merged_data_set/TR.fasta\n",
      "\n",
      "📈 Final Statistics:\n",
      "   🔧 ID Duplicate Processing:\n",
      "      Original duplicates: 152 sequences in 76 groups\n",
      "      Fixes applied: 76\n",
      "      Final unique IDs: 6,810\n",
      "   📊 By dataset:\n",
      "      TR_non_AVP: 3,719 sequences\n",
      "      TR_AVP: 3,091 sequences\n",
      "   📊 By label:\n",
      "      0: 3,719 sequences\n",
      "      1: 3,091 sequences\n",
      "   📊 By type:\n",
      "      non_AVP: 3,719 sequences\n",
      "      AVP: 3,091 sequences\n",
      "   📊 Total: 6,810 sequences\n",
      "   ✅ CSV saved: 1_Data/Processed_data_set/Final_merged_data_set/TR.csv\n",
      "   ✅ FASTA saved: 1_Data/Processed_data_set/Final_merged_data_set/TR.fasta\n",
      "   📄 Conversion log saved to: 2_Log/2.1_Training set_and_test_set_processing/step3_TR_conversion_log.json\n",
      "\n",
      "🚀 Converting TS datasets to CSV and FASTA formats\n",
      "======================================================================\n",
      "📊 Reading CSV files:\n",
      "--------------------------------------------------\n",
      "   ✅ TS_AVP.csv: 1,403 sequences\n",
      "   ✅ TS_non_AVP.csv: 7,894 sequences\n",
      "\n",
      "📊 Combined dataset: 9,297 sequences\n",
      "\n",
      "🔧 ID Duplicate Check and Fix:\n",
      "--------------------------------------------------\n",
      "🔍 Checking for duplicate IDs...\n",
      "   ⚠️  Found 2 duplicate ID groups affecting 4 sequences\n",
      "   📋 Duplicate ID examples:\n",
      "      '22846': 2 occurrences\n",
      "      '5102': 2 occurrences\n",
      "   ✅ Applied 2 ID fixes\n",
      "   📝 ID fix examples:\n",
      "      22846 → 22846_1\n",
      "      5102 → 5102_1\n",
      "\n",
      "🔀 Shuffling data with random seed 42...\n",
      "\n",
      "💾 Saving CSV file: 1_Data/Processed_data_set/Final_merged_data_set/TS.csv\n",
      "💾 Saving FASTA file: 1_Data/Processed_data_set/Final_merged_data_set/TS.fasta\n",
      "\n",
      "📈 Final Statistics:\n",
      "   🔧 ID Duplicate Processing:\n",
      "      Original duplicates: 4 sequences in 2 groups\n",
      "      Fixes applied: 2\n",
      "      Final unique IDs: 9,297\n",
      "   📊 By dataset:\n",
      "      TS_non_AVP: 7,894 sequences\n",
      "      TS_AVP: 1,403 sequences\n",
      "   📊 By label:\n",
      "      0: 7,894 sequences\n",
      "      1: 1,403 sequences\n",
      "   📊 By type:\n",
      "      non_AVP: 7,894 sequences\n",
      "      AVP: 1,403 sequences\n",
      "   📊 Total: 9,297 sequences\n",
      "   ✅ CSV saved: 1_Data/Processed_data_set/Final_merged_data_set/TS.csv\n",
      "   ✅ FASTA saved: 1_Data/Processed_data_set/Final_merged_data_set/TS.fasta\n",
      "   📄 Conversion log saved to: 2_Log/2.1_Training set_and_test_set_processing/step3_TS_conversion_log.json\n",
      "\n",
      "📋 CONVERSION SUMMARY\n",
      "======================================================================\n",
      "📊 TR Dataset:\n",
      "   ✅ TR.csv: 6,810 sequences (6,810 unique IDs)\n",
      "   ✅ TR.fasta: 6,810 sequences\n",
      "\n",
      "📊 TS Dataset:\n",
      "   ✅ TS.csv: 9,297 sequences (9,297 unique IDs)\n",
      "   ✅ TS.fasta: 9,297 sequences\n",
      "\n",
      "📁 All files saved in: 1_Data/Processed_data_set/Final_merged_data_set\n",
      "🎲 Random seed used: 42\n",
      "\n",
      "🎯 Conversion complete! All files generated with random seed 42.\n",
      "💡 Duplicate IDs have been automatically resolved with numeric suffixes (_1, _2, etc.).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def check_and_fix_duplicate_ids(df):\n",
    "    \"\"\"\n",
    "    检查并修复重复的ID，为重复ID添加数字后缀\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with columns including 'Id', 'Source', 'Dataset'\n",
    "    \n",
    "    Returns:\n",
    "    df: DataFrame with fixed IDs\n",
    "    duplicate_info: dict with duplicate statistics\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Checking for duplicate IDs...\")\n",
    "    \n",
    "    # 记录原始ID和修复后的ID\n",
    "    original_ids = df['Id'].copy()\n",
    "    id_counts = defaultdict(int)\n",
    "    fixed_ids = []\n",
    "    duplicate_info = {\n",
    "        'total_duplicates': 0,\n",
    "        'duplicate_groups': 0,\n",
    "        'fixes_applied': 0\n",
    "    }\n",
    "    \n",
    "    # 第一次遍历：统计每个ID的出现次数\n",
    "    for idx, row in df.iterrows():\n",
    "        original_id = str(row['Id']) if pd.notna(row['Id']) else f\"Unknown_{idx}\"\n",
    "        id_counts[original_id] += 1\n",
    "    \n",
    "    # 找出重复的ID\n",
    "    duplicate_ids = {id_val: count for id_val, count in id_counts.items() if count > 1}\n",
    "    \n",
    "    if duplicate_ids:\n",
    "        duplicate_info['duplicate_groups'] = len(duplicate_ids)\n",
    "        duplicate_info['total_duplicates'] = sum(duplicate_ids.values())\n",
    "        \n",
    "        print(f\"   ⚠️  Found {len(duplicate_ids)} duplicate ID groups affecting {sum(duplicate_ids.values())} sequences\")\n",
    "        \n",
    "        # 显示重复ID示例\n",
    "        print(f\"   📋 Duplicate ID examples:\")\n",
    "        for i, (dup_id, count) in enumerate(list(duplicate_ids.items())[:5]):\n",
    "            print(f\"      '{dup_id}': {count} occurrences\")\n",
    "        if len(duplicate_ids) > 5:\n",
    "            print(f\"      ... and {len(duplicate_ids) - 5} more\")\n",
    "    else:\n",
    "        print(f\"   ✅ No duplicate IDs found\")\n",
    "        return df, duplicate_info\n",
    "    \n",
    "    # 第二次遍历：修复重复ID\n",
    "    occurrence_counter = defaultdict(int)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        original_id = str(row['Id']) if pd.notna(row['Id']) else f\"Unknown_{idx}\"\n",
    "        \n",
    "        if original_id in duplicate_ids:\n",
    "            occurrence_counter[original_id] += 1\n",
    "            \n",
    "            if occurrence_counter[original_id] == 1:\n",
    "                # 第一次出现，保持原ID\n",
    "                fixed_id = original_id\n",
    "            else:\n",
    "                # 后续出现，添加数字后缀\n",
    "                suffix_number = occurrence_counter[original_id] - 1  # 从_1开始\n",
    "                \n",
    "                # 如果原ID以|结尾，在|前添加后缀\n",
    "                if original_id.endswith('|'):\n",
    "                    fixed_id = f\"{original_id[:-1]}_{suffix_number}|\"\n",
    "                else:\n",
    "                    fixed_id = f\"{original_id}_{suffix_number}\"\n",
    "                \n",
    "                duplicate_info['fixes_applied'] += 1\n",
    "        else:\n",
    "            # 非重复ID，保持原样\n",
    "            fixed_id = original_id\n",
    "        \n",
    "        fixed_ids.append(fixed_id)\n",
    "    \n",
    "    # 更新DataFrame\n",
    "    df = df.copy()\n",
    "    df['Id'] = fixed_ids\n",
    "    \n",
    "    print(f\"   ✅ Applied {duplicate_info['fixes_applied']} ID fixes\")\n",
    "    \n",
    "    # 显示修复示例\n",
    "    if duplicate_info['fixes_applied'] > 0:\n",
    "        print(f\"   📝 ID fix examples:\")\n",
    "        fix_count = 0\n",
    "        for i, (orig, fixed) in enumerate(zip(original_ids, fixed_ids)):\n",
    "            if orig != fixed and fix_count < 5:\n",
    "                print(f\"      {orig} → {fixed}\")\n",
    "                fix_count += 1\n",
    "        if duplicate_info['fixes_applied'] > 5:\n",
    "            print(f\"      ... and {duplicate_info['fixes_applied'] - 5} more fixes\")\n",
    "    \n",
    "    return df, duplicate_info\n",
    "\n",
    "def merge_datasets_to_both_formats(csv_files, output_base_path, random_seed=42):\n",
    "    \"\"\"\n",
    "    将多个CSV文件合并为FASTA和CSV两种格式，保留所有信息并打乱顺序\n",
    "    处理重复ID问题\n",
    "    \n",
    "    Parameters:\n",
    "    csv_files: list of dict, 包含文件路径和标签信息\n",
    "    output_base_path: str, 输出文件的基础路径（不含扩展名）\n",
    "    random_seed: int, 随机种子\n",
    "    \"\"\"\n",
    "    # ✅ 添加：创建日志目录\n",
    "    log_dir = \"2_Log/2.1_Training set_and_test_set_processing\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    print(f\"📊 Reading CSV files:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 读取每个CSV文件\n",
    "    for file_info in csv_files:\n",
    "        csv_path = file_info['path']\n",
    "        dataset_type = file_info['type']\n",
    "        \n",
    "        if os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path)\n",
    "            \n",
    "            print(f\"   ✅ {os.path.basename(csv_path)}: {len(df):,} sequences\")\n",
    "            \n",
    "            # 添加数据集类型信息\n",
    "            df = df.copy()\n",
    "            df['Dataset'] = dataset_type\n",
    "            \n",
    "            # 将数据添加到总列表\n",
    "            all_data.append(df)\n",
    "        else:\n",
    "            print(f\"   ❌ File not found: {csv_path}\")\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"❌ No data found!\")\n",
    "        return False, False\n",
    "    \n",
    "    # 合并所有数据\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n📊 Combined dataset: {len(combined_df):,} sequences\")\n",
    "    \n",
    "    # ========== 检查和修复重复ID ==========\n",
    "    print(f\"\\n🔧 ID Duplicate Check and Fix:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    fixed_df, duplicate_info = check_and_fix_duplicate_ids(combined_df)\n",
    "    \n",
    "    # 设置随机种子并打乱顺序\n",
    "    print(f\"\\n🔀 Shuffling data with random seed {random_seed}...\")\n",
    "    np.random.seed(random_seed)\n",
    "    shuffled_df = fixed_df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    \n",
    "    # 生成输出路径\n",
    "    csv_output_path = f\"{output_base_path}.csv\"\n",
    "    fasta_output_path = f\"{output_base_path}.fasta\"\n",
    "    \n",
    "    # ========== 保存CSV文件 ==========\n",
    "    print(f\"\\n💾 Saving CSV file: {csv_output_path}\")\n",
    "    \n",
    "    # 确保列顺序\n",
    "    desired_columns = ['Id', 'Sequence', 'Source', 'Length', 'Label', 'Type', 'Dataset']\n",
    "    available_columns = [col for col in desired_columns if col in shuffled_df.columns]\n",
    "    \n",
    "    # 保存CSV\n",
    "    shuffled_df[available_columns].to_csv(csv_output_path, index=False)\n",
    "    csv_success = True\n",
    "    \n",
    "    # ========== 保存FASTA文件 ==========\n",
    "    print(f\"💾 Saving FASTA file: {fasta_output_path}\")\n",
    "    \n",
    "    fasta_success = True\n",
    "    try:\n",
    "        with open(fasta_output_path, 'w') as fasta_file:\n",
    "            for _, row in shuffled_df.iterrows():\n",
    "                # 构建FASTA header，包含所有信息\n",
    "                header_parts = []\n",
    "                \n",
    "                # 添加所有可用信息\n",
    "                if 'Id' in row and pd.notna(row['Id']):\n",
    "                    header_parts.append(f\"ID={row['Id']}\")\n",
    "                \n",
    "                if 'Label' in row and pd.notna(row['Label']):\n",
    "                    header_parts.append(f\"Label={row['Label']}\")\n",
    "                \n",
    "                if 'Type' in row and pd.notna(row['Type']):\n",
    "                    header_parts.append(f\"Type={row['Type']}\")\n",
    "                \n",
    "                if 'Source' in row and pd.notna(row['Source']):\n",
    "                    header_parts.append(f\"Source={row['Source']}\")\n",
    "                \n",
    "                if 'Length' in row and pd.notna(row['Length']):\n",
    "                    header_parts.append(f\"Length={row['Length']}\")\n",
    "                \n",
    "                if 'Dataset' in row and pd.notna(row['Dataset']):\n",
    "                    header_parts.append(f\"Dataset={row['Dataset']}\")\n",
    "                \n",
    "                # 构建完整的FASTA header\n",
    "                fasta_header = '|'.join(header_parts)\n",
    "                \n",
    "                # 获取序列\n",
    "                sequence = str(row['Sequence']) if pd.notna(row['Sequence']) else ''\n",
    "                \n",
    "                # 写入FASTA格式\n",
    "                fasta_file.write(f\">{fasta_header}\\n\")\n",
    "                fasta_file.write(f\"{sequence}\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error writing FASTA file: {e}\")\n",
    "        fasta_success = False\n",
    "    \n",
    "    # ========== 统计信息 ==========\n",
    "    print(f\"\\n📈 Final Statistics:\")\n",
    "    \n",
    "    # ID重复处理统计\n",
    "    if duplicate_info['total_duplicates'] > 0:\n",
    "        print(f\"   🔧 ID Duplicate Processing:\")\n",
    "        print(f\"      Original duplicates: {duplicate_info['total_duplicates']} sequences in {duplicate_info['duplicate_groups']} groups\")\n",
    "        print(f\"      Fixes applied: {duplicate_info['fixes_applied']}\")\n",
    "        print(f\"      Final unique IDs: {shuffled_df['Id'].nunique():,}\")\n",
    "    \n",
    "    # 按数据集统计\n",
    "    dataset_counts = shuffled_df['Dataset'].value_counts()\n",
    "    print(f\"   📊 By dataset:\")\n",
    "    for dataset, count in dataset_counts.items():\n",
    "        print(f\"      {dataset}: {count:,} sequences\")\n",
    "    \n",
    "    # 按标签统计\n",
    "    if 'Label' in shuffled_df.columns:\n",
    "        label_counts = shuffled_df['Label'].value_counts()\n",
    "        print(f\"   📊 By label:\")\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"      {label}: {count:,} sequences\")\n",
    "    \n",
    "    # 按类型统计\n",
    "    if 'Type' in shuffled_df.columns:\n",
    "        type_counts = shuffled_df['Type'].value_counts()\n",
    "        print(f\"   📊 By type:\")\n",
    "        for type_name, count in type_counts.items():\n",
    "            print(f\"      {type_name}: {count:,} sequences\")\n",
    "    \n",
    "    print(f\"   📊 Total: {len(shuffled_df):,} sequences\")\n",
    "    \n",
    "    if csv_success:\n",
    "        print(f\"   ✅ CSV saved: {csv_output_path}\")\n",
    "    if fasta_success:\n",
    "        print(f\"   ✅ FASTA saved: {fasta_output_path}\")\n",
    "    \n",
    "    # ✅ 添加：保存转换日志\n",
    "    import json\n",
    "    \n",
    "    conversion_log = {\n",
    "        'step': 'Step 3 - CSV to FASTA Conversion',\n",
    "        'output_base_path': output_base_path,\n",
    "        'random_seed': random_seed,\n",
    "        'csv_success': csv_success,\n",
    "        'fasta_success': fasta_success,\n",
    "        'duplicate_info': duplicate_info,\n",
    "        'final_statistics': {\n",
    "            'total_sequences': len(shuffled_df),\n",
    "            'unique_ids': shuffled_df['Id'].nunique(),\n",
    "            'dataset_counts': dataset_counts.to_dict(),\n",
    "            'label_counts': label_counts.to_dict() if 'Label' in shuffled_df.columns else {},\n",
    "            'type_counts': type_counts.to_dict() if 'Type' in shuffled_df.columns else {}\n",
    "        },\n",
    "        'files_created': {\n",
    "            'csv_file': csv_output_path if csv_success else None,\n",
    "            'fasta_file': fasta_output_path if fasta_success else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 确定日志文件名\n",
    "    dataset_name = os.path.basename(output_base_path)\n",
    "    log_file = os.path.join(log_dir, f\"step3_{dataset_name}_conversion_log.json\")\n",
    "    \n",
    "    with open(log_file, 'w') as f:\n",
    "        json.dump(conversion_log, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"   📄 Conversion log saved to: {log_file}\")\n",
    "    \n",
    "    return csv_success, fasta_success\n",
    "\n",
    "# ============================================================================\n",
    "# 合并TR数据集 (TR_AVP.csv + TR_non_AVP.csv → TR.csv + TR.fasta)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"🚀 Converting TR datasets to CSV and FASTA formats\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "base_dir = \"1_Data/Processed_data_set/Final_merged_data_set\"\n",
    "\n",
    "# TR数据集配置\n",
    "tr_csv_files = [\n",
    "    {\n",
    "        'path': os.path.join(base_dir, \"TR_AVP.csv\"),\n",
    "        'type': 'TR_AVP'\n",
    "    },\n",
    "    {\n",
    "        'path': os.path.join(base_dir, \"TR_non_AVP.csv\"),\n",
    "        'type': 'TR_non_AVP'\n",
    "    }\n",
    "]\n",
    "\n",
    "tr_output_base = os.path.join(base_dir, \"TR\")\n",
    "\n",
    "# 转换TR数据集\n",
    "tr_csv_success, tr_fasta_success = merge_datasets_to_both_formats(\n",
    "    tr_csv_files, tr_output_base, random_seed=42\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# 合并TS数据集 (TS_AVP.csv + TS_non_AVP.csv → TS.csv + TS.fasta)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n🚀 Converting TS datasets to CSV and FASTA formats\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TS数据集配置\n",
    "ts_csv_files = [\n",
    "    {\n",
    "        'path': os.path.join(base_dir, \"TS_AVP.csv\"),\n",
    "        'type': 'TS_AVP'\n",
    "    },\n",
    "    {\n",
    "        'path': os.path.join(base_dir, \"TS_non_AVP.csv\"),\n",
    "        'type': 'TS_non_AVP'\n",
    "    }\n",
    "]\n",
    "\n",
    "ts_output_base = os.path.join(base_dir, \"TS\")\n",
    "\n",
    "# 转换TS数据集\n",
    "ts_csv_success, ts_fasta_success = merge_datasets_to_both_formats(\n",
    "    ts_csv_files, ts_output_base, random_seed=42\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# 总结报告\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n📋 CONVERSION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TR数据集结果\n",
    "print(f\"📊 TR Dataset:\")\n",
    "if tr_csv_success:\n",
    "    tr_csv_path = os.path.join(base_dir, \"TR.csv\")\n",
    "    if os.path.exists(tr_csv_path):\n",
    "        tr_df = pd.read_csv(tr_csv_path)\n",
    "        unique_ids = tr_df['Id'].nunique()\n",
    "        total_rows = len(tr_df)\n",
    "        print(f\"   ✅ TR.csv: {total_rows:,} sequences ({unique_ids:,} unique IDs)\")\n",
    "    else:\n",
    "        print(f\"   ❌ TR.csv: File not found\")\n",
    "else:\n",
    "    print(f\"   ❌ TR.csv: Failed to create\")\n",
    "\n",
    "if tr_fasta_success:\n",
    "    tr_fasta_path = os.path.join(base_dir, \"TR.fasta\")\n",
    "    if os.path.exists(tr_fasta_path):\n",
    "        with open(tr_fasta_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        seq_count = len([line for line in lines if line.startswith('>')])\n",
    "        print(f\"   ✅ TR.fasta: {seq_count:,} sequences\")\n",
    "    else:\n",
    "        print(f\"   ❌ TR.fasta: File not found\")\n",
    "else:\n",
    "    print(f\"   ❌ TR.fasta: Failed to create\")\n",
    "\n",
    "# TS数据集结果\n",
    "print(f\"\\n📊 TS Dataset:\")\n",
    "if ts_csv_success:\n",
    "    ts_csv_path = os.path.join(base_dir, \"TS.csv\")\n",
    "    if os.path.exists(ts_csv_path):\n",
    "        ts_df = pd.read_csv(ts_csv_path)\n",
    "        unique_ids = ts_df['Id'].nunique()\n",
    "        total_rows = len(ts_df)\n",
    "        print(f\"   ✅ TS.csv: {total_rows:,} sequences ({unique_ids:,} unique IDs)\")\n",
    "    else:\n",
    "        print(f\"   ❌ TS.csv: File not found\")\n",
    "else:\n",
    "    print(f\"   ❌ TS.csv: Failed to create\")\n",
    "\n",
    "if ts_fasta_success:\n",
    "    ts_fasta_path = os.path.join(base_dir, \"TS.fasta\")\n",
    "    if os.path.exists(ts_fasta_path):\n",
    "        with open(ts_fasta_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        seq_count = len([line for line in lines if line.startswith('>')])\n",
    "        print(f\"   ✅ TS.fasta: {seq_count:,} sequences\")\n",
    "    else:\n",
    "        print(f\"   ❌ TS.fasta: File not found\")\n",
    "else:\n",
    "    print(f\"   ❌ TS.fasta: Failed to create\")\n",
    "\n",
    "print(f\"\\n📁 All files saved in: {base_dir}\")\n",
    "print(f\"🎲 Random seed used: 42\")\n",
    "\n",
    "print(f\"\\n🎯 Conversion complete! All files generated with random seed 42.\")\n",
    "print(f\"💡 Duplicate IDs have been automatically resolved with numeric suffixes (_1, _2, etc.).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipynb (esmc)",
   "language": "python",
   "name": "esmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
