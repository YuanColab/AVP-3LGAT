{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c931b0f5",
   "metadata": {},
   "source": [
    "# 1.å®šä¹‰å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6c1516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Step 1: Detailed Dataset Processing and Statistics\n",
      "================================================================================\n",
      "ğŸš€ STEP 1: DETAILED PROCESSING OF INTERNAL AND EXTERNAL DATASETS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š PROCESSING INTERNAL AVP DATASETS\n",
      "================================================================================\n",
      "   ğŸ”¹ Processing individual internal AVP datasets:\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing: DRAVP\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 1,986\n",
      "            Step 1 - After natural AA filter: 1,756 (removed: 230)\n",
      "            Step 2 - After length filter (5-50): 1,714\n",
      "                   â”œâ”€ Too short (<5): 17\n",
      "                   â””â”€ Too long (>50): 25\n",
      "            Step 3 - After deduplication: 1,608 (removed duplicates: 106)\n",
      "            âœ… Verification: Expected 1,608, Got 1,608\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "      âœ… DRAVP completed: 1,986 â†’ 1,608 sequences\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing: AVPdb\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 2,059\n",
      "            Step 1 - After natural AA filter: 2,058 (removed: 1)\n",
      "            Step 2 - After length filter (5-50): 2,024\n",
      "                   â”œâ”€ Too short (<5): 15\n",
      "                   â””â”€ Too long (>50): 19\n",
      "            Step 3 - After deduplication: 1,784 (removed duplicates: 240)\n",
      "            âœ… Verification: Expected 1,784, Got 1,784\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "      âœ… AVPdb completed: 2,059 â†’ 1,784 sequences\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing: ACovPepDB\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 518\n",
      "            Step 1 - After natural AA filter: 381 (removed: 137)\n",
      "            Step 2 - After length filter (5-50): 350\n",
      "                   â”œâ”€ Too short (<5): 0\n",
      "                   â””â”€ Too long (>50): 31\n",
      "            Step 3 - After deduplication: 149 (removed duplicates: 201)\n",
      "            âœ… Verification: Expected 149, Got 149\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "      âœ… ACovPepDB completed: 518 â†’ 149 sequences\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing: HIPdb\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 981\n",
      "            Step 1 - After natural AA filter: 979 (removed: 2)\n",
      "            Step 2 - After length filter (5-50): 950\n",
      "                   â”œâ”€ Too short (<5): 23\n",
      "                   â””â”€ Too long (>50): 6\n",
      "            Step 3 - After deduplication: 858 (removed duplicates: 92)\n",
      "            âœ… Verification: Expected 858, Got 858\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "      âœ… HIPdb completed: 981 â†’ 858 sequences\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing: CAMP\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 1,087\n",
      "            Step 1 - After natural AA filter: 951 (removed: 136)\n",
      "            Step 2 - After length filter (5-50): 905\n",
      "                   â”œâ”€ Too short (<5): 16\n",
      "                   â””â”€ Too long (>50): 30\n",
      "            Step 3 - After deduplication: 898 (removed duplicates: 7)\n",
      "            âœ… Verification: Expected 898, Got 898\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "      âœ… CAMP completed: 1,087 â†’ 898 sequences\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing: DBAASP\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 1,488\n",
      "            Step 1 - After natural AA filter: 1,263 (removed: 225)\n",
      "            Step 2 - After length filter (5-50): 1,220\n",
      "                   â”œâ”€ Too short (<5): 19\n",
      "                   â””â”€ Too long (>50): 24\n",
      "            Step 3 - After deduplication: 1,142 (removed duplicates: 78)\n",
      "            âœ… Verification: Expected 1,142, Got 1,142\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "      âœ… DBAASP completed: 1,488 â†’ 1,142 sequences\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing: DRAMP\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 2,004\n",
      "            Step 1 - After natural AA filter: 1,771 (removed: 233)\n",
      "            Step 2 - After length filter (5-50): 1,672\n",
      "                   â”œâ”€ Too short (<5): 17\n",
      "                   â””â”€ Too long (>50): 82\n",
      "            Step 3 - After deduplication: 1,575 (removed duplicates: 97)\n",
      "            âœ… Verification: Expected 1,575, Got 1,575\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "      âœ… DRAMP completed: 2,004 â†’ 1,575 sequences\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing: dbAMP_AntiHIV\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         ğŸ“„ Parsing FASTA file: dbAMP_AntiHIV_2024.fasta\n",
      "            âœ“ 1024 records, length range: 2-137\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 1,024\n",
      "            Step 1 - After natural AA filter: 1,024 (removed: 0)\n",
      "            Step 2 - After length filter (5-50): 972\n",
      "                   â”œâ”€ Too short (<5): 23\n",
      "                   â””â”€ Too long (>50): 29\n",
      "            Step 3 - After deduplication: 970 (removed duplicates: 2)\n",
      "            âœ… Verification: Expected 970, Got 970\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "      âœ… dbAMP_AntiHIV completed: 1,024 â†’ 970 sequences\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing: dbAMP_Antiviral\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         ğŸ“„ Parsing FASTA file: dbAMP_Antiviral_2024.fasta\n",
      "            âœ“ 2018 records, length range: 2-586\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 2,018\n",
      "            Step 1 - After natural AA filter: 1,891 (removed: 127)\n",
      "            Step 2 - After length filter (5-50): 1,691\n",
      "                   â”œâ”€ Too short (<5): 27\n",
      "                   â””â”€ Too long (>50): 173\n",
      "            Step 3 - After deduplication: 1,691 (removed duplicates: 0)\n",
      "            âœ… Verification: Expected 1,691, Got 1,691\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "      âœ… dbAMP_Antiviral completed: 2,018 â†’ 1,691 sequences\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing: Peptipedia\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         ğŸ“„ Parsing FASTA file: Peptipedia_Antiviral.fasta\n",
      "            âœ“ 5899 records, length range: 2-363\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 5,899\n",
      "            Step 1 - After natural AA filter: 5,177 (removed: 722)\n",
      "            Step 2 - After length filter (5-50): 4,856\n",
      "                   â”œâ”€ Too short (<5): 85\n",
      "                   â””â”€ Too long (>50): 236\n",
      "            Step 3 - After deduplication: 4,856 (removed duplicates: 0)\n",
      "            âœ… Verification: Expected 4,856, Got 4,856\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "      âœ… Peptipedia completed: 5,899 â†’ 4,856 sequences\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ”— Merging internal AVP datasets\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“ˆ Total sequences before merge: 15,531\n",
      "         â””â”€ DRAVP: 1,608 sequences\n",
      "         â””â”€ AVPdb: 1,784 sequences\n",
      "         â””â”€ ACovPepDB: 149 sequences\n",
      "         â””â”€ HIPdb: 858 sequences\n",
      "         â””â”€ CAMP: 898 sequences\n",
      "         â””â”€ DBAASP: 1,142 sequences\n",
      "         â””â”€ DRAMP: 1,575 sequences\n",
      "         â””â”€ dbAMP_AntiHIV: 970 sequences\n",
      "         â””â”€ dbAMP_Antiviral: 1,691 sequences\n",
      "         â””â”€ Peptipedia: 4,856 sequences\n",
      "      ğŸ“‰ After final deduplication: 4,993 sequences\n",
      "      ğŸ—‘ï¸ Final duplicates removed: 10,538\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“Š PROCESSING INTERNAL non_AVP DATASETS\n",
      "================================================================================\n",
      "   ğŸ”¹ Processing UniProt non_AVP dataset:\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing: UniProt\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         ğŸ“„ Parsing FASTA file: uniprotkb_13124.fasta\n",
      "            âœ“ 13124 records, length range: 5-50\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 13,124\n",
      "            Step 1 - After natural AA filter: 12,513 (removed: 611)\n",
      "            Step 2 - After length filter (5-50): 12,513\n",
      "                   â”œâ”€ Too short (<5): 0\n",
      "                   â””â”€ Too long (>50): 0\n",
      "            Step 3 - After deduplication: 9,402 (removed duplicates: 3,111)\n",
      "            âœ… Verification: Expected 9,402, Got 9,402\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "      âœ… UniProt completed: 13,124 â†’ 9,402 sequences\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“ ID format examples:\n",
      "         sp|A0A068B6Q6|CA18_CONBE Conotoxin Bt1.8 (Fragment) OS=Conus betulinus OX=89764 PE=1 SV=1 â†’ sp|A0A068B6Q6|\n",
      "         sp|A0A0C5B5G6|MOTSC_HUMAN Mitochondrial-derived peptide MOTS-c OS=Homo sapiens OX=9606 GN=MT-RNR1 PE=1 SV=1 â†’ sp|A0A0C5B5G6|\n",
      "         sp|A0A2R8VHR8|DT3UO_MOUSE DDIT3 upstream open reading frame protein OS=Mus musculus OX=10090 GN=Ddit3 PE=2 SV=1 â†’ sp|A0A2R8VHR8|\n",
      "         sp|A0A4V8GZX0|TXNA1_OMOSC Mu-theraphotoxin-Os1a OS=Omothymus schioedtei OX=1046902 PE=1 SV=2 â†’ sp|A0A4V8GZX0|\n",
      "         sp|A5A616|MGTS_ECOLI Small protein MgtS OS=Escherichia coli (strain K12) OX=83333 GN=mgtS PE=1 SV=1 â†’ sp|A5A616|\n",
      "      âœ… Processed 9402 UniProt IDs to short format\n",
      "\n",
      "ğŸ“Š PROCESSING EXTERNAL DATASETS\n",
      "================================================================================\n",
      "   ğŸ”¹ Processing external AVP datasets:\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing: Stack-AVP-TR_pos\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         ğŸ“„ Parsing FASTA file: Stack-AVP-TR_pos.fasta\n",
      "            âœ“ 2290 records, length range: 5-50\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 2,290\n",
      "            Step 1 - After natural AA filter: 2,290 (removed: 0)\n",
      "            Step 2 - After length filter (5-50): 2,290\n",
      "                   â”œâ”€ Too short (<5): 0\n",
      "                   â””â”€ Too long (>50): 0\n",
      "            Step 3 - After deduplication: 2,290 (removed duplicates: 0)\n",
      "            âœ… Verification: Expected 2,290, Got 2,290\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "      âœ… Stack-AVP-TR_pos completed: 2,290 â†’ 2,290 sequences\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ”¹ Processing external non_AVP datasets:\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing: Stack-AVP-TR_neg\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         ğŸ“„ Parsing FASTA file: Stack-AVP-TR_neg.fasta\n",
      "            âœ“ 2311 records, length range: 6-50\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 2,311\n",
      "            Step 1 - After natural AA filter: 2,311 (removed: 0)\n",
      "            Step 2 - After length filter (5-50): 2,311\n",
      "                   â”œâ”€ Too short (<5): 0\n",
      "                   â””â”€ Too long (>50): 0\n",
      "            Step 3 - After deduplication: 2,311 (removed duplicates: 0)\n",
      "            âœ… Verification: Expected 2,311, Got 2,311\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "      âœ… Stack-AVP-TR_neg completed: 2,311 â†’ 2,311 sequences\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ”¹ Processing mixed-label datasets:\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing mixed-label dataset: AVP-HNCL_non-AMP\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         ğŸ“Š Original data: 4258 total sequences\n",
      "            â”œâ”€ AVP (positive): 2129\n",
      "            â””â”€ non_AVP (negative): 2129\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 4,258\n",
      "            Step 1 - After natural AA filter: 4,258 (removed: 0)\n",
      "            Step 2 - After length filter (5-50): 2,232\n",
      "                   â”œâ”€ Too short (<5): 0\n",
      "                   â””â”€ Too long (>50): 2,026\n",
      "            Step 3 - After deduplication: 2,232 (removed duplicates: 0)\n",
      "            âœ… Verification: Expected 2,232, Got 2,232\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ“Š After all filtering: 2232 total sequences\n",
      "            â”œâ”€ AVP: 2088 (lost: 41)\n",
      "            â””â”€ non_AVP: 144 (lost: 1985)\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“Š Processing mixed-label dataset: AVP-HNCL_non-AVP\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "         ğŸ“Š Original data: 4258 total sequences\n",
      "            â”œâ”€ AVP (positive): 2129\n",
      "            â””â”€ non_AVP (negative): 2129\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ” Detailed filtering process:\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "            Step 0 - Original sequences: 4,258\n",
      "            Step 1 - After natural AA filter: 4,258 (removed: 0)\n",
      "            Step 2 - After length filter (5-50): 3,692\n",
      "                   â”œâ”€ Too short (<5): 0\n",
      "                   â””â”€ Too long (>50): 566\n",
      "            Step 3 - After deduplication: 3,692 (removed duplicates: 0)\n",
      "            âœ… Verification: Expected 3,692, Got 3,692\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„\n",
      "         ğŸ“Š After all filtering: 3692 total sequences\n",
      "            â”œâ”€ AVP: 2088 (lost: 41)\n",
      "            â””â”€ non_AVP: 1604 (lost: 525)\n",
      "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ”— Merging external AVP datasets\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“ˆ Total sequences before merge: 6,466\n",
      "         â””â”€ Stack-AVP-TR_pos: 2,290 sequences\n",
      "         â””â”€ AVP-HNCL_non-AMP: 2,088 sequences\n",
      "         â””â”€ AVP-HNCL_non-AVP: 2,088 sequences\n",
      "      ğŸ“‰ After final deduplication: 3,412 sequences\n",
      "      ğŸ—‘ï¸ Final duplicates removed: 3,054\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ”— Merging external non_AVP datasets\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      ğŸ“ˆ Total sequences before merge: 4,059\n",
      "         â””â”€ Stack-AVP-TR_neg: 2,311 sequences\n",
      "         â””â”€ AVP-HNCL_non-AMP: 144 sequences\n",
      "         â””â”€ AVP-HNCL_non-AVP: 1,604 sequences\n",
      "      ğŸ“‰ After final deduplication: 4,049 sequences\n",
      "      ğŸ—‘ï¸ Final duplicates removed: 10\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ’¾ SAVING PROCESSED DATASETS:\n",
      "================================================================================\n",
      "   âœ… External Avp: 3,412 sequences\n",
      "      â””â”€ Saved to: 1_Data/Processed_data_set/Initial_merged_data_set/Initial_TR_AVP.csv\n",
      "      â””â”€ Columns: ['Id', 'Sequence', 'Source', 'Length', 'Label', 'Type']\n",
      "   âœ… External Non Avp: 4,049 sequences\n",
      "      â””â”€ Saved to: 1_Data/Processed_data_set/Initial_merged_data_set/Initial_TR_non_AVP.csv\n",
      "      â””â”€ Columns: ['Id', 'Sequence', 'Source', 'Length', 'Label', 'Type']\n",
      "   âœ… Internal Avp: 4,993 sequences\n",
      "      â””â”€ Saved to: 1_Data/Processed_data_set/Initial_merged_data_set/Initial_TS_AVP.csv\n",
      "      â””â”€ Columns: ['Id', 'Sequence', 'Source', 'Length', 'Label', 'Type']\n",
      "   âœ… Internal Non Avp: 9,402 sequences\n",
      "      â””â”€ Saved to: 1_Data/Processed_data_set/Initial_merged_data_set/Initial_TS_non_AVP.csv\n",
      "      â””â”€ Columns: ['Id', 'Sequence', 'Source', 'Length', 'Label', 'Type']\n",
      "\n",
      "   ğŸ“„ Processing log saved to: 2_Log/2.1_Training set_and_test_set_processing/step1_dataset_processing_log.json\n",
      "\n",
      "   ğŸ“ All files saved to: 1_Data/Processed_data_set/Initial_merged_data_set\n",
      "   ğŸ“Š Total files saved: 4\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ“‹ COMPREHENSIVE STATISTICS SUMMARY\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ”¹ INTERNAL DATASETS DETAILED BREAKDOWN:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ“Š Individual AVP dataset filtering details:\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  DRAVP                                                          â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original:    1,986 sequences                                      â”‚\n",
      "   â”‚  Non-natural AA removed:      230                                    â”‚\n",
      "   â”‚  Too short removed:       17                                          â”‚\n",
      "   â”‚  Too long removed:       25                                           â”‚\n",
      "   â”‚  Duplicates removed:      106                                        â”‚\n",
      "   â”‚  Final:    1,608 sequences                                         â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  AVPdb                                                          â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original:    2,059 sequences                                      â”‚\n",
      "   â”‚  Non-natural AA removed:        1                                      â”‚\n",
      "   â”‚  Too short removed:       15                                          â”‚\n",
      "   â”‚  Too long removed:       19                                           â”‚\n",
      "   â”‚  Duplicates removed:      240                                        â”‚\n",
      "   â”‚  Final:    1,784 sequences                                         â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  ACovPepDB                                                      â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original:      518 sequences                                        â”‚\n",
      "   â”‚  Non-natural AA removed:      137                                    â”‚\n",
      "   â”‚  Too short removed:        0                                           â”‚\n",
      "   â”‚  Too long removed:       31                                           â”‚\n",
      "   â”‚  Duplicates removed:      201                                        â”‚\n",
      "   â”‚  Final:      149 sequences                                           â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  HIPdb                                                          â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original:      981 sequences                                        â”‚\n",
      "   â”‚  Non-natural AA removed:        2                                      â”‚\n",
      "   â”‚  Too short removed:       23                                          â”‚\n",
      "   â”‚  Too long removed:        6                                            â”‚\n",
      "   â”‚  Duplicates removed:       92                                         â”‚\n",
      "   â”‚  Final:      858 sequences                                           â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  CAMP                                                           â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original:    1,087 sequences                                      â”‚\n",
      "   â”‚  Non-natural AA removed:      136                                    â”‚\n",
      "   â”‚  Too short removed:       16                                          â”‚\n",
      "   â”‚  Too long removed:       30                                           â”‚\n",
      "   â”‚  Duplicates removed:        7                                          â”‚\n",
      "   â”‚  Final:      898 sequences                                           â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  DBAASP                                                         â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original:    1,488 sequences                                      â”‚\n",
      "   â”‚  Non-natural AA removed:      225                                    â”‚\n",
      "   â”‚  Too short removed:       19                                          â”‚\n",
      "   â”‚  Too long removed:       24                                           â”‚\n",
      "   â”‚  Duplicates removed:       78                                         â”‚\n",
      "   â”‚  Final:    1,142 sequences                                         â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  DRAMP                                                          â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original:    2,004 sequences                                      â”‚\n",
      "   â”‚  Non-natural AA removed:      233                                    â”‚\n",
      "   â”‚  Too short removed:       17                                          â”‚\n",
      "   â”‚  Too long removed:       82                                           â”‚\n",
      "   â”‚  Duplicates removed:       97                                         â”‚\n",
      "   â”‚  Final:    1,575 sequences                                         â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  dbAMP_AntiHIV                                                  â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original:    1,024 sequences                                      â”‚\n",
      "   â”‚  Non-natural AA removed:        0                                      â”‚\n",
      "   â”‚  Too short removed:       23                                          â”‚\n",
      "   â”‚  Too long removed:       29                                           â”‚\n",
      "   â”‚  Duplicates removed:        2                                          â”‚\n",
      "   â”‚  Final:      970 sequences                                           â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  dbAMP_Antiviral                                                â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original:    2,018 sequences                                      â”‚\n",
      "   â”‚  Non-natural AA removed:      127                                    â”‚\n",
      "   â”‚  Too short removed:       27                                          â”‚\n",
      "   â”‚  Too long removed:      173                                          â”‚\n",
      "   â”‚  Duplicates removed:        0                                          â”‚\n",
      "   â”‚  Final:    1,691 sequences                                         â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  Peptipedia                                                     â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original:    5,899 sequences                                      â”‚\n",
      "   â”‚  Non-natural AA removed:      722                                    â”‚\n",
      "   â”‚  Too short removed:       85                                          â”‚\n",
      "   â”‚  Too long removed:      236                                          â”‚\n",
      "   â”‚  Duplicates removed:        0                                          â”‚\n",
      "   â”‚  Final:    4,856 sequences                                         â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ“Š Internal non_AVP (UniProt) filtering details:\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  UniProt non_AVP                                                â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original:   13,124 sequences                                     â”‚\n",
      "   â”‚  Non-natural AA removed:      611                                    â”‚\n",
      "   â”‚  Too short removed:        0                                           â”‚\n",
      "   â”‚  Too long removed:        0                                            â”‚\n",
      "   â”‚  Duplicates removed:    3,111                                      â”‚\n",
      "   â”‚  Final:    9,402 sequences                                         â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ”¹ EXTERNAL DATASETS DETAILED BREAKDOWN:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  ğŸ“Š Stack-AVP-TR_pos                                                       â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original: 2,290 sequences (AVP)                                            â”‚\n",
      "   â”‚  Non-natural AA removed:        0                                                â”‚\n",
      "   â”‚  Too short removed:        0                                                     â”‚\n",
      "   â”‚  Too long removed:        0                                                      â”‚\n",
      "   â”‚  Duplicates removed:        0                                                    â”‚\n",
      "   â”‚  Final:    2,290 sequences                                                   â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  ğŸ“Š Stack-AVP-TR_neg                                                       â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original: 2,311 sequences (non_AVP)                                        â”‚\n",
      "   â”‚  Non-natural AA removed:        0                                                â”‚\n",
      "   â”‚  Too short removed:        0                                                     â”‚\n",
      "   â”‚  Too long removed:        0                                                      â”‚\n",
      "   â”‚  Duplicates removed:        0                                                    â”‚\n",
      "   â”‚  Final:    2,311 sequences                                                   â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  ğŸ“Š AVP-HNCL_non-AMP                                                       â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original: 4,258 (AVP: 2129, non_AVP: 2129)                                 â”‚\n",
      "   â”‚  Non-natural AA removed:        0                                                â”‚\n",
      "   â”‚  Too short removed:        0                                                     â”‚\n",
      "   â”‚  Too long removed:    2,026                                                  â”‚\n",
      "   â”‚  Duplicates removed:        0                                                    â”‚\n",
      "   â”‚  Final: 2,232 (AVP: 2088, non_AVP: 144)                                     â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "   â”‚  ğŸ“Š AVP-HNCL_non-AVP                                                       â”‚\n",
      "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "   â”‚  Original: 4,258 (AVP: 2129, non_AVP: 2129)                                 â”‚\n",
      "   â”‚  Non-natural AA removed:        0                                                â”‚\n",
      "   â”‚  Too short removed:        0                                                     â”‚\n",
      "   â”‚  Too long removed:      566                                                    â”‚\n",
      "   â”‚  Duplicates removed:        0                                                    â”‚\n",
      "   â”‚  Final: 3,692 (AVP: 2088, non_AVP: 1604)                                    â”‚\n",
      "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "ğŸ”¹ FINAL DATASET SUMMARY:\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "   ğŸ“ˆ External AVP: 3,412 sequences (Initial_TR_AVP.csv)\n",
      "   ğŸ“ˆ External non_AVP: 4,049 sequences (Initial_TR_non_AVP.csv)\n",
      "   ğŸ“ˆ Internal AVP: 4,993 sequences (Initial_TS_AVP.csv)\n",
      "   ğŸ“ˆ Internal non_AVP: 9,402 sequences (Initial_TS_non_AVP.csv)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ“Š Grand Total: 21,856 sequences\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ“„ Complete Step 1 summary saved to: 2_Log/2.1_Training set_and_test_set_processing/step1_complete_processing_summary.json\n",
      "\n",
      "âœ… Step 1 completed successfully with detailed statistics!\n",
      "   ğŸ“ Results stored in step1_detailed_results variable\n",
      "   ğŸ’¾ CSV files saved in: 1_Data/Processed_data_set/Initial_merged_data_set/\n",
      "   ğŸ“Š Ready for Step 2: Overlap analysis\n",
      "\n",
      "ğŸ¯ Step 1 Complete - Detailed filtering statistics generated for all datasets!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# åŸºç¡€é…ç½®\n",
    "# ============================================================================\n",
    "MIN_SEQUENCE_LENGTH = 5\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "# ============================================================================\n",
    "# é€šç”¨å·¥å…·å‡½æ•°\n",
    "# ============================================================================\n",
    "def filter_peptides_detailed(peptides_data, sequence_col='Sequence', min_length=MIN_SEQUENCE_LENGTH, max_length=MAX_SEQUENCE_LENGTH):\n",
    "    \"\"\"è¯¦ç»†çš„è¿‡æ»¤æ­¥éª¤ï¼šç­›é€‰5-50çš„å¤©ç„¶æ°¨åŸºé…¸é•¿åº¦åŒ…æ‹¬å¤§å°å†™çš„ä¸é‡å¤çš„æ°¨åŸºé…¸åºåˆ—ï¼Œè¿”å›æ¯æ­¥è¯¦ç»†ç»Ÿè®¡\"\"\"\n",
    "    natural_aa = set('ACDEFGHIKLMNPQRSTVWYacdefghiklmnpqrstvwy')\n",
    "    \n",
    "    # åˆå§‹åŒ–ç»Ÿè®¡\n",
    "    stats = {\n",
    "        'step0_original': len(peptides_data),\n",
    "        'step1_after_natural_aa': 0,\n",
    "        'step2_after_length': 0,\n",
    "        'step3_final': 0,\n",
    "        'removed_non_natural_aa': 0,\n",
    "        'removed_too_short': 0,\n",
    "        'removed_too_long': 0,\n",
    "        'removed_duplicates': 0\n",
    "    }\n",
    "    \n",
    "    print(f\"         {'â”„' * 60}\")\n",
    "    print(f\"         ğŸ” Detailed filtering process:\")\n",
    "    print(f\"         {'â”„' * 60}\")\n",
    "    print(f\"            Step 0 - Original sequences: {stats['step0_original']:,}\")\n",
    "    \n",
    "    # æ­¥éª¤1: æ£€æŸ¥å¤©ç„¶æ°¨åŸºé…¸\n",
    "    filtered_data = peptides_data.copy()\n",
    "    mask = filtered_data[sequence_col].apply(lambda seq: all(aa in natural_aa for aa in seq))\n",
    "    stats['removed_non_natural_aa'] = len(filtered_data) - mask.sum()\n",
    "    filtered_data = filtered_data[mask]\n",
    "    stats['step1_after_natural_aa'] = len(filtered_data)\n",
    "    print(f\"            Step 1 - After natural AA filter: {stats['step1_after_natural_aa']:,} (removed: {stats['removed_non_natural_aa']:,})\")\n",
    "    \n",
    "    # æ­¥éª¤2: è¿‡æ»¤é•¿åº¦\n",
    "    filtered_data['temp_length'] = filtered_data[sequence_col].apply(len)\n",
    "    \n",
    "    # å…ˆè¿‡æ»¤å¤ªçŸ­çš„\n",
    "    too_short_mask = filtered_data['temp_length'] >= min_length\n",
    "    stats['removed_too_short'] = len(filtered_data) - too_short_mask.sum()\n",
    "    filtered_data = filtered_data[too_short_mask]\n",
    "    \n",
    "    # å†è¿‡æ»¤å¤ªé•¿çš„\n",
    "    too_long_mask = filtered_data['temp_length'] <= max_length\n",
    "    stats['removed_too_long'] = len(filtered_data) - too_long_mask.sum()\n",
    "    filtered_data = filtered_data[too_long_mask]\n",
    "    \n",
    "    stats['step2_after_length'] = len(filtered_data)\n",
    "    print(f\"            Step 2 - After length filter ({min_length}-{max_length}): {stats['step2_after_length']:,}\")\n",
    "    print(f\"                   â”œâ”€ Too short (<{min_length}): {stats['removed_too_short']:,}\")\n",
    "    print(f\"                   â””â”€ Too long (>{max_length}): {stats['removed_too_long']:,}\")\n",
    "    \n",
    "    # æ­¥éª¤3: å»é™¤é‡å¤ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰\n",
    "    filtered_data['temp_upper'] = filtered_data[sequence_col].str.upper()\n",
    "    before_dedup = len(filtered_data)\n",
    "    filtered_data = filtered_data.drop_duplicates(subset=['temp_upper'])\n",
    "    stats['removed_duplicates'] = before_dedup - len(filtered_data)\n",
    "    stats['step3_final'] = len(filtered_data)\n",
    "    print(f\"            Step 3 - After deduplication: {stats['step3_final']:,} (removed duplicates: {stats['removed_duplicates']:,})\")\n",
    "    \n",
    "    # è½¬æ¢ä¸ºå¤§å†™å¹¶æ¸…ç†ä¸´æ—¶åˆ—\n",
    "    filtered_data[sequence_col] = filtered_data[sequence_col].str.upper()\n",
    "    filtered_data = filtered_data.drop(['temp_length', 'temp_upper'], axis=1, errors='ignore')\n",
    "    \n",
    "    # éªŒè¯æ€»æ•°\n",
    "    total_removed = stats['removed_non_natural_aa'] + stats['removed_too_short'] + stats['removed_too_long'] + stats['removed_duplicates']\n",
    "    expected_final = stats['step0_original'] - total_removed\n",
    "    print(f\"            âœ… Verification: Expected {expected_final:,}, Got {stats['step3_final']:,}\")\n",
    "    print(f\"         {'â”„' * 60}\")\n",
    "    \n",
    "    return filtered_data, stats\n",
    "\n",
    "def parse_fasta_to_df(fasta_file, verbose=True):\n",
    "    \"\"\"å°†FASTAæ–‡ä»¶è§£æä¸ºpandas DataFrame\"\"\"\n",
    "    if verbose:\n",
    "        print(f\"         ğŸ“„ Parsing FASTA file: {os.path.basename(fasta_file)}\")\n",
    "    \n",
    "    records = []\n",
    "    encodings_to_try = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1', 'ascii']\n",
    "    \n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            with open(fasta_file, 'r', encoding=encoding, errors='ignore') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            current_id = None\n",
    "            current_seq = []\n",
    "            \n",
    "            for line in content.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                    \n",
    "                if line.startswith('>'):\n",
    "                    if current_id and current_seq:\n",
    "                        records.append({'Id': current_id, 'Sequence': ''.join(current_seq)})\n",
    "                    current_id = line[1:]\n",
    "                    current_seq = []\n",
    "                else:\n",
    "                    current_seq.append(line)\n",
    "            \n",
    "            if current_id and current_seq:\n",
    "                records.append({'Id': current_id, 'Sequence': ''.join(current_seq)})\n",
    "            \n",
    "            if verbose and records:\n",
    "                lengths = [len(rec['Sequence']) for rec in records]\n",
    "                print(f\"            âœ“ {len(records)} records, length range: {min(lengths)}-{max(lengths)}\")\n",
    "            break\n",
    "            \n",
    "        except (UnicodeDecodeError, UnicodeError):\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(records) if records else pd.DataFrame(columns=['Id', 'Sequence'])\n",
    "\n",
    "def process_dataset_detailed(file_path, sequence_col, id_col=None, source_name=\"\", file_type=\"csv\"):\n",
    "    \"\"\"è¯¦ç»†çš„æ•°æ®é›†å¤„ç†å‡½æ•°\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"      âŒ File {file_path} does not exist\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        print(f\"      {'â”€' * 70}\")\n",
    "        print(f\"      ğŸ“Š Processing: {source_name}\")\n",
    "        print(f\"      {'â”€' * 70}\")\n",
    "        \n",
    "        # è¯»å–æ•°æ®\n",
    "        if file_type == \"csv\":\n",
    "            data = pd.read_csv(file_path)\n",
    "        elif file_type == \"excel\":\n",
    "            data = pd.read_excel(file_path)\n",
    "        elif file_type == \"fasta\":\n",
    "            data = parse_fasta_to_df(file_path, verbose=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type\")\n",
    "        \n",
    "        if sequence_col not in data.columns:\n",
    "            print(f\"      âŒ Sequence column '{sequence_col}' not found\")\n",
    "            return None, None\n",
    "        \n",
    "        # è¯¦ç»†è¿‡æ»¤æ•°æ®\n",
    "        filtered_data, stats = filter_peptides_detailed(data, sequence_col=sequence_col)\n",
    "        \n",
    "        # æ ‡å‡†åŒ–åˆ—\n",
    "        if id_col and id_col in filtered_data.columns:\n",
    "            filtered_data = filtered_data[[id_col, sequence_col]].copy()\n",
    "            filtered_data = filtered_data.rename(columns={id_col: 'Id', sequence_col: 'Sequence'})\n",
    "        else:\n",
    "            filtered_data = filtered_data[[sequence_col]].copy()\n",
    "            filtered_data = filtered_data.rename(columns={sequence_col: 'Sequence'})\n",
    "            filtered_data['Id'] = [f\"{source_name}_{i+1}\" for i in range(len(filtered_data))]\n",
    "            filtered_data = filtered_data[['Id', 'Sequence']]\n",
    "        \n",
    "        filtered_data['Source'] = source_name\n",
    "        filtered_data['Length'] = filtered_data['Sequence'].apply(len)\n",
    "        \n",
    "        print(f\"      âœ… {source_name} completed: {stats['step0_original']:,} â†’ {stats['step3_final']:,} sequences\")\n",
    "        print(f\"      {'â”€' * 70}\")\n",
    "        \n",
    "        return filtered_data, stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Error processing {source_name}: {str(e)}\")\n",
    "        print(f\"      {'â”€' * 70}\")\n",
    "        return None, None\n",
    "\n",
    "def merge_datasets_with_priority_detailed(datasets_list, priority_order, dataset_type=\"\"):\n",
    "    \"\"\"æŒ‰ä¼˜å…ˆçº§åˆå¹¶æ•°æ®é›†å¹¶å»é‡ï¼Œæ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡\"\"\"\n",
    "    valid_datasets = [ds for ds in datasets_list if ds is not None]\n",
    "    \n",
    "    if not valid_datasets:\n",
    "        return pd.DataFrame(columns=['Id', 'Sequence', 'Source', 'Length'])\n",
    "    \n",
    "    print(f\"\\n   {'â”€' * 70}\")\n",
    "    print(f\"   ğŸ”— Merging {dataset_type} datasets\")\n",
    "    print(f\"   {'â”€' * 70}\")\n",
    "    \n",
    "    # è®¡ç®—åˆå¹¶å‰çš„ç»Ÿè®¡\n",
    "    total_before = sum(len(ds) for ds in valid_datasets)\n",
    "    print(f\"      ğŸ“ˆ Total sequences before merge: {total_before:,}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºå„æ•°æ®é›†è´¡çŒ®\n",
    "    for i, ds in enumerate(valid_datasets):\n",
    "        source = ds['Source'].iloc[0] if len(ds) > 0 else f\"Dataset_{i+1}\"\n",
    "        print(f\"         â””â”€ {source}: {len(ds):,} sequences\")\n",
    "    \n",
    "    combined_df = pd.concat(valid_datasets, ignore_index=True)\n",
    "    combined_df['Length'] = combined_df['Sequence'].apply(len)\n",
    "    \n",
    "    # æŒ‰ä¼˜å…ˆçº§æ’åº\n",
    "    source_priority = {source: i+1 for i, source in enumerate(priority_order)}\n",
    "    combined_df['Priority'] = combined_df['Source'].map(source_priority)\n",
    "    combined_df = combined_df.sort_values('Priority')\n",
    "    \n",
    "    # å»é‡\n",
    "    before_final_dedup = len(combined_df)\n",
    "    final_dataset = combined_df.drop_duplicates(subset=['Sequence'], keep='first')\n",
    "    final_dataset = final_dataset.drop('Priority', axis=1).reset_index(drop=True)\n",
    "    \n",
    "    final_dedup_removed = before_final_dedup - len(final_dataset)\n",
    "    print(f\"      ğŸ“‰ After final deduplication: {len(final_dataset):,} sequences\")\n",
    "    print(f\"      ğŸ—‘ï¸ Final duplicates removed: {final_dedup_removed:,}\")\n",
    "    print(f\"   {'â”€' * 70}\")\n",
    "    \n",
    "    return final_dataset\n",
    "\n",
    "def save_datasets_to_csv(datasets_dict, output_dir):\n",
    "    \"\"\"ä¿å­˜æ•°æ®é›†ä¸ºCSVæ–‡ä»¶\"\"\"\n",
    "    print(f\"\\nğŸ’¾ SAVING PROCESSED DATASETS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # âœ… æ·»åŠ ï¼šåˆ›å»ºæ—¥å¿—ç›®å½•å¹¶ä¿å­˜å¤„ç†ä¿¡æ¯\n",
    "    log_dir = \"2_Log/2.1_Training set_and_test_set_processing\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # å®šä¹‰æ–‡ä»¶åæ˜ å°„\n",
    "    filename_mapping = {\n",
    "        'external_avp': 'Initial_TR_AVP.csv',\n",
    "        'external_non_avp': 'Initial_TR_non_AVP.csv',\n",
    "        'internal_avp': 'Initial_TS_AVP.csv',\n",
    "        'internal_non_avp': 'Initial_TS_non_AVP.csv'\n",
    "    }\n",
    "    \n",
    "    # ä¿å­˜æ¯ä¸ªæ•°æ®é›†\n",
    "    saved_files = {}\n",
    "    processing_log = []  # âœ… æ·»åŠ ï¼šè®°å½•å¤„ç†æ—¥å¿—\n",
    "    \n",
    "    for key, filename in filename_mapping.items():\n",
    "        if key in datasets_dict and datasets_dict[key] is not None:\n",
    "            dataset = datasets_dict[key]\n",
    "            if len(dataset) > 0:\n",
    "                # ç¡®ä¿åˆ—é¡ºåºï¼šId, Sequence, Source, Length, Label, Type\n",
    "                columns_order = ['Id', 'Sequence', 'Source', 'Length', 'Label', 'Type']\n",
    "                \n",
    "                # æ£€æŸ¥å¹¶é‡æ–°æ’åˆ—åˆ—\n",
    "                available_columns = [col for col in columns_order if col in dataset.columns]\n",
    "                dataset_to_save = dataset[available_columns].copy()\n",
    "                \n",
    "                file_path = os.path.join(output_dir, filename)\n",
    "                dataset_to_save.to_csv(file_path, index=False)\n",
    "                saved_files[key] = file_path\n",
    "                \n",
    "                # âœ… æ·»åŠ ï¼šè®°å½•å¤„ç†ä¿¡æ¯\n",
    "                log_entry = {\n",
    "                    'dataset': key.replace('_', ' ').title(),\n",
    "                    'sequences': len(dataset),\n",
    "                    'file_path': file_path,\n",
    "                    'columns': list(dataset_to_save.columns)\n",
    "                }\n",
    "                processing_log.append(log_entry)\n",
    "                \n",
    "                print(f\"   âœ… {key.replace('_', ' ').title()}: {len(dataset):,} sequences\")\n",
    "                print(f\"      â””â”€ Saved to: {file_path}\")\n",
    "                print(f\"      â””â”€ Columns: {list(dataset_to_save.columns)}\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸  {key.replace('_', ' ').title()}: No data to save\")\n",
    "        else:\n",
    "            print(f\"   âŒ {key.replace('_', ' ').title()}: Dataset not found\")\n",
    "    \n",
    "    # âœ… æ·»åŠ ï¼šä¿å­˜å¤„ç†æ—¥å¿—åˆ°æŒ‡å®šç›®å½•\n",
    "    if processing_log:\n",
    "        import json\n",
    "        \n",
    "        log_data = {\n",
    "            'step': 'Step 1 - Dataset Processing',\n",
    "            'total_files_saved': len(saved_files),\n",
    "            'datasets': processing_log\n",
    "        }\n",
    "        \n",
    "        log_file = os.path.join(log_dir, \"step1_dataset_processing_log.json\")\n",
    "        with open(log_file, 'w') as f:\n",
    "            json.dump(log_data, f, indent=2)\n",
    "        print(f\"\\n   ğŸ“„ Processing log saved to: {log_file}\")\n",
    "    \n",
    "    print(f\"\\n   ğŸ“ All files saved to: {output_dir}\")\n",
    "    print(f\"   ğŸ“Š Total files saved: {len(saved_files)}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "# ============================================================================\n",
    "# ç¬¬ä¸€æ­¥ï¼šå¤„ç†å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®é›†ï¼ˆå¸¦è¯¦ç»†ç»Ÿè®¡ï¼‰\n",
    "# ============================================================================\n",
    "\n",
    "def step1_process_datasets_detailed():\n",
    "    \"\"\"ç¬¬ä¸€æ­¥ï¼šè¯¦ç»†å¤„ç†å¹¶ç»Ÿè®¡å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®é›†\"\"\"\n",
    "    print(\"ğŸš€ STEP 1: DETAILED PROCESSING OF INTERNAL AND EXTERNAL DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # âœ… æ·»åŠ ï¼šåˆ›å»ºæ—¥å¿—ç›®å½•\n",
    "    log_dir = \"2_Log/2.1_Training set_and_test_set_processing\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # ========== å¤„ç†å†…éƒ¨AVPæ•°æ®é›† ==========\n",
    "    print(\"\\nğŸ“Š PROCESSING INTERNAL AVP DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # é…ç½®æ‰€æœ‰æ•°æ®æº\n",
    "    avp_configs = [\n",
    "        {'file_path': '1_Data/Raw_data/AVPdataset/dravp_antiviral_peptides.xlsx', 'sequence_col': 'Sequence', 'id_col': 'DRAVP_ID', 'source_name': 'DRAVP', 'file_type': 'excel'},\n",
    "        {'file_path': '1_Data/Raw_data/AVPdataset/AVPdb_data.xls', 'sequence_col': 'Sequence', 'id_col': 'Id', 'source_name': 'AVPdb', 'file_type': 'excel'},\n",
    "        {'file_path': '1_Data/Raw_data/AVPdataset/ACovPepDB_Data_Entirety.csv', 'sequence_col': 'Sequence', 'id_col': 'ACovPid', 'source_name': 'ACovPepDB', 'file_type': 'csv'},\n",
    "        {'file_path': '1_Data/Raw_data/AVPdataset/HIPdb_data.xls', 'sequence_col': 'SEQUENCE', 'id_col': 'ID', 'source_name': 'HIPdb', 'file_type': 'excel'}\n",
    "    ]\n",
    "    \n",
    "    amp_configs = [\n",
    "        {'file_path': '1_Data/Raw_data/AMPdataset/CAMP.xlsx', 'sequence_col': 'Seqence', 'id_col': 'Camp_ID', 'source_name': 'CAMP', 'file_type': 'excel'},\n",
    "        {'file_path': '1_Data/Raw_data/AMPdataset/dbaasp_peptides.xlsx', 'sequence_col': 'SEQUENCE', 'id_col': 'ID', 'source_name': 'DBAASP', 'file_type': 'excel'},\n",
    "        {'file_path': '1_Data/Raw_data/AMPdataset/dramp_general_avps.xlsx', 'sequence_col': 'Sequence', 'id_col': 'DRAMP_ID', 'source_name': 'DRAMP', 'file_type': 'excel'},\n",
    "        {'file_path': '1_Data/Raw_data/AMPdataset/dbAMP_AntiHIV_2024.fasta', 'sequence_col': 'Sequence', 'id_col': 'Id', 'source_name': 'dbAMP_AntiHIV', 'file_type': 'fasta'},\n",
    "        {'file_path': '1_Data/Raw_data/AMPdataset/dbAMP_Antiviral_2024.fasta', 'sequence_col': 'Sequence', 'id_col': 'Id', 'source_name': 'dbAMP_Antiviral', 'file_type': 'fasta'}\n",
    "    ]\n",
    "    \n",
    "    other_configs = [\n",
    "        {'file_path': '1_Data/Raw_data/Peptipedia_Antiviral.fasta', 'sequence_col': 'Sequence', 'id_col': 'Id', 'source_name': 'Peptipedia', 'file_type': 'fasta'}\n",
    "    ]\n",
    "    \n",
    "    # å¤„ç†æ‰€æœ‰å†…éƒ¨AVPæ•°æ®æº\n",
    "    all_internal_avp_datasets = []\n",
    "    internal_avp_detailed_stats = {}\n",
    "    \n",
    "    print(\"   ğŸ”¹ Processing individual internal AVP datasets:\")\n",
    "    for config in avp_configs + amp_configs + other_configs:\n",
    "        result = process_dataset_detailed(**config)\n",
    "        if result and result[0] is not None:\n",
    "            dataset, stats = result\n",
    "            all_internal_avp_datasets.append(dataset)\n",
    "            internal_avp_detailed_stats[config['source_name']] = stats\n",
    "    \n",
    "    # åˆå¹¶å†…éƒ¨AVPæ•°æ®é›†\n",
    "    priority_order = ['DRAVP', 'AVPdb', 'ACovPepDB', 'HIPdb', 'CAMP', 'DBAASP', 'DRAMP', 'dbAMP_AntiHIV', 'dbAMP_Antiviral', 'Peptipedia']\n",
    "    internal_avp_dataset = merge_datasets_with_priority_detailed(all_internal_avp_datasets, priority_order, \"internal AVP\")\n",
    "    internal_avp_dataset['Label'] = 1\n",
    "    internal_avp_dataset['Type'] = 'AVP'\n",
    "    \n",
    "    # ========== å¤„ç†å†…éƒ¨non_AVPæ•°æ®é›† ==========\n",
    "    print(f\"\\nğŸ“Š PROCESSING INTERNAL non_AVP DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    file_path = '1_Data/Raw_data/non_AVP/uniprotkb_13124.fasta'\n",
    "    print(\"   ğŸ”¹ Processing UniProt non_AVP dataset:\")\n",
    "\n",
    "    result = process_dataset_detailed(file_path, 'Sequence', 'Id', 'UniProt', 'fasta')\n",
    "    if result[0] is not None:\n",
    "        internal_non_avp_dataset, non_avp_detailed_stats = result\n",
    "        \n",
    "        # å¤„ç†UniProt IDæ ¼å¼\n",
    "        def process_uniprot_id(uniprot_id):\n",
    "            \"\"\"\n",
    "            å¤„ç†UniProt IDï¼Œåªä¿ç•™å‰ä¸¤ä¸ªç«–æ çš„éƒ¨åˆ†\n",
    "            ä¾‹å¦‚: sp|A5A616|YXXX_HUMAN -> sp|A5A616|\n",
    "            \"\"\"\n",
    "            if pd.isna(uniprot_id):\n",
    "                return uniprot_id\n",
    "            \n",
    "            id_str = str(uniprot_id).strip()\n",
    "            \n",
    "            # æŸ¥æ‰¾æ‰€æœ‰ç«–æ çš„ä½ç½®\n",
    "            pipe_positions = [i for i, char in enumerate(id_str) if char == '|']\n",
    "            \n",
    "            if len(pipe_positions) >= 2:\n",
    "                # ä¿ç•™åˆ°ç¬¬äºŒä¸ªç«–æ ä¹‹åï¼ˆåŒ…å«ç¬¬äºŒä¸ªç«–æ ï¼‰\n",
    "                return id_str[:pipe_positions[1] + 1]\n",
    "            elif len(pipe_positions) == 1:\n",
    "                # å¦‚æœåªæœ‰ä¸€ä¸ªç«–æ ï¼Œä¿ç•™åˆ°ç¬¬ä¸€ä¸ªç«–æ ä¹‹å\n",
    "                return id_str[:pipe_positions[0] + 1]\n",
    "            else:\n",
    "                # å¦‚æœæ²¡æœ‰ç«–æ ï¼Œè¿”å›åŸID\n",
    "                return id_str\n",
    "        \n",
    "        # åº”ç”¨IDå¤„ç†\n",
    "        original_ids = internal_non_avp_dataset['Id'].copy()\n",
    "        internal_non_avp_dataset['Id'] = internal_non_avp_dataset['Id'].apply(process_uniprot_id)\n",
    "        \n",
    "        # æ˜¾ç¤ºIDå¤„ç†ç¤ºä¾‹\n",
    "        print(f\"      ğŸ“ ID format examples:\")\n",
    "        for i in range(min(5, len(original_ids))):\n",
    "            old_id = original_ids.iloc[i]\n",
    "            new_id = internal_non_avp_dataset['Id'].iloc[i]\n",
    "            print(f\"         {old_id} â†’ {new_id}\")\n",
    "        \n",
    "        print(f\"      âœ… Processed {len(internal_non_avp_dataset)} UniProt IDs to short format\")\n",
    "        \n",
    "        internal_non_avp_dataset['Label'] = 0\n",
    "        internal_non_avp_dataset['Type'] = 'non_AVP'\n",
    "    else:\n",
    "        print(\"      âŒ Failed to process non_AVP dataset\")\n",
    "        return None\n",
    "    \n",
    "    # ========== å¤„ç†å¤–éƒ¨æ•°æ®é›† ==========\n",
    "    print(f\"\\nğŸ“Š PROCESSING EXTERNAL DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    external_files = {\n",
    "        'AVP': {\n",
    "            'Stack-AVP-TR_pos': '1_Data/Raw_data/External_dataset/Stack-AVP-TR_pos.fasta'\n",
    "        },\n",
    "        'non_AVP': {\n",
    "            'Stack-AVP-TR_neg': '1_Data/Raw_data/External_dataset/Stack-AVP-TR_neg.fasta'\n",
    "        },\n",
    "        'mixed': {\n",
    "            'AVP-HNCL_non-AMP': '1_Data/Raw_data/External_dataset/AVP-HNCL_non-AMP TR dataset.txt',\n",
    "            'AVP-HNCL_non-AVP': '1_Data/Raw_data/External_dataset/AVP-HNCL_non-AVP TR dataset.txt'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    external_individual_datasets = {'AVP': [], 'non_AVP': []}\n",
    "    external_detailed_stats = {}\n",
    "    \n",
    "    # å¤„ç†çº¯AVPæ–‡ä»¶\n",
    "    print(\"   ğŸ”¹ Processing external AVP datasets:\")\n",
    "    for name, file_path in external_files['AVP'].items():\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"      âš ï¸ File not found: {name}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            result = process_dataset_detailed(file_path, 'Sequence', 'Id', name, 'fasta')\n",
    "            if result and result[0] is not None:\n",
    "                filtered_df, stats = result\n",
    "                filtered_df['Label'] = 1  # AVP\n",
    "                external_individual_datasets['AVP'].append(filtered_df)\n",
    "                external_detailed_stats[name] = {**stats, 'type': 'AVP'}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ Error processing {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # å¤„ç†çº¯non_AVPæ–‡ä»¶\n",
    "    print(\"   ğŸ”¹ Processing external non_AVP datasets:\")\n",
    "    for name, file_path in external_files['non_AVP'].items():\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"      âš ï¸ File not found: {name}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            result = process_dataset_detailed(file_path, 'Sequence', 'Id', name, 'fasta')\n",
    "            if result and result[0] is not None:\n",
    "                filtered_df, stats = result\n",
    "                filtered_df['Label'] = 0  # non_AVP\n",
    "                external_individual_datasets['non_AVP'].append(filtered_df)\n",
    "                external_detailed_stats[name] = {**stats, 'type': 'non_AVP'}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ Error processing {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # å¤„ç†æ··åˆæ ‡ç­¾çš„txtæ–‡ä»¶\n",
    "    print(\"   ğŸ”¹ Processing mixed-label datasets:\")\n",
    "    for name, file_path in external_files['mixed'].items():\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"      âš ï¸ File not found: {name}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            sequences_data = []\n",
    "            encodings = ['utf-8', 'latin-1', 'cp1252', 'gbk']\n",
    "            \n",
    "            print(f\"      {'â”€' * 70}\")\n",
    "            print(f\"      ğŸ“Š Processing mixed-label dataset: {name}\")\n",
    "            print(f\"      {'â”€' * 70}\")\n",
    "            \n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding=encoding) as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯FASTAæ ¼å¼\n",
    "                    if '>' in content:\n",
    "                        # FASTAæ ¼å¼å¤„ç†\n",
    "                        current_id = None\n",
    "                        current_seq = []\n",
    "                        \n",
    "                        for line in content.split('\\n'):\n",
    "                            line = line.strip()\n",
    "                            if not line:\n",
    "                                continue\n",
    "                                \n",
    "                            if line.startswith('>'):\n",
    "                                # ä¿å­˜å‰ä¸€ä¸ªåºåˆ—\n",
    "                                if current_id and current_seq:\n",
    "                                    sequence = ''.join(current_seq)\n",
    "                                    # ä»IDä¸­æ¨æ–­æ ‡ç­¾\n",
    "                                    header_lower = current_id.lower()\n",
    "                                    if any(pos_indicator in header_lower for pos_indicator in ['pos']):\n",
    "                                        label = 1  # AVP\n",
    "                                    elif any(neg_indicator in header_lower for neg_indicator in ['neg']):\n",
    "                                        label = 0  # non_AVP\n",
    "                                    else:\n",
    "                                        # æ ¹æ®æ–‡ä»¶åæ¨æ–­\n",
    "                                        if 'non-AMP' in name or 'non-AVP' in name:\n",
    "                                            label = 0  # non_AVP\n",
    "                                        else:\n",
    "                                            label = 1  # AVP (é»˜è®¤)\n",
    "                                    \n",
    "                                    sequences_data.append({\n",
    "                                        'Id': current_id,\n",
    "                                        'Sequence': sequence,\n",
    "                                        'Label': label\n",
    "                                    })\n",
    "                                \n",
    "                                current_id = line[1:]  # å»æ‰ '>'\n",
    "                                current_seq = []\n",
    "                            else:\n",
    "                                current_seq.append(line)\n",
    "                        \n",
    "                        # ä¿å­˜æœ€åä¸€ä¸ªåºåˆ—\n",
    "                        if current_id and current_seq:\n",
    "                            sequence = ''.join(current_seq)\n",
    "                            # ä»IDä¸­æ¨æ–­æ ‡ç­¾\n",
    "                            header_lower = current_id.lower()\n",
    "                            if any(pos_indicator in header_lower for pos_indicator in ['pos']):\n",
    "                                label = 1  # AVP\n",
    "                            elif any(neg_indicator in header_lower for neg_indicator in ['neg']):\n",
    "                                label = 0  # non_AVP\n",
    "                            else:\n",
    "                                # æ ¹æ®æ–‡ä»¶åæ¨æ–­\n",
    "                                if 'non-AMP' in name or 'non-AVP' in name:\n",
    "                                    label = 0  # non_AVP\n",
    "                                else:\n",
    "                                    label = 1  # AVP (é»˜è®¤)\n",
    "                            \n",
    "                            sequences_data.append({\n",
    "                                'Id': current_id,\n",
    "                                'Sequence': sequence,\n",
    "                                'Label': label\n",
    "                            })\n",
    "                    \n",
    "                    else:\n",
    "                        # éFASTAæ ¼å¼ï¼ŒæŒ‰è¡Œå¤„ç†\n",
    "                        for line_num, line in enumerate(content.split('\\n'), 1):\n",
    "                            line = line.strip()\n",
    "                            if not line or line.startswith('#'):\n",
    "                                continue\n",
    "                            \n",
    "                            # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦\n",
    "                            parts = None\n",
    "                            for sep in ['\\t', ' ', ',', ';']:\n",
    "                                if sep in line:\n",
    "                                    parts = [p.strip() for p in line.split(sep) if p.strip()]\n",
    "                                    break\n",
    "                            \n",
    "                            if parts is None:\n",
    "                                parts = [line.strip()]\n",
    "                            \n",
    "                            if len(parts) == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            # è§£æåºåˆ—å’Œæ ‡ç­¾\n",
    "                            sequence = None\n",
    "                            label = None\n",
    "                            \n",
    "                            for part in parts:\n",
    "                                # æ£€æŸ¥æ˜¯å¦ä¸ºæ ‡ç­¾\n",
    "                                if part.lower() in ['pos', 'positive', '1']:\n",
    "                                    label = 1  # AVP\n",
    "                                elif part.lower() in ['neg', 'negative', '0']:\n",
    "                                    label = 0  # non_AVP\n",
    "                                else:\n",
    "                                    # æ£€æŸ¥æ˜¯å¦ä¸ºåºåˆ—ï¼ˆåŒ…å«æ°¨åŸºé…¸å­—ç¬¦ï¼‰\n",
    "                                    if len(part) > 3 and all(c.upper() in 'ACDEFGHIKLMNPQRSTVWY' for c in part if c.isalpha()):\n",
    "                                        sequence = part\n",
    "                            \n",
    "                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„åºåˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªéæ ‡ç­¾éƒ¨åˆ†\n",
    "                            if sequence is None:\n",
    "                                for part in parts:\n",
    "                                    if part.lower() not in ['pos', 'negative', 'positive', 'neg', '1', '0']:\n",
    "                                        if len(part) > 3:  # åºåˆ—é•¿åº¦è‡³å°‘ä¸º4\n",
    "                                            sequence = part\n",
    "                                            break\n",
    "                            \n",
    "                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ï¼Œæ ¹æ®æ–‡ä»¶åæ¨æ–­\n",
    "                            if label is None:\n",
    "                                if 'non-AMP' in name or 'non-AVP' in name:\n",
    "                                    label = 0  # non_AVP\n",
    "                                else:\n",
    "                                    label = 1  # AVP (é»˜è®¤)\n",
    "                            \n",
    "                            if sequence:\n",
    "                                sequences_data.append({\n",
    "                                    'Id': f\"{name}_{line_num}\",\n",
    "                                    'Sequence': sequence,\n",
    "                                    'Label': label\n",
    "                                })\n",
    "                    \n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            if sequences_data:\n",
    "                df = pd.DataFrame(sequences_data)\n",
    "                original_count = len(df)\n",
    "                \n",
    "                # ç»Ÿè®¡åŸå§‹æ ‡ç­¾åˆ†å¸ƒ\n",
    "                original_avp_count = len(df[df['Label'] == 1])\n",
    "                original_non_avp_count = len(df[df['Label'] == 0])\n",
    "                \n",
    "                print(f\"         ğŸ“Š Original data: {original_count} total sequences\")\n",
    "                print(f\"            â”œâ”€ AVP (positive): {original_avp_count}\")\n",
    "                print(f\"            â””â”€ non_AVP (negative): {original_non_avp_count}\")\n",
    "                print(f\"         {'â”„' * 50}\")\n",
    "                \n",
    "                # åº”ç”¨è¯¦ç»†è¿‡æ»¤\n",
    "                filtered_df, detailed_stats = filter_peptides_detailed(df, sequence_col='Sequence')\n",
    "                \n",
    "                if len(filtered_df) > 0:\n",
    "                    filtered_df['Source'] = name\n",
    "                    filtered_df['Length'] = filtered_df['Sequence'].apply(len)\n",
    "                    \n",
    "                    # æŒ‰æ ‡ç­¾åˆ†ç»„\n",
    "                    avp_df = filtered_df[filtered_df['Label'] == 1].copy()\n",
    "                    non_avp_df = filtered_df[filtered_df['Label'] == 0].copy()\n",
    "                    \n",
    "                    print(f\"         {'â”„' * 50}\")\n",
    "                    print(f\"         ğŸ“Š After all filtering: {len(filtered_df)} total sequences\")\n",
    "                    print(f\"            â”œâ”€ AVP: {len(avp_df)} (lost: {original_avp_count - len(avp_df)})\")\n",
    "                    print(f\"            â””â”€ non_AVP: {len(non_avp_df)} (lost: {original_non_avp_count - len(non_avp_df)})\")\n",
    "                    \n",
    "                    if len(avp_df) > 0:\n",
    "                        external_individual_datasets['AVP'].append(avp_df)\n",
    "                    if len(non_avp_df) > 0:\n",
    "                        external_individual_datasets['non_AVP'].append(non_avp_df)\n",
    "                    \n",
    "                    external_detailed_stats[name] = {\n",
    "                        **detailed_stats,\n",
    "                        'original_avp_count': original_avp_count,\n",
    "                        'original_non_avp_count': original_non_avp_count,\n",
    "                        'filtered_avp_count': len(avp_df),\n",
    "                        'filtered_non_avp_count': len(non_avp_df)\n",
    "                    }\n",
    "                else:\n",
    "                    print(f\"         âš ï¸ No sequences passed filtering\")\n",
    "            else:\n",
    "                print(f\"         âš ï¸ No sequences found in file\")\n",
    "            \n",
    "            print(f\"      {'â”€' * 70}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ Error processing {name}: {str(e)}\")\n",
    "            print(f\"      {'â”€' * 70}\")\n",
    "            continue\n",
    "    \n",
    "    # åˆå¹¶å¤–éƒ¨æ•°æ®é›†\n",
    "    external_avp_dataset = merge_datasets_with_priority_detailed(\n",
    "        external_individual_datasets['AVP'], \n",
    "        ['Stack-AVP-TR_pos', 'AVP-HNCL_non-AMP', 'AVP-HNCL_non-AVP'], \n",
    "        \"external AVP\"\n",
    "    )\n",
    "    if len(external_avp_dataset) > 0:\n",
    "        external_avp_dataset['Type'] = 'AVP'\n",
    "    \n",
    "    external_non_avp_dataset = merge_datasets_with_priority_detailed(\n",
    "        external_individual_datasets['non_AVP'],\n",
    "        ['Stack-AVP-TR_neg', 'AVP-HNCL_non-AMP', 'AVP-HNCL_non-AVP'],\n",
    "        \"external non_AVP\"\n",
    "    )\n",
    "    if len(external_non_avp_dataset) > 0:\n",
    "        external_non_avp_dataset['Type'] = 'non_AVP'\n",
    "    \n",
    "    # ========== ä¿å­˜å¤„ç†åçš„æ•°æ®é›† ==========\n",
    "    datasets_to_save = {\n",
    "        'internal_avp': internal_avp_dataset,\n",
    "        'internal_non_avp': internal_non_avp_dataset,\n",
    "        'external_avp': external_avp_dataset,\n",
    "        'external_non_avp': external_non_avp_dataset\n",
    "    }\n",
    "    \n",
    "    output_dir = \"1_Data/Processed_data_set/Initial_merged_data_set\"\n",
    "    saved_files = save_datasets_to_csv(datasets_to_save, output_dir)\n",
    "    \n",
    "    # ========== æ‰“å°è¯¦ç»†ç»Ÿè®¡æ±‡æ€» ==========\n",
    "    print(f\"\\n{'â•' * 80}\")\n",
    "    print(f\"ğŸ“‹ COMPREHENSIVE STATISTICS SUMMARY\")\n",
    "    print(f\"{'â•' * 80}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”¹ INTERNAL DATASETS DETAILED BREAKDOWN:\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    print(f\"   ğŸ“Š Individual AVP dataset filtering details:\")\n",
    "    for source, stats in internal_avp_detailed_stats.items():\n",
    "        print(f\"   {'â”Œ' + 'â”€' * 65 + 'â”'}\")\n",
    "        print(f\"   â”‚  {source:<61s}  â”‚\")\n",
    "        print(f\"   {'â”œ' + 'â”€' * 65 + 'â”¤'}\")\n",
    "        print(f\"   â”‚  Original: {stats['step0_original']:>8,} sequences{' ' * (65 - len(f'Original: {stats['step0_original']:,} sequences') - 2)}â”‚\")\n",
    "        print(f\"   â”‚  Non-natural AA removed: {stats['removed_non_natural_aa']:>8,}{' ' * (65 - len(f'Non-natural AA removed: {stats['removed_non_natural_aa']:,}') - 2)}â”‚\")\n",
    "        print(f\"   â”‚  Too short removed: {stats['removed_too_short']:>8,}{' ' * (65 - len(f'Too short removed: {stats['removed_too_short']:,}') - 2)}â”‚\")\n",
    "        print(f\"   â”‚  Too long removed: {stats['removed_too_long']:>8,}{' ' * (65 - len(f'Too long removed: {stats['removed_too_long']:,}') - 2)}â”‚\")\n",
    "        print(f\"   â”‚  Duplicates removed: {stats['removed_duplicates']:>8,}{' ' * (65 - len(f'Duplicates removed: {stats['removed_duplicates']:,}') - 2)}â”‚\")\n",
    "        print(f\"   â”‚  Final: {stats['step3_final']:>8,} sequences{' ' * (65 - len(f'Final: {stats['step3_final']:,} sequences') - 2)}â”‚\")\n",
    "        print(f\"   {'â””' + 'â”€' * 65 + 'â”˜'}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    print(f\"   ğŸ“Š Internal non_AVP (UniProt) filtering details:\")\n",
    "    stats = non_avp_detailed_stats\n",
    "    print(f\"   {'â”Œ' + 'â”€' * 65 + 'â”'}\")\n",
    "    print(f\"   â”‚  {'UniProt non_AVP':<61s}  â”‚\")\n",
    "    print(f\"   {'â”œ' + 'â”€' * 65 + 'â”¤'}\")\n",
    "    print(f\"   â”‚  Original: {stats['step0_original']:>8,} sequences{' ' * (65 - len(f'Original: {stats['step0_original']:,} sequences') - 2)}â”‚\")\n",
    "    print(f\"   â”‚  Non-natural AA removed: {stats['removed_non_natural_aa']:>8,}{' ' * (65 - len(f'Non-natural AA removed: {stats['removed_non_natural_aa']:,}') - 2)}â”‚\")\n",
    "    print(f\"   â”‚  Too short removed: {stats['removed_too_short']:>8,}{' ' * (65 - len(f'Too short removed: {stats['removed_too_short']:,}') - 2)}â”‚\")\n",
    "    print(f\"   â”‚  Too long removed: {stats['removed_too_long']:>8,}{' ' * (65 - len(f'Too long removed: {stats['removed_too_long']:,}') - 2)}â”‚\")\n",
    "    print(f\"   â”‚  Duplicates removed: {stats['removed_duplicates']:>8,}{' ' * (65 - len(f'Duplicates removed: {stats['removed_duplicates']:,}') - 2)}â”‚\")\n",
    "    print(f\"   â”‚  Final: {stats['step3_final']:>8,} sequences{' ' * (65 - len(f'Final: {stats['step3_final']:,} sequences') - 2)}â”‚\")\n",
    "    print(f\"   {'â””' + 'â”€' * 65 + 'â”˜'}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”¹ EXTERNAL DATASETS DETAILED BREAKDOWN:\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    for source, stats in external_detailed_stats.items():\n",
    "        print(f\"   {'â”Œ' + 'â”€' * 75 + 'â”'}\")\n",
    "        print(f\"   â”‚  ğŸ“Š {source:<69s}  â”‚\")\n",
    "        print(f\"   {'â”œ' + 'â”€' * 75 + 'â”¤'}\")\n",
    "        \n",
    "        if 'original_avp_count' in stats:\n",
    "            # æ··åˆæ ‡ç­¾æ•°æ®é›†\n",
    "            orig_text = f\"Original: {stats['step0_original']:,} (AVP: {stats['original_avp_count']}, non_AVP: {stats['original_non_avp_count']})\"\n",
    "            print(f\"   â”‚  {orig_text:<73s}  â”‚\")\n",
    "            print(f\"   â”‚  Non-natural AA removed: {stats['removed_non_natural_aa']:>8,}{' ' * (75 - len(f'Non-natural AA removed: {stats['removed_non_natural_aa']:,}') - 2)}â”‚\")\n",
    "            print(f\"   â”‚  Too short removed: {stats['removed_too_short']:>8,}{' ' * (75 - len(f'Too short removed: {stats['removed_too_short']:,}') - 2)}â”‚\")\n",
    "            print(f\"   â”‚  Too long removed: {stats['removed_too_long']:>8,}{' ' * (75 - len(f'Too long removed: {stats['removed_too_long']:,}') - 2)}â”‚\")\n",
    "            print(f\"   â”‚  Duplicates removed: {stats['removed_duplicates']:>8,}{' ' * (75 - len(f'Duplicates removed: {stats['removed_duplicates']:,}') - 2)}â”‚\")\n",
    "            final_text = f\"Final: {stats['step3_final']:,} (AVP: {stats['filtered_avp_count']}, non_AVP: {stats['filtered_non_avp_count']})\"\n",
    "            print(f\"   â”‚  {final_text:<73s}  â”‚\")\n",
    "        else:\n",
    "            # å•ä¸€æ ‡ç­¾æ•°æ®é›†\n",
    "            orig_text = f\"Original: {stats['step0_original']:,} sequences ({stats.get('type', 'Unknown')})\"\n",
    "            print(f\"   â”‚  {orig_text:<73s}  â”‚\")\n",
    "            print(f\"   â”‚  Non-natural AA removed: {stats['removed_non_natural_aa']:>8,}{' ' * (75 - len(f'Non-natural AA removed: {stats['removed_non_natural_aa']:,}') - 2)}â”‚\")\n",
    "            print(f\"   â”‚  Too short removed: {stats['removed_too_short']:>8,}{' ' * (75 - len(f'Too short removed: {stats['removed_too_short']:,}') - 2)}â”‚\")\n",
    "            print(f\"   â”‚  Too long removed: {stats['removed_too_long']:>8,}{' ' * (75 - len(f'Too long removed: {stats['removed_too_long']:,}') - 2)}â”‚\")\n",
    "            print(f\"   â”‚  Duplicates removed: {stats['removed_duplicates']:>8,}{' ' * (75 - len(f'Duplicates removed: {stats['removed_duplicates']:,}') - 2)}â”‚\")\n",
    "            print(f\"   â”‚  Final: {stats['step3_final']:>8,} sequences{' ' * (75 - len(f'Final: {stats['step3_final']:,} sequences') - 2)}â”‚\")\n",
    "        \n",
    "        print(f\"   {'â””' + 'â”€' * 75 + 'â”˜'}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"{'â•' * 80}\")\n",
    "    print(f\"ğŸ”¹ FINAL DATASET SUMMARY:\")\n",
    "    print(f\"{'â•' * 80}\")\n",
    "    print(f\"   ğŸ“ˆ External AVP: {len(external_avp_dataset):,} sequences (Initial_TR_AVP.csv)\")\n",
    "    print(f\"   ğŸ“ˆ External non_AVP: {len(external_non_avp_dataset):,} sequences (Initial_TR_non_AVP.csv)\")\n",
    "    print(f\"   ğŸ“ˆ Internal AVP: {len(internal_avp_dataset):,} sequences (Initial_TS_AVP.csv)\")\n",
    "    print(f\"   ğŸ“ˆ Internal non_AVP: {len(internal_non_avp_dataset):,} sequences (Initial_TS_non_AVP.csv)\") \n",
    "\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    grand_total = len(internal_avp_dataset) + len(internal_non_avp_dataset) + len(external_avp_dataset) + len(external_non_avp_dataset)\n",
    "    print(f\"   ğŸ“Š Grand Total: {grand_total:,} sequences\")\n",
    "    print(f\"{'â•' * 80}\")\n",
    "    \n",
    "    # âœ… æ·»åŠ ï¼šä¿å­˜Step 1å®Œæ•´æ‘˜è¦æ—¥å¿—\n",
    "    import json\n",
    "    \n",
    "    overall_log = {\n",
    "        'step': 'Complete Step 1 Processing',\n",
    "        'internal_avp_stats': internal_avp_detailed_stats,\n",
    "        'internal_non_avp_stats': non_avp_detailed_stats,\n",
    "        'external_stats': external_detailed_stats,\n",
    "        'final_summary': {\n",
    "            'internal_avp_count': len(internal_avp_dataset),\n",
    "            'internal_non_avp_count': len(internal_non_avp_dataset),\n",
    "            'external_avp_count': len(external_avp_dataset),\n",
    "            'external_non_avp_count': len(external_non_avp_dataset),\n",
    "            'grand_total': grand_total\n",
    "        },\n",
    "        'saved_files': saved_files\n",
    "    }\n",
    "    \n",
    "    overall_log_file = os.path.join(log_dir, \"step1_complete_processing_summary.json\")\n",
    "    with open(overall_log_file, 'w') as f:\n",
    "        json.dump(overall_log, f, indent=2, default=str)\n",
    "    print(f\"\\nğŸ“„ Complete Step 1 summary saved to: {overall_log_file}\")\n",
    "    \n",
    "    return {\n",
    "        'internal_avp': internal_avp_dataset,\n",
    "        'internal_non_avp': internal_non_avp_dataset,\n",
    "        'external_avp': external_avp_dataset,\n",
    "        'external_non_avp': external_non_avp_dataset,\n",
    "        'internal_avp_stats': internal_avp_detailed_stats,\n",
    "        'internal_non_avp_stats': non_avp_detailed_stats,\n",
    "        'external_stats': external_detailed_stats,\n",
    "        'saved_files': saved_files\n",
    "    }\n",
    "\n",
    "# è¿è¡Œè¯¦ç»†çš„ç¬¬ä¸€æ­¥å¤„ç†\n",
    "print(\"ğŸš€ Starting Step 1: Detailed Dataset Processing and Statistics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    step1_detailed_results = step1_process_datasets_detailed()\n",
    "    \n",
    "    if step1_detailed_results:\n",
    "        print(\"\\nâœ… Step 1 completed successfully with detailed statistics!\")\n",
    "        print(f\"   ğŸ“ Results stored in step1_detailed_results variable\")\n",
    "        print(f\"   ğŸ’¾ CSV files saved in: 1_Data/Processed_data_set/Initial_merged_data_set/\")\n",
    "        print(f\"   ğŸ“Š Ready for Step 2: Overlap analysis\")\n",
    "    else:\n",
    "        print(\"âŒ Step 1 failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during Step 1: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nğŸ¯ Step 1 Complete - Detailed filtering statistics generated for all datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48a311",
   "metadata": {},
   "source": [
    "# 2.æ•°æ®é›†ç‹¬ç«‹åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69ff0b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Starting Step 2: Enhanced Overlap Analysis and Dataset Independence\n",
      "================================================================================\n",
      "\n",
      "ğŸš€ STEP 2: ENHANCED OVERLAP ANALYSIS AND DATASET INDEPENDENCE\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š INITIAL DATASET OVERVIEW:\n",
      "   ğŸ“ˆ Initial_TR_AVP: 3,412 sequences\n",
      "   ğŸ“ˆ Initial_TR_non_AVP: 4,049 sequences\n",
      "   ğŸ“ˆ Initial_TS_AVP: 4,993 sequences\n",
      "   ğŸ“ˆ Initial_TS_non_AVP: 9,402 sequences\n",
      "   ğŸ“Š Total: 21,856 sequences\n",
      "\n",
      "ğŸ” PHASE 1: LABEL CONFLICT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "   ğŸ” Analyzing label conflicts across datasets...\n",
      "   âš ï¸  Found 521 sequences with label conflicts\n",
      "   ğŸ“Š Total conflict records: 521\n",
      "   âœ… Label conflicts saved to: 1_Data/Processed_data_set/Final_merged_data_set/Overlap_data.csv\n",
      "\n",
      "ğŸ§¹ PHASE 2: REMOVING CONFLICT SEQUENCES FROM DATASETS\n",
      "================================================================================\n",
      "   ğŸ“Š Initial_TS_AVP:\n",
      "      Original: 4,993 sequences\n",
      "      Cleaned: 4,474 sequences\n",
      "      Removed: 519 sequences (10.39%)\n",
      "   ğŸ“Š Initial_TS_non_AVP:\n",
      "      Original: 9,402 sequences\n",
      "      Cleaned: 9,172 sequences\n",
      "      Removed: 230 sequences (2.45%)\n",
      "   ğŸ“Š Initial_TR_AVP:\n",
      "      Original: 3,412 sequences\n",
      "      Cleaned: 3,091 sequences\n",
      "      Removed: 321 sequences (9.41%)\n",
      "   ğŸ“Š Initial_TR_non_AVP:\n",
      "      Original: 4,049 sequences\n",
      "      Cleaned: 3,719 sequences\n",
      "      Removed: 330 sequences (8.15%)\n",
      "\n",
      "ğŸ”„ PHASE 3: REMOVING DUPLICATE SEQUENCES\n",
      "================================================================================\n",
      "   ğŸ” Comparing datasets for duplicates...\n",
      "\n",
      "   ğŸ“ˆ Duplicate removal results:\n",
      "      AVP datasets overlap: 3,071 sequences\n",
      "      non_AVP datasets overlap: 1,278 sequences\n",
      "      TS_AVP (unique custom): 1,403 sequences\n",
      "      TS_non_AVP (unique custom): 7,894 sequences\n",
      "      TR_AVP (external): 3,091 sequences\n",
      "      TR_non_AVP (external): 3,719 sequences\n",
      "\n",
      "ğŸ’¾ PHASE 4: SAVING FINAL DATASETS\n",
      "================================================================================\n",
      "   âœ… TS_AVP: 1,403 sequences â†’ TS_AVP.csv\n",
      "   âœ… TS_non_AVP: 7,894 sequences â†’ TS_non_AVP.csv\n",
      "   âœ… TR_AVP: 3,091 sequences â†’ TR_AVP.csv\n",
      "   âœ… TR_non_AVP: 3,719 sequences â†’ TR_non_AVP.csv\n",
      "\n",
      "âœ… PHASE 5: INDEPENDENCE VERIFICATION\n",
      "================================================================================\n",
      "   âœ… TS_AVP â†” TS_non_AVP: Independent\n",
      "   âœ… TS_AVP â†” TR_AVP: Independent\n",
      "   âœ… TS_AVP â†” TR_non_AVP: Independent\n",
      "   âœ… TS_non_AVP â†” TR_AVP: Independent\n",
      "   âœ… TS_non_AVP â†” TR_non_AVP: Independent\n",
      "   âœ… TR_AVP â†” TR_non_AVP: Independent\n",
      "\n",
      "   ğŸ‰ ALL DATASETS ARE COMPLETELY INDEPENDENT!\n",
      "\n",
      "ğŸ“‹ COMPREHENSIVE FINAL REPORT\n",
      "================================================================================\n",
      "\n",
      "ğŸ”¹ LABEL CONFLICT ANALYSIS:\n",
      "   âš ï¸  Conflicting sequences: 521\n",
      "   ğŸ“„ Overlap data file: Overlap_data.csv\n",
      "\n",
      "ğŸ”¹ DATASET TRANSFORMATION:\n",
      "   Initial_TS_AVP           :  4,993 â†’  4,474 (-519)\n",
      "   Initial_TS_non_AVP       :  9,402 â†’  9,172 (-230)\n",
      "   Initial_TR_AVP           :  3,412 â†’  3,091 (-321)\n",
      "   Initial_TR_non_AVP       :  4,049 â†’  3,719 (-330)\n",
      "\n",
      "ğŸ”¹ FINAL INDEPENDENT DATASETS:\n",
      "   TS_AVP         :  1,403 sequences\n",
      "   TS_non_AVP     :  7,894 sequences\n",
      "   TR_AVP         :  3,091 sequences\n",
      "   TR_non_AVP     :  3,719 sequences\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   Total Final    : 16,107 sequences\n",
      "\n",
      "ğŸ”¹ DATA REDUCTION SUMMARY:\n",
      "   Initial total: 21,856 sequences\n",
      "   Final total: 16,107 sequences\n",
      "   Reduction: 5,749 sequences (26.30%)\n",
      "\n",
      "   ğŸ“„ Statistics saved to: 2_Log/2.1_Training set_and_test_set_processing/step2_overlap_analysis_statistics.json\n",
      "\n",
      "âœ… Step 2 completed successfully!\n",
      "   ğŸ“ Results stored in step2_results variable\n",
      "   ğŸ‰ All final datasets are completely independent!\n",
      "   ğŸ’¾ Final datasets saved to: 1_Data/Processed_data_set/Final_merged_data_set/\n",
      "   ğŸ“Š Files created:\n",
      "      - TS_AVP.csv\n",
      "      - TS_non_AVP.csv\n",
      "      - TR_AVP.csv\n",
      "      - TR_non_AVP.csv\n",
      "      - Overlap_data.csv (label conflicts)\n",
      "      - processing_statistics.json (detailed stats)\n",
      "\n",
      "ğŸ¯ Step 2 Complete - Final independent datasets ready for model training!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ç¬¬äºŒæ­¥ï¼šé‡å åˆ†æå’Œæ•°æ®é›†ç‹¬ç«‹åŒ–\n",
    "# ============================================================================\n",
    "\n",
    "def step2_overlap_analysis_enhanced(step1_results):\n",
    "    \"\"\"ç¬¬äºŒæ­¥ï¼šå¢å¼ºç‰ˆé‡å åˆ†æå’Œæ•°æ®é›†ç‹¬ç«‹åŒ–\"\"\"\n",
    "    print(\"\\nğŸš€ STEP 2: ENHANCED OVERLAP ANALYSIS AND DATASET INDEPENDENCE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # è·å–å››ä¸ªæ•°æ®é›†\n",
    "    initial_custom_avp = step1_results['internal_avp'].copy()\n",
    "    initial_custom_non_avp = step1_results['internal_non_avp'].copy()\n",
    "    initial_tr_avp = step1_results['external_avp'].copy()\n",
    "    initial_tr_non_avp = step1_results['external_non_avp'].copy()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š INITIAL DATASET OVERVIEW:\")\n",
    "    print(f\"   ğŸ“ˆ Initial_TR_AVP: {len(initial_tr_avp):,} sequences\")\n",
    "    print(f\"   ğŸ“ˆ Initial_TR_non_AVP: {len(initial_tr_non_avp):,} sequences\")\n",
    "    print(f\"   ğŸ“ˆ Initial_TS_AVP: {len(initial_custom_avp):,} sequences\")\n",
    "    print(f\"   ğŸ“ˆ Initial_TS_non_AVP: {len(initial_custom_non_avp):,} sequences\")\n",
    "\n",
    "    print(f\"   ğŸ“Š Total: {len(initial_custom_avp) + len(initial_custom_non_avp) + len(initial_tr_avp) + len(initial_tr_non_avp):,} sequences\")\n",
    "    \n",
    "    # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "    output_dir = \"1_Data/Processed_data_set/Final_merged_data_set\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # âœ… æ·»åŠ ï¼šåˆ›å»ºæ—¥å¿—ç›®å½•\n",
    "    log_dir = \"2_Log/2.1_Training set_and_test_set_processing\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # ========== ç¬¬ä¸€é˜¶æ®µï¼šæ ‡ç­¾å†²çªåˆ†æ ==========\n",
    "    print(f\"\\nğŸ” PHASE 1: LABEL CONFLICT ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # åˆ›å»ºæ•°æ®é›†å­—å…¸ï¼Œä¾¿äºåˆ†æ\n",
    "    datasets = {\n",
    "        'Initial_TS_AVP': {'data': initial_custom_avp, 'expected_label': 1},        # âœ… ä¿®æ”¹\n",
    "        'Initial_TS_non_AVP': {'data': initial_custom_non_avp, 'expected_label': 0}, # âœ… ä¿®æ”¹\n",
    "        'Initial_TR_AVP': {'data': initial_tr_avp, 'expected_label': 1},\n",
    "        'Initial_TR_non_AVP': {'data': initial_tr_non_avp, 'expected_label': 0}\n",
    "    }\n",
    "    \n",
    "    # æ”¶é›†æ‰€æœ‰åºåˆ—åŠå…¶æ¥æºå’Œæ ‡ç­¾\n",
    "    sequence_sources = {}\n",
    "    \n",
    "    for dataset_name, dataset_info in datasets.items():\n",
    "        data = dataset_info['data']\n",
    "        expected_label = dataset_info['expected_label']\n",
    "        \n",
    "        for _, row in data.iterrows():\n",
    "            sequence = row['Sequence'].upper()\n",
    "            actual_label = row['Label']\n",
    "            \n",
    "            if sequence not in sequence_sources:\n",
    "                sequence_sources[sequence] = []\n",
    "            \n",
    "            sequence_sources[sequence].append({\n",
    "                'dataset': dataset_name,\n",
    "                'expected_label': expected_label,\n",
    "                'actual_label': actual_label,\n",
    "                'source': row['Source'] if 'Source' in row else 'Unknown',\n",
    "                'id': row['Id'] if 'Id' in row else 'Unknown'\n",
    "            })\n",
    "    \n",
    "    # è¯†åˆ«æ ‡ç­¾å†²çª\n",
    "    label_conflicts = []\n",
    "    conflict_sequences = set()\n",
    "    \n",
    "    print(f\"\\n   ğŸ” Analyzing label conflicts across datasets...\")\n",
    "    \n",
    "    for sequence, sources in sequence_sources.items():\n",
    "        if len(sources) > 1:  # åºåˆ—å‡ºç°åœ¨å¤šä¸ªæ•°æ®é›†ä¸­\n",
    "            labels = [source['expected_label'] for source in sources]\n",
    "            if len(set(labels)) > 1:  # æ ‡ç­¾ä¸ä¸€è‡´\n",
    "                conflict_sequences.add(sequence)\n",
    "                \n",
    "                # åˆ›å»ºå†²çªè®°å½•\n",
    "                conflict_record = {\n",
    "                    'Sequence': sequence,\n",
    "                    'Length': len(sequence),\n",
    "                    'Conflict_Type': 'Label_Mismatch',\n",
    "                    'Dataset_Count': len(sources)\n",
    "                }\n",
    "                \n",
    "                # æ·»åŠ æ¯ä¸ªæ•°æ®é›†çš„ä¿¡æ¯\n",
    "                for i, source in enumerate(sources):\n",
    "                    conflict_record[f'Dataset_{i+1}'] = source['dataset']\n",
    "                    conflict_record[f'Label_{i+1}'] = source['expected_label']\n",
    "                    conflict_record[f'Source_{i+1}'] = source['source']\n",
    "                    conflict_record[f'ID_{i+1}'] = source['id']\n",
    "                \n",
    "                # å¡«å……ç©ºåˆ—ï¼ˆå¦‚æœæŸäº›åºåˆ—å‡ºç°æ¬¡æ•°å°‘äºæœ€å¤§å‡ºç°æ¬¡æ•°ï¼‰\n",
    "                max_occurrences = max(len(sources) for sources in sequence_sources.values() if len(sources) > 1)\n",
    "                for i in range(len(sources), max_occurrences):\n",
    "                    conflict_record[f'Dataset_{i+1}'] = ''\n",
    "                    conflict_record[f'Label_{i+1}'] = ''\n",
    "                    conflict_record[f'Source_{i+1}'] = ''\n",
    "                    conflict_record[f'ID_{i+1}'] = ''\n",
    "                \n",
    "                label_conflicts.append(conflict_record)\n",
    "    \n",
    "    print(f\"   âš ï¸  Found {len(conflict_sequences):,} sequences with label conflicts\")\n",
    "    print(f\"   ğŸ“Š Total conflict records: {len(label_conflicts):,}\")\n",
    "    \n",
    "    # ä¿å­˜æ ‡ç­¾å†²çªæ•°æ®\n",
    "    if label_conflicts:\n",
    "        conflicts_df = pd.DataFrame(label_conflicts)\n",
    "        conflicts_file = os.path.join(output_dir, \"Overlap_data.csv\")\n",
    "        conflicts_df.to_csv(conflicts_file, index=False)\n",
    "        print(f\"   âœ… Label conflicts saved to: {conflicts_file}\")\n",
    "    else:\n",
    "        print(f\"   ğŸ‰ No label conflicts found!\")\n",
    "    \n",
    "    # ========== ç¬¬äºŒé˜¶æ®µï¼šä»æ•°æ®é›†ä¸­ç§»é™¤æ ‡ç­¾å†²çªåºåˆ— ==========\n",
    "    print(f\"\\nğŸ§¹ PHASE 2: REMOVING CONFLICT SEQUENCES FROM DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ç§»é™¤å†²çªåºåˆ—\n",
    "    cleaned_datasets = {}\n",
    "    removal_stats = {}\n",
    "    \n",
    "    for dataset_name, dataset_info in datasets.items():\n",
    "        original_data = dataset_info['data']\n",
    "        original_count = len(original_data)\n",
    "        \n",
    "        # ç§»é™¤å†²çªåºåˆ—\n",
    "        cleaned_data = original_data[~original_data['Sequence'].str.upper().isin(conflict_sequences)].copy()\n",
    "        cleaned_count = len(cleaned_data)\n",
    "        removed_count = original_count - cleaned_count\n",
    "        \n",
    "        cleaned_datasets[dataset_name] = cleaned_data\n",
    "        removal_stats[dataset_name] = {\n",
    "            'original': original_count,\n",
    "            'cleaned': cleaned_count,\n",
    "            'removed': removed_count,\n",
    "            'removal_rate': (removed_count / original_count * 100) if original_count > 0 else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"   ğŸ“Š {dataset_name}:\")\n",
    "        print(f\"      Original: {original_count:,} sequences\")\n",
    "        print(f\"      Cleaned: {cleaned_count:,} sequences\")\n",
    "        print(f\"      Removed: {removed_count:,} sequences ({removal_stats[dataset_name]['removal_rate']:.2f}%)\")\n",
    "    \n",
    "    # ========== ç¬¬ä¸‰é˜¶æ®µï¼šå»é™¤é‡å¤åºåˆ— ==========\n",
    "    print(f\"\\nğŸ”„ PHASE 3: REMOVING DUPLICATE SEQUENCES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # æå–æ¸…ç†åçš„æ•°æ®é›†\n",
    "    cleaned_tr_avp = cleaned_datasets['Initial_TR_AVP']\n",
    "    cleaned_tr_non_avp = cleaned_datasets['Initial_TR_non_AVP']\n",
    "    cleaned_custom_avp = cleaned_datasets['Initial_TS_AVP']\n",
    "    cleaned_custom_non_avp = cleaned_datasets['Initial_TS_non_AVP']\n",
    "   \n",
    "    # TRæ•°æ®é›†çš„åºåˆ—é›†åˆ\n",
    "    tr_avp_sequences = set(cleaned_tr_avp['Sequence'].str.upper())\n",
    "    tr_non_avp_sequences = set(cleaned_tr_non_avp['Sequence'].str.upper())\n",
    "    \n",
    "    print(f\"   ğŸ” Comparing datasets for duplicates...\")\n",
    "    \n",
    "    # TS_AVP: Initial_TS_AVPä¸­ä¸Initial_TR_AVPä¸é‡å¤çš„åºåˆ—\n",
    "    custom_avp_sequences = set(cleaned_custom_avp['Sequence'].str.upper())\n",
    "    ts_avp_sequences = custom_avp_sequences - tr_avp_sequences\n",
    "    ts_avp_data = cleaned_custom_avp[cleaned_custom_avp['Sequence'].str.upper().isin(ts_avp_sequences)].copy()\n",
    "    \n",
    "    # TS_non_AVP: Initial_TS_non_AVPä¸­ä¸Initial_TR_non_AVPä¸é‡å¤çš„åºåˆ—\n",
    "    custom_non_avp_sequences = set(cleaned_custom_non_avp['Sequence'].str.upper())\n",
    "    ts_non_avp_sequences = custom_non_avp_sequences - tr_non_avp_sequences\n",
    "    ts_non_avp_data = cleaned_custom_non_avp[cleaned_custom_non_avp['Sequence'].str.upper().isin(ts_non_avp_sequences)].copy()\n",
    "    \n",
    "    # TRæ•°æ®é›†ä¿æŒä¸å˜ï¼ˆå·²æ¸…ç†å†²çªï¼‰\n",
    "    tr_avp_data = cleaned_tr_avp.copy()\n",
    "    tr_non_avp_data = cleaned_tr_non_avp.copy()\n",
    "    \n",
    "    # ç»Ÿè®¡å»é‡ç»“æœ\n",
    "    avp_overlap = len(custom_avp_sequences.intersection(tr_avp_sequences))\n",
    "    non_avp_overlap = len(custom_non_avp_sequences.intersection(tr_non_avp_sequences))\n",
    "    \n",
    "    print(f\"\\n   ğŸ“ˆ Duplicate removal results:\")\n",
    "    print(f\"      AVP datasets overlap: {avp_overlap:,} sequences\")\n",
    "    print(f\"      non_AVP datasets overlap: {non_avp_overlap:,} sequences\")\n",
    "    print(f\"      TS_AVP (unique custom): {len(ts_avp_data):,} sequences\")\n",
    "    print(f\"      TS_non_AVP (unique custom): {len(ts_non_avp_data):,} sequences\")\n",
    "    print(f\"      TR_AVP (external): {len(tr_avp_data):,} sequences\")\n",
    "    print(f\"      TR_non_AVP (external): {len(tr_non_avp_data):,} sequences\")\n",
    "    \n",
    "    # ========== ç¬¬å››é˜¶æ®µï¼šä¿å­˜æœ€ç»ˆæ•°æ®é›† ==========\n",
    "    print(f\"\\nğŸ’¾ PHASE 4: SAVING FINAL DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    final_datasets = {\n",
    "        'TS_AVP': ts_avp_data,\n",
    "        'TS_non_AVP': ts_non_avp_data,\n",
    "        'TR_AVP': tr_avp_data,\n",
    "        'TR_non_AVP': tr_non_avp_data\n",
    "    }\n",
    "    \n",
    "    saved_files = {}\n",
    "    columns_order = ['Id', 'Sequence', 'Source', 'Length', 'Label', 'Type']\n",
    "    \n",
    "    for dataset_name, data in final_datasets.items():\n",
    "        if len(data) > 0:\n",
    "            # ç¡®ä¿åˆ—é¡ºåº\n",
    "            available_columns = [col for col in columns_order if col in data.columns]\n",
    "            data_to_save = data[available_columns].copy()\n",
    "            \n",
    "            filename = f\"{dataset_name}.csv\"\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            data_to_save.to_csv(file_path, index=False)\n",
    "            saved_files[dataset_name] = file_path\n",
    "            \n",
    "            print(f\"   âœ… {dataset_name}: {len(data):,} sequences â†’ {filename}\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  {dataset_name}: No data to save\")\n",
    "    \n",
    "    # ========== ç¬¬äº”é˜¶æ®µï¼šç‹¬ç«‹æ€§éªŒè¯ ==========\n",
    "    print(f\"\\nâœ… PHASE 5: INDEPENDENCE VERIFICATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # åˆ›å»ºæœ€ç»ˆæ•°æ®é›†çš„åºåˆ—é›†åˆ\n",
    "    final_sequences = {}\n",
    "    for name, data in final_datasets.items():\n",
    "        if len(data) > 0:\n",
    "            final_sequences[name] = set(data['Sequence'].str.upper())\n",
    "        else:\n",
    "            final_sequences[name] = set()\n",
    "    \n",
    "    # éªŒè¯ç‹¬ç«‹æ€§\n",
    "    verification_passed = True\n",
    "    overlaps_found = []\n",
    "    \n",
    "    dataset_names = list(final_sequences.keys())\n",
    "    for i, name1 in enumerate(dataset_names):\n",
    "        for j, name2 in enumerate(dataset_names):\n",
    "            if i < j:  # é¿å…é‡å¤æ£€æŸ¥\n",
    "                overlap = final_sequences[name1].intersection(final_sequences[name2])\n",
    "                if len(overlap) > 0:\n",
    "                    verification_passed = False\n",
    "                    overlaps_found.append((name1, name2, len(overlap)))\n",
    "                    print(f\"   âŒ OVERLAP FOUND: {name1} â†” {name2}: {len(overlap):,} sequences\")\n",
    "                else:\n",
    "                    print(f\"   âœ… {name1} â†” {name2}: Independent\")\n",
    "    \n",
    "    if verification_passed:\n",
    "        print(f\"\\n   ğŸ‰ ALL DATASETS ARE COMPLETELY INDEPENDENT!\")\n",
    "    else:\n",
    "        print(f\"\\n   âš ï¸  WARNING: {len(overlaps_found)} overlaps found between final datasets\")\n",
    "    \n",
    "    # ========== æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š ==========\n",
    "    print(f\"\\nğŸ“‹ COMPREHENSIVE FINAL REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nğŸ”¹ LABEL CONFLICT ANALYSIS:\")\n",
    "    print(f\"   âš ï¸  Conflicting sequences: {len(conflict_sequences):,}\")\n",
    "    print(f\"   ğŸ“„ Overlap data file: Overlap_data.csv\")\n",
    "    \n",
    "    print(f\"\\nğŸ”¹ DATASET TRANSFORMATION:\")\n",
    "    for name, stats in removal_stats.items():\n",
    "        print(f\"   {name:25s}: {stats['original']:>6,} â†’ {stats['cleaned']:>6,} (-{stats['removed']:,})\")\n",
    "    \n",
    "    print(f\"\\nğŸ”¹ FINAL INDEPENDENT DATASETS:\")\n",
    "    total_final = 0\n",
    "    for name, data in final_datasets.items():\n",
    "        count = len(data)\n",
    "        total_final += count\n",
    "        print(f\"   {name:15s}: {count:>6,} sequences\")\n",
    "    \n",
    "    print(f\"   {'â”€' * 40}\")\n",
    "    print(f\"   {'Total Final':15s}: {total_final:>6,} sequences\")\n",
    "    \n",
    "    print(f\"\\nğŸ”¹ DATA REDUCTION SUMMARY:\")\n",
    "    total_initial = sum(len(dataset['data']) for dataset in datasets.values())\n",
    "    reduction = total_initial - total_final\n",
    "    reduction_rate = (reduction / total_initial * 100) if total_initial > 0 else 0\n",
    "    print(f\"   Initial total: {total_initial:,} sequences\")\n",
    "    print(f\"   Final total: {total_final:,} sequences\")\n",
    "    print(f\"   Reduction: {reduction:,} sequences ({reduction_rate:.2f}%)\")\n",
    "    \n",
    "    final_stats = {\n",
    "        'label_conflicts': {\n",
    "            'conflict_sequences_count': len(conflict_sequences),\n",
    "            'conflict_records_count': len(label_conflicts)\n",
    "        },\n",
    "        'removal_stats': removal_stats,\n",
    "        'final_datasets': {name: len(data) for name, data in final_datasets.items()},\n",
    "        'independence_verification': {\n",
    "            'passed': verification_passed,\n",
    "            'overlaps_found': overlaps_found\n",
    "        },\n",
    "        'summary': {\n",
    "            'initial_total': total_initial,\n",
    "            'final_total': total_final,\n",
    "            'reduction': reduction,\n",
    "            'reduction_rate': reduction_rate\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    # âœ… ä¿®æ”¹ï¼šä¿å­˜åˆ°æ—¥å¿—ç›®å½•\n",
    "    stats_file = os.path.join(log_dir, \"step2_overlap_analysis_statistics.json\")\n",
    "    \n",
    "    # âœ… æ·»åŠ ï¼šæ›´è¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯\n",
    "    detailed_log = {\n",
    "        'step': 'Step 2 - Overlap Analysis and Dataset Independence',\n",
    "        'statistics': final_stats,\n",
    "        'saved_files': saved_files,\n",
    "        'verification_passed': verification_passed\n",
    "    }\n",
    "    \n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(detailed_log, f, indent=2, default=str)\n",
    "    print(f\"\\n   ğŸ“„ Statistics saved to: {stats_file}\")\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'final_datasets': final_datasets,\n",
    "        'label_conflicts': label_conflicts,\n",
    "        'conflict_sequences': conflict_sequences,\n",
    "        'removal_stats': removal_stats,\n",
    "        'saved_files': saved_files,\n",
    "        'verification_passed': verification_passed,\n",
    "        'final_stats': final_stats\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# è¿è¡Œç¬¬äºŒæ­¥åˆ†æ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸš€ Starting Step 2: Enhanced Overlap Analysis and Dataset Independence\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # ç¡®ä¿ç¬¬ä¸€æ­¥å·²å®Œæˆ\n",
    "    if 'step1_detailed_results' not in locals():\n",
    "        print(\"âŒ Error: Step 1 results not found. Please run Step 1 first.\")\n",
    "    else:\n",
    "        step2_results = step2_overlap_analysis_enhanced(step1_detailed_results)\n",
    "        \n",
    "        if step2_results:\n",
    "            print(\"\\nâœ… Step 2 completed successfully!\")\n",
    "            print(f\"   ğŸ“ Results stored in step2_results variable\")\n",
    "            \n",
    "            if step2_results['verification_passed']:\n",
    "                print(f\"   ğŸ‰ All final datasets are completely independent!\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸  Warning: Some overlaps detected in final datasets\")\n",
    "            \n",
    "            print(f\"   ğŸ’¾ Final datasets saved to: 1_Data/Processed_data_set/Final_merged_data_set/\")\n",
    "            print(f\"   ğŸ“Š Files created:\")\n",
    "            for dataset_name, file_path in step2_results['saved_files'].items():\n",
    "                print(f\"      - {dataset_name}.csv\")\n",
    "            print(f\"      - Overlap_data.csv (label conflicts)\")\n",
    "            print(f\"      - processing_statistics.json (detailed stats)\")\n",
    "        else:\n",
    "            print(\"âŒ Step 2 failed\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during Step 2: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nğŸ¯ Step 2 Complete - Final independent datasets ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17607b26",
   "metadata": {},
   "source": [
    "# 3.csv to fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4140caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Converting TR datasets to CSV and FASTA formats\n",
      "======================================================================\n",
      "ğŸ“Š Reading CSV files:\n",
      "--------------------------------------------------\n",
      "   âœ… TR_AVP.csv: 3,091 sequences\n",
      "   âœ… TR_non_AVP.csv: 3,719 sequences\n",
      "\n",
      "ğŸ“Š Combined dataset: 6,810 sequences\n",
      "\n",
      "ğŸ”§ ID Duplicate Check and Fix:\n",
      "--------------------------------------------------\n",
      "ğŸ” Checking for duplicate IDs...\n",
      "   âš ï¸  Found 76 duplicate ID groups affecting 152 sequences\n",
      "   ğŸ“‹ Duplicate ID examples:\n",
      "      'neg1786': 2 occurrences\n",
      "      'neg1762': 2 occurrences\n",
      "      'neg1761': 2 occurrences\n",
      "      'neg1740': 2 occurrences\n",
      "      'neg1703': 2 occurrences\n",
      "      ... and 71 more\n",
      "   âœ… Applied 76 ID fixes\n",
      "   ğŸ“ ID fix examples:\n",
      "      neg2124 â†’ neg2124_1\n",
      "      neg105 â†’ neg105_1\n",
      "      neg171 â†’ neg171_1\n",
      "      neg299 â†’ neg299_1\n",
      "      neg560 â†’ neg560_1\n",
      "      ... and 71 more fixes\n",
      "\n",
      "ğŸ”€ Shuffling data with random seed 42...\n",
      "\n",
      "ğŸ’¾ Saving CSV file: 1_Data/Processed_data_set/Final_merged_data_set/TR.csv\n",
      "ğŸ’¾ Saving FASTA file: 1_Data/Processed_data_set/Final_merged_data_set/TR.fasta\n",
      "\n",
      "ğŸ“ˆ Final Statistics:\n",
      "   ğŸ”§ ID Duplicate Processing:\n",
      "      Original duplicates: 152 sequences in 76 groups\n",
      "      Fixes applied: 76\n",
      "      Final unique IDs: 6,810\n",
      "   ğŸ“Š By dataset:\n",
      "      TR_non_AVP: 3,719 sequences\n",
      "      TR_AVP: 3,091 sequences\n",
      "   ğŸ“Š By label:\n",
      "      0: 3,719 sequences\n",
      "      1: 3,091 sequences\n",
      "   ğŸ“Š By type:\n",
      "      non_AVP: 3,719 sequences\n",
      "      AVP: 3,091 sequences\n",
      "   ğŸ“Š Total: 6,810 sequences\n",
      "   âœ… CSV saved: 1_Data/Processed_data_set/Final_merged_data_set/TR.csv\n",
      "   âœ… FASTA saved: 1_Data/Processed_data_set/Final_merged_data_set/TR.fasta\n",
      "   ğŸ“„ Conversion log saved to: 2_Log/2.1_Training set_and_test_set_processing/step3_TR_conversion_log.json\n",
      "\n",
      "ğŸš€ Converting TS datasets to CSV and FASTA formats\n",
      "======================================================================\n",
      "ğŸ“Š Reading CSV files:\n",
      "--------------------------------------------------\n",
      "   âœ… TS_AVP.csv: 1,403 sequences\n",
      "   âœ… TS_non_AVP.csv: 7,894 sequences\n",
      "\n",
      "ğŸ“Š Combined dataset: 9,297 sequences\n",
      "\n",
      "ğŸ”§ ID Duplicate Check and Fix:\n",
      "--------------------------------------------------\n",
      "ğŸ” Checking for duplicate IDs...\n",
      "   âš ï¸  Found 2 duplicate ID groups affecting 4 sequences\n",
      "   ğŸ“‹ Duplicate ID examples:\n",
      "      '22846': 2 occurrences\n",
      "      '5102': 2 occurrences\n",
      "   âœ… Applied 2 ID fixes\n",
      "   ğŸ“ ID fix examples:\n",
      "      22846 â†’ 22846_1\n",
      "      5102 â†’ 5102_1\n",
      "\n",
      "ğŸ”€ Shuffling data with random seed 42...\n",
      "\n",
      "ğŸ’¾ Saving CSV file: 1_Data/Processed_data_set/Final_merged_data_set/TS.csv\n",
      "ğŸ’¾ Saving FASTA file: 1_Data/Processed_data_set/Final_merged_data_set/TS.fasta\n",
      "\n",
      "ğŸ“ˆ Final Statistics:\n",
      "   ğŸ”§ ID Duplicate Processing:\n",
      "      Original duplicates: 4 sequences in 2 groups\n",
      "      Fixes applied: 2\n",
      "      Final unique IDs: 9,297\n",
      "   ğŸ“Š By dataset:\n",
      "      TS_non_AVP: 7,894 sequences\n",
      "      TS_AVP: 1,403 sequences\n",
      "   ğŸ“Š By label:\n",
      "      0: 7,894 sequences\n",
      "      1: 1,403 sequences\n",
      "   ğŸ“Š By type:\n",
      "      non_AVP: 7,894 sequences\n",
      "      AVP: 1,403 sequences\n",
      "   ğŸ“Š Total: 9,297 sequences\n",
      "   âœ… CSV saved: 1_Data/Processed_data_set/Final_merged_data_set/TS.csv\n",
      "   âœ… FASTA saved: 1_Data/Processed_data_set/Final_merged_data_set/TS.fasta\n",
      "   ğŸ“„ Conversion log saved to: 2_Log/2.1_Training set_and_test_set_processing/step3_TS_conversion_log.json\n",
      "\n",
      "ğŸ“‹ CONVERSION SUMMARY\n",
      "======================================================================\n",
      "ğŸ“Š TR Dataset:\n",
      "   âœ… TR.csv: 6,810 sequences (6,810 unique IDs)\n",
      "   âœ… TR.fasta: 6,810 sequences\n",
      "\n",
      "ğŸ“Š TS Dataset:\n",
      "   âœ… TS.csv: 9,297 sequences (9,297 unique IDs)\n",
      "   âœ… TS.fasta: 9,297 sequences\n",
      "\n",
      "ğŸ“ All files saved in: 1_Data/Processed_data_set/Final_merged_data_set\n",
      "ğŸ² Random seed used: 42\n",
      "\n",
      "ğŸ¯ Conversion complete! All files generated with random seed 42.\n",
      "ğŸ’¡ Duplicate IDs have been automatically resolved with numeric suffixes (_1, _2, etc.).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def check_and_fix_duplicate_ids(df):\n",
    "    \"\"\"\n",
    "    æ£€æŸ¥å¹¶ä¿®å¤é‡å¤çš„IDï¼Œä¸ºé‡å¤IDæ·»åŠ æ•°å­—åç¼€\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with columns including 'Id', 'Source', 'Dataset'\n",
    "    \n",
    "    Returns:\n",
    "    df: DataFrame with fixed IDs\n",
    "    duplicate_info: dict with duplicate statistics\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” Checking for duplicate IDs...\")\n",
    "    \n",
    "    # è®°å½•åŸå§‹IDå’Œä¿®å¤åçš„ID\n",
    "    original_ids = df['Id'].copy()\n",
    "    id_counts = defaultdict(int)\n",
    "    fixed_ids = []\n",
    "    duplicate_info = {\n",
    "        'total_duplicates': 0,\n",
    "        'duplicate_groups': 0,\n",
    "        'fixes_applied': 0\n",
    "    }\n",
    "    \n",
    "    # ç¬¬ä¸€æ¬¡éå†ï¼šç»Ÿè®¡æ¯ä¸ªIDçš„å‡ºç°æ¬¡æ•°\n",
    "    for idx, row in df.iterrows():\n",
    "        original_id = str(row['Id']) if pd.notna(row['Id']) else f\"Unknown_{idx}\"\n",
    "        id_counts[original_id] += 1\n",
    "    \n",
    "    # æ‰¾å‡ºé‡å¤çš„ID\n",
    "    duplicate_ids = {id_val: count for id_val, count in id_counts.items() if count > 1}\n",
    "    \n",
    "    if duplicate_ids:\n",
    "        duplicate_info['duplicate_groups'] = len(duplicate_ids)\n",
    "        duplicate_info['total_duplicates'] = sum(duplicate_ids.values())\n",
    "        \n",
    "        print(f\"   âš ï¸  Found {len(duplicate_ids)} duplicate ID groups affecting {sum(duplicate_ids.values())} sequences\")\n",
    "        \n",
    "        # æ˜¾ç¤ºé‡å¤IDç¤ºä¾‹\n",
    "        print(f\"   ğŸ“‹ Duplicate ID examples:\")\n",
    "        for i, (dup_id, count) in enumerate(list(duplicate_ids.items())[:5]):\n",
    "            print(f\"      '{dup_id}': {count} occurrences\")\n",
    "        if len(duplicate_ids) > 5:\n",
    "            print(f\"      ... and {len(duplicate_ids) - 5} more\")\n",
    "    else:\n",
    "        print(f\"   âœ… No duplicate IDs found\")\n",
    "        return df, duplicate_info\n",
    "    \n",
    "    # ç¬¬äºŒæ¬¡éå†ï¼šä¿®å¤é‡å¤ID\n",
    "    occurrence_counter = defaultdict(int)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        original_id = str(row['Id']) if pd.notna(row['Id']) else f\"Unknown_{idx}\"\n",
    "        \n",
    "        if original_id in duplicate_ids:\n",
    "            occurrence_counter[original_id] += 1\n",
    "            \n",
    "            if occurrence_counter[original_id] == 1:\n",
    "                # ç¬¬ä¸€æ¬¡å‡ºç°ï¼Œä¿æŒåŸID\n",
    "                fixed_id = original_id\n",
    "            else:\n",
    "                # åç»­å‡ºç°ï¼Œæ·»åŠ æ•°å­—åç¼€\n",
    "                suffix_number = occurrence_counter[original_id] - 1  # ä»_1å¼€å§‹\n",
    "                \n",
    "                # å¦‚æœåŸIDä»¥|ç»“å°¾ï¼Œåœ¨|å‰æ·»åŠ åç¼€\n",
    "                if original_id.endswith('|'):\n",
    "                    fixed_id = f\"{original_id[:-1]}_{suffix_number}|\"\n",
    "                else:\n",
    "                    fixed_id = f\"{original_id}_{suffix_number}\"\n",
    "                \n",
    "                duplicate_info['fixes_applied'] += 1\n",
    "        else:\n",
    "            # éé‡å¤IDï¼Œä¿æŒåŸæ ·\n",
    "            fixed_id = original_id\n",
    "        \n",
    "        fixed_ids.append(fixed_id)\n",
    "    \n",
    "    # æ›´æ–°DataFrame\n",
    "    df = df.copy()\n",
    "    df['Id'] = fixed_ids\n",
    "    \n",
    "    print(f\"   âœ… Applied {duplicate_info['fixes_applied']} ID fixes\")\n",
    "    \n",
    "    # æ˜¾ç¤ºä¿®å¤ç¤ºä¾‹\n",
    "    if duplicate_info['fixes_applied'] > 0:\n",
    "        print(f\"   ğŸ“ ID fix examples:\")\n",
    "        fix_count = 0\n",
    "        for i, (orig, fixed) in enumerate(zip(original_ids, fixed_ids)):\n",
    "            if orig != fixed and fix_count < 5:\n",
    "                print(f\"      {orig} â†’ {fixed}\")\n",
    "                fix_count += 1\n",
    "        if duplicate_info['fixes_applied'] > 5:\n",
    "            print(f\"      ... and {duplicate_info['fixes_applied'] - 5} more fixes\")\n",
    "    \n",
    "    return df, duplicate_info\n",
    "\n",
    "def merge_datasets_to_both_formats(csv_files, output_base_path, random_seed=42):\n",
    "    \"\"\"\n",
    "    å°†å¤šä¸ªCSVæ–‡ä»¶åˆå¹¶ä¸ºFASTAå’ŒCSVä¸¤ç§æ ¼å¼ï¼Œä¿ç•™æ‰€æœ‰ä¿¡æ¯å¹¶æ‰“ä¹±é¡ºåº\n",
    "    å¤„ç†é‡å¤IDé—®é¢˜\n",
    "    \n",
    "    Parameters:\n",
    "    csv_files: list of dict, åŒ…å«æ–‡ä»¶è·¯å¾„å’Œæ ‡ç­¾ä¿¡æ¯\n",
    "    output_base_path: str, è¾“å‡ºæ–‡ä»¶çš„åŸºç¡€è·¯å¾„ï¼ˆä¸å«æ‰©å±•åï¼‰\n",
    "    random_seed: int, éšæœºç§å­\n",
    "    \"\"\"\n",
    "    # âœ… æ·»åŠ ï¼šåˆ›å»ºæ—¥å¿—ç›®å½•\n",
    "    log_dir = \"2_Log/2.1_Training set_and_test_set_processing\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    print(f\"ğŸ“Š Reading CSV files:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # è¯»å–æ¯ä¸ªCSVæ–‡ä»¶\n",
    "    for file_info in csv_files:\n",
    "        csv_path = file_info['path']\n",
    "        dataset_type = file_info['type']\n",
    "        \n",
    "        if os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path)\n",
    "            \n",
    "            print(f\"   âœ… {os.path.basename(csv_path)}: {len(df):,} sequences\")\n",
    "            \n",
    "            # æ·»åŠ æ•°æ®é›†ç±»å‹ä¿¡æ¯\n",
    "            df = df.copy()\n",
    "            df['Dataset'] = dataset_type\n",
    "            \n",
    "            # å°†æ•°æ®æ·»åŠ åˆ°æ€»åˆ—è¡¨\n",
    "            all_data.append(df)\n",
    "        else:\n",
    "            print(f\"   âŒ File not found: {csv_path}\")\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"âŒ No data found!\")\n",
    "        return False, False\n",
    "    \n",
    "    # åˆå¹¶æ‰€æœ‰æ•°æ®\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Combined dataset: {len(combined_df):,} sequences\")\n",
    "    \n",
    "    # ========== æ£€æŸ¥å’Œä¿®å¤é‡å¤ID ==========\n",
    "    print(f\"\\nğŸ”§ ID Duplicate Check and Fix:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    fixed_df, duplicate_info = check_and_fix_duplicate_ids(combined_df)\n",
    "    \n",
    "    # è®¾ç½®éšæœºç§å­å¹¶æ‰“ä¹±é¡ºåº\n",
    "    print(f\"\\nğŸ”€ Shuffling data with random seed {random_seed}...\")\n",
    "    np.random.seed(random_seed)\n",
    "    shuffled_df = fixed_df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    \n",
    "    # ç”Ÿæˆè¾“å‡ºè·¯å¾„\n",
    "    csv_output_path = f\"{output_base_path}.csv\"\n",
    "    fasta_output_path = f\"{output_base_path}.fasta\"\n",
    "    \n",
    "    # ========== ä¿å­˜CSVæ–‡ä»¶ ==========\n",
    "    print(f\"\\nğŸ’¾ Saving CSV file: {csv_output_path}\")\n",
    "    \n",
    "    # ç¡®ä¿åˆ—é¡ºåº\n",
    "    desired_columns = ['Id', 'Sequence', 'Source', 'Length', 'Label', 'Type', 'Dataset']\n",
    "    available_columns = [col for col in desired_columns if col in shuffled_df.columns]\n",
    "    \n",
    "    # ä¿å­˜CSV\n",
    "    shuffled_df[available_columns].to_csv(csv_output_path, index=False)\n",
    "    csv_success = True\n",
    "    \n",
    "    # ========== ä¿å­˜FASTAæ–‡ä»¶ ==========\n",
    "    print(f\"ğŸ’¾ Saving FASTA file: {fasta_output_path}\")\n",
    "    \n",
    "    fasta_success = True\n",
    "    try:\n",
    "        with open(fasta_output_path, 'w') as fasta_file:\n",
    "            for _, row in shuffled_df.iterrows():\n",
    "                # æ„å»ºFASTA headerï¼ŒåŒ…å«æ‰€æœ‰ä¿¡æ¯\n",
    "                header_parts = []\n",
    "                \n",
    "                # æ·»åŠ æ‰€æœ‰å¯ç”¨ä¿¡æ¯\n",
    "                if 'Id' in row and pd.notna(row['Id']):\n",
    "                    header_parts.append(f\"ID={row['Id']}\")\n",
    "                \n",
    "                if 'Label' in row and pd.notna(row['Label']):\n",
    "                    header_parts.append(f\"Label={row['Label']}\")\n",
    "                \n",
    "                if 'Type' in row and pd.notna(row['Type']):\n",
    "                    header_parts.append(f\"Type={row['Type']}\")\n",
    "                \n",
    "                if 'Source' in row and pd.notna(row['Source']):\n",
    "                    header_parts.append(f\"Source={row['Source']}\")\n",
    "                \n",
    "                if 'Length' in row and pd.notna(row['Length']):\n",
    "                    header_parts.append(f\"Length={row['Length']}\")\n",
    "                \n",
    "                if 'Dataset' in row and pd.notna(row['Dataset']):\n",
    "                    header_parts.append(f\"Dataset={row['Dataset']}\")\n",
    "                \n",
    "                # æ„å»ºå®Œæ•´çš„FASTA header\n",
    "                fasta_header = '|'.join(header_parts)\n",
    "                \n",
    "                # è·å–åºåˆ—\n",
    "                sequence = str(row['Sequence']) if pd.notna(row['Sequence']) else ''\n",
    "                \n",
    "                # å†™å…¥FASTAæ ¼å¼\n",
    "                fasta_file.write(f\">{fasta_header}\\n\")\n",
    "                fasta_file.write(f\"{sequence}\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error writing FASTA file: {e}\")\n",
    "        fasta_success = False\n",
    "    \n",
    "    # ========== ç»Ÿè®¡ä¿¡æ¯ ==========\n",
    "    print(f\"\\nğŸ“ˆ Final Statistics:\")\n",
    "    \n",
    "    # IDé‡å¤å¤„ç†ç»Ÿè®¡\n",
    "    if duplicate_info['total_duplicates'] > 0:\n",
    "        print(f\"   ğŸ”§ ID Duplicate Processing:\")\n",
    "        print(f\"      Original duplicates: {duplicate_info['total_duplicates']} sequences in {duplicate_info['duplicate_groups']} groups\")\n",
    "        print(f\"      Fixes applied: {duplicate_info['fixes_applied']}\")\n",
    "        print(f\"      Final unique IDs: {shuffled_df['Id'].nunique():,}\")\n",
    "    \n",
    "    # æŒ‰æ•°æ®é›†ç»Ÿè®¡\n",
    "    dataset_counts = shuffled_df['Dataset'].value_counts()\n",
    "    print(f\"   ğŸ“Š By dataset:\")\n",
    "    for dataset, count in dataset_counts.items():\n",
    "        print(f\"      {dataset}: {count:,} sequences\")\n",
    "    \n",
    "    # æŒ‰æ ‡ç­¾ç»Ÿè®¡\n",
    "    if 'Label' in shuffled_df.columns:\n",
    "        label_counts = shuffled_df['Label'].value_counts()\n",
    "        print(f\"   ğŸ“Š By label:\")\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"      {label}: {count:,} sequences\")\n",
    "    \n",
    "    # æŒ‰ç±»å‹ç»Ÿè®¡\n",
    "    if 'Type' in shuffled_df.columns:\n",
    "        type_counts = shuffled_df['Type'].value_counts()\n",
    "        print(f\"   ğŸ“Š By type:\")\n",
    "        for type_name, count in type_counts.items():\n",
    "            print(f\"      {type_name}: {count:,} sequences\")\n",
    "    \n",
    "    print(f\"   ğŸ“Š Total: {len(shuffled_df):,} sequences\")\n",
    "    \n",
    "    if csv_success:\n",
    "        print(f\"   âœ… CSV saved: {csv_output_path}\")\n",
    "    if fasta_success:\n",
    "        print(f\"   âœ… FASTA saved: {fasta_output_path}\")\n",
    "    \n",
    "    # âœ… æ·»åŠ ï¼šä¿å­˜è½¬æ¢æ—¥å¿—\n",
    "    import json\n",
    "    \n",
    "    conversion_log = {\n",
    "        'step': 'Step 3 - CSV to FASTA Conversion',\n",
    "        'output_base_path': output_base_path,\n",
    "        'random_seed': random_seed,\n",
    "        'csv_success': csv_success,\n",
    "        'fasta_success': fasta_success,\n",
    "        'duplicate_info': duplicate_info,\n",
    "        'final_statistics': {\n",
    "            'total_sequences': len(shuffled_df),\n",
    "            'unique_ids': shuffled_df['Id'].nunique(),\n",
    "            'dataset_counts': dataset_counts.to_dict(),\n",
    "            'label_counts': label_counts.to_dict() if 'Label' in shuffled_df.columns else {},\n",
    "            'type_counts': type_counts.to_dict() if 'Type' in shuffled_df.columns else {}\n",
    "        },\n",
    "        'files_created': {\n",
    "            'csv_file': csv_output_path if csv_success else None,\n",
    "            'fasta_file': fasta_output_path if fasta_success else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # ç¡®å®šæ—¥å¿—æ–‡ä»¶å\n",
    "    dataset_name = os.path.basename(output_base_path)\n",
    "    log_file = os.path.join(log_dir, f\"step3_{dataset_name}_conversion_log.json\")\n",
    "    \n",
    "    with open(log_file, 'w') as f:\n",
    "        json.dump(conversion_log, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"   ğŸ“„ Conversion log saved to: {log_file}\")\n",
    "    \n",
    "    return csv_success, fasta_success\n",
    "\n",
    "# ============================================================================\n",
    "# åˆå¹¶TRæ•°æ®é›† (TR_AVP.csv + TR_non_AVP.csv â†’ TR.csv + TR.fasta)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ğŸš€ Converting TR datasets to CSV and FASTA formats\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "base_dir = \"1_Data/Processed_data_set/Final_merged_data_set\"\n",
    "\n",
    "# TRæ•°æ®é›†é…ç½®\n",
    "tr_csv_files = [\n",
    "    {\n",
    "        'path': os.path.join(base_dir, \"TR_AVP.csv\"),\n",
    "        'type': 'TR_AVP'\n",
    "    },\n",
    "    {\n",
    "        'path': os.path.join(base_dir, \"TR_non_AVP.csv\"),\n",
    "        'type': 'TR_non_AVP'\n",
    "    }\n",
    "]\n",
    "\n",
    "tr_output_base = os.path.join(base_dir, \"TR\")\n",
    "\n",
    "# è½¬æ¢TRæ•°æ®é›†\n",
    "tr_csv_success, tr_fasta_success = merge_datasets_to_both_formats(\n",
    "    tr_csv_files, tr_output_base, random_seed=42\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# åˆå¹¶TSæ•°æ®é›† (TS_AVP.csv + TS_non_AVP.csv â†’ TS.csv + TS.fasta)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nğŸš€ Converting TS datasets to CSV and FASTA formats\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TSæ•°æ®é›†é…ç½®\n",
    "ts_csv_files = [\n",
    "    {\n",
    "        'path': os.path.join(base_dir, \"TS_AVP.csv\"),\n",
    "        'type': 'TS_AVP'\n",
    "    },\n",
    "    {\n",
    "        'path': os.path.join(base_dir, \"TS_non_AVP.csv\"),\n",
    "        'type': 'TS_non_AVP'\n",
    "    }\n",
    "]\n",
    "\n",
    "ts_output_base = os.path.join(base_dir, \"TS\")\n",
    "\n",
    "# è½¬æ¢TSæ•°æ®é›†\n",
    "ts_csv_success, ts_fasta_success = merge_datasets_to_both_formats(\n",
    "    ts_csv_files, ts_output_base, random_seed=42\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# æ€»ç»“æŠ¥å‘Š\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nğŸ“‹ CONVERSION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TRæ•°æ®é›†ç»“æœ\n",
    "print(f\"ğŸ“Š TR Dataset:\")\n",
    "if tr_csv_success:\n",
    "    tr_csv_path = os.path.join(base_dir, \"TR.csv\")\n",
    "    if os.path.exists(tr_csv_path):\n",
    "        tr_df = pd.read_csv(tr_csv_path)\n",
    "        unique_ids = tr_df['Id'].nunique()\n",
    "        total_rows = len(tr_df)\n",
    "        print(f\"   âœ… TR.csv: {total_rows:,} sequences ({unique_ids:,} unique IDs)\")\n",
    "    else:\n",
    "        print(f\"   âŒ TR.csv: File not found\")\n",
    "else:\n",
    "    print(f\"   âŒ TR.csv: Failed to create\")\n",
    "\n",
    "if tr_fasta_success:\n",
    "    tr_fasta_path = os.path.join(base_dir, \"TR.fasta\")\n",
    "    if os.path.exists(tr_fasta_path):\n",
    "        with open(tr_fasta_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        seq_count = len([line for line in lines if line.startswith('>')])\n",
    "        print(f\"   âœ… TR.fasta: {seq_count:,} sequences\")\n",
    "    else:\n",
    "        print(f\"   âŒ TR.fasta: File not found\")\n",
    "else:\n",
    "    print(f\"   âŒ TR.fasta: Failed to create\")\n",
    "\n",
    "# TSæ•°æ®é›†ç»“æœ\n",
    "print(f\"\\nğŸ“Š TS Dataset:\")\n",
    "if ts_csv_success:\n",
    "    ts_csv_path = os.path.join(base_dir, \"TS.csv\")\n",
    "    if os.path.exists(ts_csv_path):\n",
    "        ts_df = pd.read_csv(ts_csv_path)\n",
    "        unique_ids = ts_df['Id'].nunique()\n",
    "        total_rows = len(ts_df)\n",
    "        print(f\"   âœ… TS.csv: {total_rows:,} sequences ({unique_ids:,} unique IDs)\")\n",
    "    else:\n",
    "        print(f\"   âŒ TS.csv: File not found\")\n",
    "else:\n",
    "    print(f\"   âŒ TS.csv: Failed to create\")\n",
    "\n",
    "if ts_fasta_success:\n",
    "    ts_fasta_path = os.path.join(base_dir, \"TS.fasta\")\n",
    "    if os.path.exists(ts_fasta_path):\n",
    "        with open(ts_fasta_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        seq_count = len([line for line in lines if line.startswith('>')])\n",
    "        print(f\"   âœ… TS.fasta: {seq_count:,} sequences\")\n",
    "    else:\n",
    "        print(f\"   âŒ TS.fasta: File not found\")\n",
    "else:\n",
    "    print(f\"   âŒ TS.fasta: Failed to create\")\n",
    "\n",
    "print(f\"\\nğŸ“ All files saved in: {base_dir}\")\n",
    "print(f\"ğŸ² Random seed used: 42\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Conversion complete! All files generated with random seed 42.\")\n",
    "print(f\"ğŸ’¡ Duplicate IDs have been automatically resolved with numeric suffixes (_1, _2, etc.).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipynb (esmc)",
   "language": "python",
   "name": "esmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
