import os
import sys
import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight

import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import json
from datetime import datetime
import pickle
import time
warnings.filterwarnings('ignore')

# ==================== åŸºç¡€é…ç½® ====================
# è®¾ç½®å­—ä½“å’Œç”»å›¾å‚æ•°
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 12
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['savefig.dpi'] = 1200
plt.rcParams['figure.dpi'] = 1200

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
if os.getcwd() not in sys.path:
    sys.path.append(os.getcwd())

# å¯¼å…¥é¡¹ç›®ç‰¹å®šæ¨¡å—
from torch_geometric.loader import DataLoader
from Feature_extraction.utils import load_dataset
from .GAT import create_2layer_gat, create_3layer_gat, create_model

class GATLayerComparisonTrainer:
    """
    GATå±‚æ•°å¯¹æ¯”è®­ç»ƒå™¨
    ä¸“é—¨ç”¨äºæ¯”è¾ƒ2å±‚å’Œ3å±‚GATåœ¨ä¸åŒè·ç¦»é˜ˆå€¼æ•°æ®ä¸Šçš„è¡¨ç°
    """
    
    def __init__(self, base_save_dir="4_Results_TR_RS", seed=42, use_class_weights=True, show_plots=True):
        """
        åˆå§‹åŒ–GATå±‚æ•°å¯¹æ¯”è®­ç»ƒå™¨
        
        å‚æ•°:
            base_save_dir (str): åŸºç¡€ä¿å­˜ç›®å½•
            seed (int): éšæœºç§å­
            use_class_weights (bool): æ˜¯å¦ä½¿ç”¨ç±»åˆ«æƒé‡
            show_plots (bool): æ˜¯å¦æ˜¾ç¤ºå›¾è¡¨
        """
        self.base_save_dir = Path(base_save_dir)
        self.seed = seed
        self.use_class_weights = use_class_weights
        self.show_plots = show_plots
        
        # GATå±‚æ•°é…ç½®
        self.gat_configs = {
            '2Layer_GAT': {'num_layers': 2, 'name': '2Layer_GAT'},
            '3Layer_GAT': {'num_layers': 3, 'name': '3Layer_GAT'}
        }
        
        # è·ç¦»é˜ˆå€¼æ˜ å°„
        self.distance_mapping = {
            '4.0A': '4.0A',
            '8.0A': '8.0A', 
            '12.0A': '12.0A'
        }
        
        # åˆå§‹åŒ–è®¾å¤‡å’Œç¯å¢ƒ
        self.setup_seed()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ğŸ”¬ GAT Layer Comparison Trainer Initialized")
        print(f"ğŸ–¥ï¸ Using device: {self.device}")
        print(f"ğŸ“ Base save directory: {self.base_save_dir}")
        print(f"âš–ï¸ Using class weights: {self.use_class_weights}")
        print(f"ğŸ“Š Show plots: {self.show_plots}")
    
    def setup_seed(self):
        """è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å®éªŒç»“æœå¯é‡å¤"""
        torch.manual_seed(self.seed)
        torch.cuda.manual_seed(self.seed)
        torch.cuda.manual_seed_all(self.seed)
        np.random.seed(self.seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.enabled = False
        torch.use_deterministic_algorithms(True, warn_only=True)
        os.environ['PYTHONHASHSEED'] = str(self.seed)
        os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    
    def extract_distance_from_path(self, train_path):
        """ä»è·¯å¾„ä¸­æå–è·ç¦»é˜ˆå€¼"""
        path_str = str(train_path)
        for distance_key in self.distance_mapping.keys():
            if distance_key in path_str:
                return distance_key
        return 'unknown'
    
    def create_directory_structure(self, train_paths):
        """
        åˆ›å»ºGATå±‚æ•°å¯¹æ¯”çš„ç›®å½•ç»“æ„
        
        å‚æ•°:
            train_paths (list): è®­ç»ƒæ•°æ®è·¯å¾„åˆ—è¡¨
            
        è¿”å›:
            all_dirs (dict): æ‰€æœ‰ç›®å½•ç»“æ„çš„å­—å…¸
        """
        all_dirs = {}
        
        for train_path in train_paths:
            distance = self.extract_distance_from_path(train_path)
            
            # åˆ›å»ºè·ç¦»ç‰¹å®šçš„ä¸»ç›®å½•
            distance_dir = self.base_save_dir / f"GAT_Comparison_{distance}"
            distance_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¸ºæ¯ç§GATé…ç½®åˆ›å»ºå­ç›®å½•
            distance_structure = {}
            for gat_type, config in self.gat_configs.items():
                gat_name = config['name']
                gat_dir = distance_dir / gat_name
                
                distance_structure[gat_type] = {
                    'base_dir': gat_dir,
                    'models': gat_dir / "models",
                    'train_results': gat_dir / "train_results",
                    'training_plots': gat_dir / "training_plots"
                }
                
                # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•
                for dir_path in distance_structure[gat_type].values():
                    if isinstance(dir_path, Path):
                        dir_path.mkdir(parents=True, exist_ok=True)
            
            all_dirs[distance] = {
                'distance_dir': distance_dir,
                'gat_dirs': distance_structure
            }
            
            print(f"ğŸ“ Created directory structure for {distance}")
        
        return all_dirs
    
    def calculate_class_weights(self, labels):
        """è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†æ•°æ®ä¸å¹³è¡¡é—®é¢˜"""
        if not self.use_class_weights:
            return None
            
        unique_labels = np.unique(labels)
        class_weights = compute_class_weight('balanced', classes=unique_labels, y=labels)
        weight_dict = dict(zip(unique_labels, class_weights))
        
        print(f"âš–ï¸ Class weights: Class 0: {weight_dict[0]:.3f}, Class 1: {weight_dict[1]:.3f}")
        
        return torch.FloatTensor(class_weights).to(self.device)
    
    def calculate_specificity(self, y_true, y_pred):
        """è®¡ç®—ç‰¹å¼‚æ€§ (Specificity)"""
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        return specificity
    
    def evaluate_model(self, model, data_loader, class_weights=None):
        """è¯¦ç»†è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        model.eval()
        all_preds = []
        all_probs = []
        all_labels = []
        total_loss = 0

        if self.use_class_weights and class_weights is not None:
            criterion = torch.nn.CrossEntropyLoss(weight=class_weights)
        else:
            criterion = torch.nn.CrossEntropyLoss()
        
        with torch.no_grad():
            for batch in data_loader:
                batch = batch.to(self.device)
                
                # GATæ¨¡å‹çš„forwardæ¥å£
                outputs, _ = model(batch.x, batch.edge_index, batch.batch)
                
                loss = criterion(outputs, batch.y)
                total_loss += loss.item()
                
                probs = torch.softmax(outputs, dim=1)[:, 1]
                preds = torch.argmax(outputs, dim=1)
                
                all_preds.extend(preds.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
                all_labels.extend(batch.y.cpu().numpy())
        
        avg_loss = total_loss / len(data_loader)
        
        metrics = {
            'loss': avg_loss,
            'accuracy': accuracy_score(all_labels, all_preds),
            'precision': precision_score(all_labels, all_preds, average='binary'),
            'recall': recall_score(all_labels, all_preds, average='binary'),
            'f1': f1_score(all_labels, all_preds, average='binary'),
            'auc': roc_auc_score(all_labels, all_probs),
            'mcc': matthews_corrcoef(all_labels, all_preds),
            'specificity': self.calculate_specificity(all_labels, all_preds)
        }
        
        return metrics
    
    def train_single_fold(self, gat_type, model_params, training_params, 
                         fold_train_data, fold_val_data, fold_num, gat_dirs, 
                         class_weights, distance):
        """è®­ç»ƒå•ä¸ªfoldçš„GATæ¨¡å‹"""
        gat_config = self.gat_configs[gat_type]
        num_layers = gat_config['num_layers']
        gat_name = gat_config['name']
        
        print(f"\nğŸš€ Training {gat_name} on {distance} data - Fold {fold_num}")
        
        # åˆ›å»ºæ¨¡å‹
        model = create_model(
            model_params['node_feature_dim'], 
            num_layers=num_layers,
            **{k: v for k, v in model_params.items() if k != 'node_feature_dim'}
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer = torch.optim.AdamW(model.parameters(), 
                                     lr=training_params['lr'], 
                                     weight_decay=training_params.get('weight_decay', 0.01))
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=10, min_lr=1e-6, verbose=False
        )
        
        # æŸå¤±å‡½æ•°
        if self.use_class_weights and class_weights is not None:
            criterion = torch.nn.CrossEntropyLoss(weight=class_weights)
        else:
            criterion = torch.nn.CrossEntropyLoss()
        
        # æ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(fold_train_data, batch_size=training_params['batch_size'], shuffle=True)
        val_loader = DataLoader(fold_val_data, batch_size=training_params['batch_size'], shuffle=False)
        
        # è®­ç»ƒçŠ¶æ€
        best_val_auc = 0
        best_model_state = None
        best_epoch = 0
        best_metrics = None
        patience_counter = 0
        patience = 20
        
        # è®­ç»ƒå†å²
        training_history = {
            'train_losses': [],
            'val_losses': [],
            'train_accuracies': [],
            'val_accuracies': [],
            'val_aucs': []
        }
        
        # ä¸»è®­ç»ƒå¾ªç¯
        num_epochs = training_params['max_epochs']
        
        for epoch in range(num_epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            total_loss = 0
            train_preds = []
            train_labels = []
            
            for batch in train_loader:
                batch = batch.to(self.device)
                optimizer.zero_grad()
                
                outputs, _ = model(batch.x, batch.edge_index, batch.batch)
                loss = criterion(outputs, batch.y)
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                total_loss += loss.item()
                
                preds = torch.argmax(outputs, dim=1)
                train_preds.extend(preds.cpu().numpy())
                train_labels.extend(batch.y.cpu().numpy())
            
            # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
            avg_train_loss = total_loss / len(train_loader)
            train_accuracy = accuracy_score(train_labels, train_preds)
            
            training_history['train_losses'].append(avg_train_loss)
            training_history['train_accuracies'].append(train_accuracy)
            
            # éªŒè¯é˜¶æ®µ
            val_metrics = self.evaluate_model(model, val_loader, class_weights)
            val_loss = val_metrics['loss']
            val_accuracy = val_metrics['accuracy']
            val_auc = val_metrics['auc']
            
            training_history['val_losses'].append(val_loss)
            training_history['val_accuracies'].append(val_accuracy)
            training_history['val_aucs'].append(val_auc)
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step(val_auc)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_auc > best_val_auc:
                best_val_auc = val_auc
                best_epoch = epoch + 1
                best_model_state = model.state_dict().copy()
                best_metrics = val_metrics.copy()
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                best_model_path = gat_dirs[gat_type]['models'] / f'best_model_fold_{fold_num}.pth'
                torch.save({
                    'model_state_dict': best_model_state,
                    'fold': fold_num,
                    'best_val_auc': best_val_auc,
                    'best_epoch': best_epoch,
                    'best_val_metrics': best_metrics,
                    'model_params': model_params,
                    'training_params': training_params,
                    'num_layers': num_layers
                }, best_model_path)
                
            else:
                patience_counter += 1
            
            # è®­ç»ƒè¿›åº¦æ˜¾ç¤º
            if (epoch + 1) % 5 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                print(f"  Epoch {epoch+1}/{num_epochs} - Val AUC: {val_auc:.4f} - Val ACC: {val_accuracy:.4f} - LR: {current_lr:.6f}")
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= patience:
                print(f"  Early stopping at epoch {epoch+1}")
                print(f"  Best model from epoch {best_epoch} (AUC: {best_val_auc:.4f})")
                break
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_df = pd.DataFrame({
            'epoch': range(1, len(training_history['train_losses']) + 1),
            'train_loss': training_history['train_losses'],
            'val_loss': training_history['val_losses'],
            'train_accuracy': training_history['train_accuracies'],
            'val_accuracy': training_history['val_accuracies'],
            'val_auc': training_history['val_aucs']
        })
        numeric_columns = history_df.select_dtypes(include=[np.number]).columns.drop('epoch')
        history_df[numeric_columns] = history_df[numeric_columns].round(4)
        
        history_df.to_csv(gat_dirs[gat_type]['train_results'] / f'training_history_fold_{fold_num}.csv', index=False)
        
        print(f"âœ… {gat_name} Fold {fold_num} completed - Best AUC: {best_val_auc:.4f} at Epoch {best_epoch}")
        
        return best_metrics, training_history
    
    def train_distance_dataset(self, train_path, model_params, training_params, all_dirs, n_folds=5):
        """è®­ç»ƒå•ä¸ªè·ç¦»é˜ˆå€¼æ•°æ®é›†çš„æ‰€æœ‰GATé…ç½®"""
        distance = self.extract_distance_from_path(train_path)
        
        print(f"\n{'='*80}")
        print(f"ğŸ§¬ Training GAT Models on {distance} Dataset")
        print(f"ğŸ“ Data path: {train_path}")
        print(f"{'='*80}")
        
        # ğŸ” æ£€æŸ¥å·²å®Œæˆçš„è®­ç»ƒ
        distance_dirs = all_dirs[distance]
        gat_dirs = distance_dirs['gat_dirs']
        
        existing_results = {}
        for gat_type in self.gat_configs.keys():
            cv_summary_file = gat_dirs[gat_type]['train_results'] / 'cv_summary.json'
            if cv_summary_file.exists():
                try:
                    with open(cv_summary_file, 'r') as f:
                        existing_results[gat_type] = json.load(f)
                    print(f"âœ… Found existing results for {self.gat_configs[gat_type]['name']}")
                except:
                    print(f"âš ï¸ Corrupted results file for {self.gat_configs[gat_type]['name']}, will retrain")
        
        # å¦‚æœæ‰€æœ‰GATç±»å‹éƒ½å·²å®Œæˆï¼Œè·³è¿‡è®­ç»ƒ
        if len(existing_results) == len(self.gat_configs):
            print(f"ğŸ¯ All GAT models for {distance} already trained! Skipping...")
            
            # é‡æ„å·²æœ‰ç»“æœ
            distance_results = {}
            for gat_type, cv_summary in existing_results.items():
                distance_results[gat_type] = {
                    'cv_summary': cv_summary,
                    'fold_results': None,  
                    'training_history': None
                }
            
            return distance_results
        
        # åŠ è½½æ•°æ®ï¼ˆåªæœ‰åœ¨éœ€è¦è®­ç»ƒæ—¶æ‰åŠ è½½ï¼‰
        train_data = load_dataset(train_path)
        print(f"âœ… Loaded {len(train_data)} graphs")
        
        # æ•°æ®å‡†å¤‡
        train_labels = np.array([graph.y.item() for graph in train_data])
        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=self.seed)
        
        # å­˜å‚¨æ‰€æœ‰ç»“æœ
        distance_results = {}
        
        # è®­ç»ƒæ¯ç§GATé…ç½®
        for gat_type in self.gat_configs.keys():
            # ğŸ” æ£€æŸ¥å•ä¸ªGATç±»å‹æ˜¯å¦å·²å®Œæˆ
            if gat_type in existing_results:
                print(f"âœ… {self.gat_configs[gat_type]['name']} already trained, using existing results")
                distance_results[gat_type] = {
                    'cv_summary': existing_results[gat_type],
                    'fold_results': None,
                    'training_history': None
                }
                continue
            
            print(f"\n{'='*60}")
            print(f"ğŸ”„ Training {self.gat_configs[gat_type]['name']} on {distance} data")
            print(f"{'='*60}")
            
            fold_results = []
            all_training_history = []
            
            # KæŠ˜äº¤å‰éªŒè¯
            for fold, (train_idx, val_idx) in enumerate(skf.split(range(len(train_data)), train_labels)):
                print(f"\nğŸ”„ Fold {fold + 1}/{n_folds}")
                
                # æ•°æ®åˆ’åˆ†
                fold_train_data = [train_data[i] for i in train_idx]
                fold_val_data = [train_data[i] for i in val_idx]
                
                fold_train_labels = train_labels[train_idx]
                fold_val_labels = train_labels[val_idx]
                
                # æ‰“å°æ•°æ®åˆ†å¸ƒ
                print(f"Training: {len(fold_train_data)} samples (AVP: {sum(fold_train_labels)} - {sum(fold_train_labels)/len(fold_train_labels)*100:.1f}%)")
                print(f"Validation: {len(fold_val_data)} samples (AVP: {sum(fold_val_labels)} - {sum(fold_val_labels)/len(fold_val_labels)*100:.1f}%)")
                
                # è®¡ç®—ç±»åˆ«æƒé‡
                class_weights = self.calculate_class_weights(fold_train_labels)
                
                # è®­ç»ƒå½“å‰fold
                fold_metrics, fold_history = self.train_single_fold(
                    gat_type, model_params, training_params,
                    fold_train_data, fold_val_data, fold + 1, gat_dirs,
                    class_weights, distance
                )
                
                fold_results.append(fold_metrics)
                all_training_history.append(fold_history)
                
                # æ‰“å°foldç»“æœ
                print(f"Fold {fold + 1} Results:")
                for metric, value in fold_metrics.items():
                    print(f"  {metric}: {value:.4f}")
            
            # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
            cv_df = pd.DataFrame(fold_results)
            cv_df['fold'] = range(1, len(fold_results) + 1)
            
            numeric_columns = cv_df.select_dtypes(include=[np.number]).columns
            cv_df[numeric_columns] = cv_df[numeric_columns].round(4)
            
            cv_df.to_csv(gat_dirs[gat_type]['train_results'] / 'cross_validation_results.csv', index=False)
            
            # è®¡ç®—ç»Ÿè®¡æ‘˜è¦
            cv_summary = {}
            for metric in fold_results[0].keys():
                values = [fold[metric] for fold in fold_results]
                cv_summary[metric] = {
                    'mean': round(np.mean(values), 4),
                    'std': round(np.std(values), 4)
                }
            
            # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
            with open(gat_dirs[gat_type]['train_results'] / 'cv_summary.json', 'w') as f:
                json.dump(cv_summary, f, indent=2)
            
            # ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾
            self.plot_training_curves(gat_type, all_training_history, gat_dirs, distance)
            
            # ç”Ÿæˆç»“æœæ±‡æ€»å›¾
            self.plot_results_summary(gat_type, fold_results, gat_dirs, distance)
            
            distance_results[gat_type] = {
                'fold_results': fold_results,
                'cv_summary': cv_summary,
                'training_history': all_training_history
            }
            
            print(f"âœ… {self.gat_configs[gat_type]['name']} training completed!")
        
        # ç”Ÿæˆè·ç¦»ç‰¹å®šçš„æ¯”è¾ƒå›¾
        self.plot_distance_comparison(distance_results, distance_dirs['distance_dir'], distance)
        
        return distance_results
    
    def plot_training_curves(self, gat_type, all_training_history, gat_dirs, distance):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if not all_training_history:
            return
        
        gat_name = self.gat_configs[gat_type]['name']
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'{gat_name} on {distance} - 5-Fold Training Curves', 
                     fontsize=18, fontweight='bold', y=0.96)
        
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
        
        # è®­ç»ƒæŸå¤±
        ax1 = axes[0, 0]
        for fold_idx, history in enumerate(all_training_history):
            epochs = range(1, len(history['train_losses']) + 1)
            ax1.plot(epochs, history['train_losses'], color=colors[fold_idx], 
                    linewidth=2, label=f'Fold {fold_idx + 1}', alpha=0.8)
        ax1.set_title('Training Loss', fontweight='bold', fontsize=14)
        ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Training Loss', fontsize=12, fontweight='bold')
        ax1.legend(fontsize=10)
        ax1.grid(True, alpha=0.3)
        
        # éªŒè¯æŸå¤±
        ax2 = axes[0, 1]
        for fold_idx, history in enumerate(all_training_history):
            epochs = range(1, len(history['val_losses']) + 1)
            ax2.plot(epochs, history['val_losses'], color=colors[fold_idx], 
                    linewidth=2, label=f'Fold {fold_idx + 1}', alpha=0.8)
        ax2.set_title('Validation Loss', fontweight='bold', fontsize=14)
        ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Validation Loss', fontsize=12, fontweight='bold')
        ax2.legend(fontsize=10)
        ax2.grid(True, alpha=0.3)
        
        # éªŒè¯å‡†ç¡®åº¦
        ax3 = axes[1, 0]
        for fold_idx, history in enumerate(all_training_history):
            epochs = range(1, len(history['val_accuracies']) + 1)
            ax3.plot(epochs, history['val_accuracies'], color=colors[fold_idx], 
                    linewidth=2, label=f'Fold {fold_idx + 1}', alpha=0.8)
        ax3.set_title('Validation Accuracy', fontweight='bold', fontsize=14)
        ax3.set_xlabel('Epoch', fontsize=12, fontweight='bold')
        ax3.set_ylabel('Validation Accuracy', fontsize=12, fontweight='bold')
        ax3.legend(fontsize=10)
        ax3.grid(True, alpha=0.3)
        
        # éªŒè¯AUC
        ax4 = axes[1, 1]
        for fold_idx, history in enumerate(all_training_history):
            epochs = range(1, len(history['val_aucs']) + 1)
            ax4.plot(epochs, history['val_aucs'], color=colors[fold_idx], 
                    linewidth=2, label=f'Fold {fold_idx + 1}', alpha=0.8)
        ax4.set_title('Validation AUC', fontweight='bold', fontsize=14)
        ax4.set_xlabel('Epoch', fontsize=12, fontweight='bold')
        ax4.set_ylabel('Validation AUC', fontsize=12, fontweight='bold')
        ax4.legend(fontsize=10)
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout(rect=[0, 0, 1, 0.94])
        
        # ä¿å­˜å›¾è¡¨
        save_path = gat_dirs[gat_type]['training_plots'] / 'training_curves.png'
        plt.savefig(save_path, dpi=1200, bbox_inches='tight')
        
        if self.show_plots:
            plt.show()
        else:
            plt.close()
        
        print(f"ğŸ“Š Training curves saved: {save_path}")
    
    def plot_results_summary(self, gat_type, fold_results, gat_dirs, distance):
        """ç»˜åˆ¶ç»“æœæ±‡æ€»å›¾"""
        gat_name = self.gat_configs[gat_type]['name']
        
        metrics_mapping = {
            'accuracy': 'Accuracy',
            'recall': 'Sensitivity/SN/Recall', 
            'specificity': 'Specificity/SP',
            'f1': 'F1 Score',
            'mcc': 'MCC',
            'auc': 'AUC'
        }
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.ravel()
        
        for i, (metric_key, metric_label) in enumerate(metrics_mapping.items()):
            values = [fold[metric_key] for fold in fold_results]
            folds = range(1, len(fold_results) + 1)
            
            axes[i].bar(folds, values, alpha=0.7, color=plt.cm.Set3(i))
            axes[i].axhline(y=np.mean(values), color='red', linestyle='--', linewidth=2, 
                           label=f'Mean: {np.mean(values):.4f}')
            
            axes[i].set_title(f'{metric_label}', fontweight='bold', fontsize=12)
            axes[i].set_xlabel('Fold', fontsize=10, fontweight='bold')
            axes[i].set_ylabel(metric_label, fontsize=10, fontweight='bold')
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
            
            for j, v in enumerate(values):
                axes[i].text(j + 1, v + max(values) * 0.01, f'{v:.3f}', 
                           ha='center', va='bottom', fontsize=9, fontweight='bold')
        
        plt.suptitle(f'{gat_name} on {distance} - Cross-Validation Results', 
                     fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        save_path = gat_dirs[gat_type]['training_plots'] / 'results_summary.png'
        plt.savefig(save_path, dpi=1200, bbox_inches='tight')
        
        if self.show_plots:
            plt.show()
        else:
            plt.close()
        
        print(f"ğŸ“Š Results summary saved: {save_path}")
    
    def plot_distance_comparison(self, distance_results, distance_dir, distance):
        """ç»˜åˆ¶åŒä¸€è·ç¦»ä¸‹2å±‚å’Œ3å±‚GATçš„æ¯”è¾ƒå›¾"""
        metrics = ['accuracy', 'recall', 'specificity', 'f1', 'mcc', 'auc']
        metric_labels = ['Accuracy', 'Sensitivity/SN/Recall', 'Specificity/SP', 'F1 Score', 'MCC', 'AUC']
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.ravel()
        
        gat_types = ['2layer_gat', '3layer_gat']
        gat_names = ['2-Layer GAT', '3-Layer GAT']
        colors = ['#1f77b4', '#ff7f0e']
        
        for i, (metric, label) in enumerate(zip(metrics, metric_labels)):
            means = []
            stds = []
            
            for gat_type in gat_types:
                if gat_type in distance_results:
                    cv_summary = distance_results[gat_type]['cv_summary']
                    means.append(cv_summary[metric]['mean'])
                    stds.append(cv_summary[metric]['std'])
                else:
                    means.append(0)
                    stds.append(0)
            
            x_pos = np.arange(len(gat_names))
            bars = axes[i].bar(x_pos, means, yerr=stds, alpha=0.7, color=colors, capsize=5)
            
            axes[i].set_title(f'{label}', fontweight='bold', fontsize=12)
            axes[i].set_xlabel('GAT Configuration', fontsize=10, fontweight='bold')
            axes[i].set_ylabel(label, fontsize=10, fontweight='bold')
            axes[i].set_xticks(x_pos)
            axes[i].set_xticklabels(gat_names)
            axes[i].grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for j, (mean, std) in enumerate(zip(means, stds)):
                axes[i].text(j, mean + std + max(means) * 0.01, f'{mean:.3f}', 
                           ha='center', va='bottom', fontsize=10, fontweight='bold')
            
            # æ ‡è®°æ›´å¥½çš„ç»“æœ
            if len(means) == 2 and means[0] != means[1]:
                better_idx = 0 if means[0] > means[1] else 1
                bars[better_idx].set_edgecolor('green')
                bars[better_idx].set_linewidth(3)
        
        plt.suptitle(f'2-Layer vs 3-Layer GAT Comparison on {distance} Dataset', 
                     fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        save_path = distance_dir / 'gat_layer_comparison.png'
        plt.savefig(save_path, dpi=1200, bbox_inches='tight')
        
        if self.show_plots:
            plt.show()
        else:
            plt.close()
        
        print(f"ğŸ“Š GAT train results comparison saved: {save_path}")
    
    def generate_comprehensive_comparison(self, all_distance_results):
        """ç”Ÿæˆæ‰€æœ‰è·ç¦»å’Œå±‚æ•°çš„ç»¼åˆæ¯”è¾ƒæŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š Generating Comprehensive Train Performance Comparison Report")
        print(f"{'='*80}")
        
        # åˆ›å»ºç»¼åˆæ¯”è¾ƒç›®å½•
        comprehensive_dir = self.base_save_dir / "Comprehensive_Train_Comparison"
        comprehensive_dir.mkdir(parents=True, exist_ok=True)
        
        # å‡†å¤‡æ•°æ®
        comparison_data = []
        metrics = ['accuracy', 'recall', 'specificity', 'f1', 'mcc', 'auc']
        
        for distance, distance_results in all_distance_results.items():
            for gat_type, results in distance_results.items():
                row = {
                    'Distance': distance,
                    'GAT_Type': self.gat_configs[gat_type]['name'],
                    'Layers': self.gat_configs[gat_type]['num_layers']
                }
                
                cv_summary = results['cv_summary']
                for metric in metrics:
                    # ä¿®æ”¹è¿™é‡Œï¼šç¡®ä¿4ä½å°æ•°ç²¾åº¦
                    mean_val = round(cv_summary[metric]['mean'], 3)
                    std_val = round(cv_summary[metric]['std'], 3)
                    row[metric] = f"{mean_val:.3f} Â± {std_val:.3f}"
                
                comparison_data.append(row)
        
        # ä¿å­˜æ¯”è¾ƒè¡¨æ ¼
        comparison_df = pd.DataFrame(comparison_data)
        comparison_df.to_csv(comprehensive_dir / 'comprehensive_train_comparison.csv', index=False)
        
        # ç”Ÿæˆç»¼åˆæ¯”è¾ƒå›¾
        self.plot_comprehensive_comparison(comparison_data, comprehensive_dir)
        
        # ç”Ÿæˆæœ€ä½³é…ç½®åˆ†æ
        best_configs = self.analyze_best_configurations(comparison_data, comprehensive_dir)
        
        # ä¿®æ”¹è¿™é‡Œï¼šä¿å­˜å®Œæ•´æŠ¥å‘Šæ—¶ä¹Ÿä½¿ç”¨æ ¼å¼åŒ–å­—ç¬¦ä¸²
        report = {
            'comparison_data': comparison_data,
            'best_configurations': best_configs,
            'summary': {
                'total_experiments': len(comparison_data),
                'distance_thresholds': list(all_distance_results.keys()),
                'gat_configurations': list(self.gat_configs.keys())
            },
            # æ·»åŠ æ ¼å¼åŒ–çš„æœ€ä½³é…ç½®æ‘˜è¦
            'best_configurations_formatted': {}
        }
        
        # ä¸ºæœ€ä½³é…ç½®æ·»åŠ æ ¼å¼åŒ–ç‰ˆæœ¬
        for metric_name, config in best_configs.items():
            if config:
                report['best_configurations_formatted'][metric_name] = {
                    'config': f"{config['gat_type']} on {config['distance']}",
                    'performance': f"{config['score']:.3f} Â± {config['std']:.3f}"
                }
        
        # ä¸ºæ¯ä¸ªè·ç¦»å’ŒGATé…ç½®æ·»åŠ æ ¼å¼åŒ–ç‰ˆæœ¬
        report['formatted_results'] = {}
        for distance, distance_results in all_distance_results.items():
            report['formatted_results'][distance] = {}
            for gat_type, results in distance_results.items():
                gat_name = self.gat_configs[gat_type]['name']
                cv_summary = results['cv_summary']
                
                formatted_metrics = {}
                for metric in metrics:
                    mean_val = round(cv_summary[metric]['mean'], 3)
                    std_val = round(cv_summary[metric]['std'], 3)
                    formatted_metrics[metric] = f"{mean_val:.3f} Â± {std_val:.3f}"
                
                report['formatted_results'][distance][gat_name] = formatted_metrics
        
        with open(comprehensive_dir / 'comprehensive_train_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Comprehensive comparison complete! Results saved to: {comprehensive_dir}")
        
        return report
    
    def plot_comprehensive_comparison(self, comparison_data, comprehensive_dir):
        """ç»˜åˆ¶ç»¼åˆæ¯”è¾ƒå›¾è¡¨"""
        metrics = ['accuracy', 'recall', 'specificity', 'f1', 'mcc', 'auc']
        metric_labels = ['Accuracy', 'Sensitivity/SN/Recall', 'Specificity/SP', 'F1 Score', 'MCC', 'AUC']
        
        # åˆ›å»ºå¤§å‹ç»¼åˆæ¯”è¾ƒå›¾
        fig, axes = plt.subplots(2, 3, figsize=(24, 16))
        axes = axes.ravel()
        
        distances = ['4.0A', '8.0A', '12.0A']
        gat_types = ['2Layer_GAT', '3Layer_GAT']
        
        # è®¾ç½®é¢œè‰²
        colors = {'2Layer_GAT': '#1f77b4', '3Layer_GAT': '#ff7f0e'}
        
        for i, (metric, label) in enumerate(zip(metrics, metric_labels)):
            # å‡†å¤‡æ•°æ® - ä¿®æ”¹è¿™éƒ¨åˆ†
            data_2layer = []
            data_3layer = []
            std_2layer = []
            std_3layer = []
            
            for distance in distances:
                for row in comparison_data:
                    if row['Distance'] == distance:
                        # ä»æ ¼å¼åŒ–å­—ç¬¦ä¸²ä¸­æå–æ•°å€¼
                        metric_str = row[metric]  # æ ¼å¼: "0.8327 Â± 0.0169"
                        parts = metric_str.split(' Â± ')
                        mean_val = float(parts[0])
                        std_val = float(parts[1])
                        
                        if row['GAT_Type'] == '2Layer_GAT':
                            data_2layer.append(mean_val)
                            std_2layer.append(std_val)
                        elif row['GAT_Type'] == '3Layer_GAT':
                            data_3layer.append(mean_val)
                            std_3layer.append(std_val)
            
            # ç»˜åˆ¶åˆ†ç»„æŸ±çŠ¶å›¾
            x = np.arange(len(distances))
            width = 0.35
            
            bars1 = axes[i].bar(x - width/2, data_2layer, width, yerr=std_2layer, 
                            label='2-Layer GAT', color=colors['2Layer_GAT'], alpha=0.8, capsize=5)
            bars2 = axes[i].bar(x + width/2, data_3layer, width, yerr=std_3layer,
                            label='3-Layer GAT', color=colors['3Layer_GAT'], alpha=0.8, capsize=5)
            
            axes[i].set_title(f'{label}', fontweight='bold', fontsize=14)
            axes[i].set_xlabel('Distance Threshold', fontsize=12, fontweight='bold')
            axes[i].set_ylabel(label, fontsize=12, fontweight='bold')
            axes[i].set_xticks(x)
            axes[i].set_xticklabels(distances)
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾ - ä¿®æ”¹è¿™éƒ¨åˆ†
            max_height = max(max(data_2layer) if data_2layer else [0], max(data_3layer) if data_3layer else [0])
            for j, (bar1, bar2) in enumerate(zip(bars1, bars2)):
                if j < len(data_2layer):
                    axes[i].text(bar1.get_x() + bar1.get_width()/2, 
                            data_2layer[j] + std_2layer[j] + max_height * 0.02, 
                            f'{data_2layer[j]:.3f}', ha='center', va='bottom', 
                            fontsize=9, fontweight='bold')
                if j < len(data_3layer):
                    axes[i].text(bar2.get_x() + bar2.get_width()/2, 
                            data_3layer[j] + std_3layer[j] + max_height * 0.02,
                            f'{data_3layer[j]:.3f}', ha='center', va='bottom', 
                            fontsize=9, fontweight='bold')
                
                # æ ‡è®°æ›´å¥½çš„ç»“æœ
                if j < len(data_2layer) and j < len(data_3layer):
                    if data_2layer[j] > data_3layer[j]:
                        bar1.set_edgecolor('green')
                        bar1.set_linewidth(2)
                    elif data_3layer[j] > data_2layer[j]:
                        bar2.set_edgecolor('green')
                        bar2.set_linewidth(2)
        
        plt.suptitle('Comprehensive GAT Train Performance Comparison Across Distance Thresholds', 
                    fontsize=18, fontweight='bold', y=0.98)
        plt.tight_layout()
        
        save_path = comprehensive_dir / 'comprehensive_train_comparison.png'
        plt.savefig(save_path, dpi=1200, bbox_inches='tight')
        
        if self.show_plots:
            plt.show()
        else:
            plt.close()
        
        print(f"ğŸ“Š Comprehensive comparison plot saved: {save_path}")
    
    def analyze_best_configurations(self, comparison_data, comprehensive_dir):
        """åˆ†ææœ€ä½³é…ç½®"""
        metrics = ['accuracy', 'recall', 'specificity', 'f1', 'mcc', 'auc']
        metric_labels = ['Accuracy', 'Sensitivity/SN/Recall', 'Specificity/SP', 'F1 Score', 'MCC', 'AUC']
        
        best_configs = {}
        
        for metric, label in zip(metrics, metric_labels):
            best_score = 0
            best_config = None
            
            for row in comparison_data:
                # ä»æ ¼å¼åŒ–å­—ç¬¦ä¸²ä¸­æå–åˆ†æ•° 
                metric_str = row[metric] 
                parts = metric_str.split(' Â± ')
                score = float(parts[0])
                std = float(parts[1])
                
                if score > best_score:
                    best_score = score
                    best_config = {
                        'distance': row['Distance'],
                        'gat_type': row['GAT_Type'],
                        'layers': row['Layers'],
                        'score': round(score, 3),
                        'std': round(std, 3)
                    }
            
            best_configs[label] = best_config
        
        # æ‰“å°æœ€ä½³é…ç½®
        print(f"\nğŸ† Best Configurations by Metric:")
        print("="*80)
        for metric_label, config in best_configs.items():
            if config:
                print(f"{metric_label}: {config['gat_type']} on {config['distance']} ({config['score']:.4f} Â± {config['std']:.4f})")
        
        # ä¿å­˜æœ€ä½³é…ç½®æ‘˜è¦è¡¨æ ¼
        best_configs_df = pd.DataFrame([
            {
                'Metric': metric_label,
                'Best_Configuration': f"{config['gat_type']} on {config['distance']}",
                'Performance': f"{config['score']:.3f} Â± {config['std']:.3f}",
                'Score': config['score'],
                'Std': config['std']
            }
            for metric_label, config in best_configs.items() if config
        ])
        
        best_configs_df.to_csv(comprehensive_dir / 'best_configurations.csv', index=False)
        
        return best_configs

def train_gat_layer_comparison(
    train_paths=[
        '3_Graph_Data/TR/TR_ESMC_4.0A.pkl',
        '3_Graph_Data/TR/TR_ESMC_8.0A.pkl', 
        '3_Graph_Data/TR/TR_ESMC_12.0A.pkl'
    ],
    base_save_dir="4_Results_TR_RS",
    random_seed=42,
    model_params=None,
    training_params=None,
    n_folds=5,
    use_class_weights=True,
    show_plots=True
):
    """
    GATå±‚æ•°å¯¹æ¯”è®­ç»ƒä¸»å‡½æ•°
    
    å‚æ•°:
        train_paths (list): è®­ç»ƒæ•°æ®è·¯å¾„åˆ—è¡¨
        base_save_dir (str): åŸºç¡€ä¿å­˜ç›®å½•
        random_seed (int): éšæœºç§å­
        model_params (dict): æ¨¡å‹å‚æ•°
        training_params (dict): è®­ç»ƒå‚æ•°
        n_folds (int): äº¤å‰éªŒè¯æŠ˜æ•°
        use_class_weights (bool): æ˜¯å¦ä½¿ç”¨ç±»åˆ«æƒé‡
        show_plots (bool): æ˜¯å¦æ˜¾ç¤ºå›¾è¡¨
        
    è¿”å›:
        comprehensive_report (dict): ç»¼åˆæ¯”è¾ƒæŠ¥å‘Š
    """
    
    print("ğŸ”¬ GAT Layer Comparison Training System")
    print("="*80)
    print(f"ğŸ“ Training datasets:")
    for path in train_paths:
        print(f"  - {path}")
    print(f"ğŸ“ Results will be saved to: {base_save_dir}")
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = GATLayerComparisonTrainer(
        base_save_dir=base_save_dir,
        seed=random_seed,
        use_class_weights=use_class_weights,
        show_plots=show_plots
    )
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ•°æ®é›†è·å–è¾“å…¥ç»´åº¦
    first_data = load_dataset(train_paths[0])
    input_dim = first_data[0].x.shape[1]
    print(f"ğŸ”¬ Input feature dimension: {input_dim}")
    
    # è®¾ç½®é»˜è®¤å‚æ•°
    if model_params is None:
        model_params = {
            'node_feature_dim': input_dim,
            'hidden_dim': 128,
            'output_dim': 2,
            'drop': 0.3,  # é’ˆå¯¹GATä¼˜åŒ–çš„dropout
            'heads': 8,   # å¢åŠ æ³¨æ„åŠ›å¤´æ•°
            'k': 0.7,
            'add_self_loops': True
        }
    else:
        model_params['node_feature_dim'] = input_dim
    
    if training_params is None:
        training_params = {
            'lr': 0.0001,
            'batch_size': 64,
            'max_epochs': 100,
            'weight_decay': 0.01
        }
    
    print(f"ğŸ¤– Model parameters: {model_params}")
    print(f"ğŸ‹ï¸ Training parameters: {training_params}")
    
    # åˆ›å»ºç›®å½•ç»“æ„
    all_dirs = trainer.create_directory_structure(train_paths)
    
    # è®­ç»ƒæ‰€æœ‰è·ç¦»æ•°æ®é›†
    all_distance_results = {}
    
    for train_path in train_paths:
        try:
            print(f"\n{'='*100}")
            print(f"ğŸš€ Processing {train_path}")
            print(f"{'='*100}")
            
            distance_results = trainer.train_distance_dataset(
                train_path, model_params, training_params, all_dirs, n_folds
            )
            
            distance = trainer.extract_distance_from_path(train_path)
            all_distance_results[distance] = distance_results
            
            print(f"âœ… {train_path} processing completed!")
            
        except Exception as e:
            print(f"âŒ Error processing {train_path}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ç”Ÿæˆç»¼åˆæ¯”è¾ƒæŠ¥å‘Š
    if all_distance_results:
        comprehensive_report = trainer.generate_comprehensive_comparison(all_distance_results)
    else:
        comprehensive_report = None
        print("âŒ No successful training results to compare")
    
    # æ‰“å°æœ€ç»ˆæ‘˜è¦
    print(f"\n{'='*80}")
    print(f"ğŸ‰ GAT Layer Comparison Training Complete!")
    print(f"{'='*80}")
    print(f"ğŸ“ Results saved to: {base_save_dir}")
    print(f"ğŸ§¬ Processed datasets: {len(all_distance_results)}/{len(train_paths)}")
    
    for distance in all_distance_results.keys():
        print(f"  âœ… {distance} dataset")
    
    return comprehensive_report

# ==================== Jupyter Notebook å…¼å®¹å‡½æ•° ====================
def run_gat_comparison_notebook():
    """ä¸“é—¨ä¸º Jupyter Notebook è®¾è®¡çš„è¿è¡Œå‡½æ•°"""
    
    # æ•°æ®è·¯å¾„
    train_paths = [
        '3_Graph_Data/TR/TR_ESMC_4.0A.pkl',
        '3_Graph_Data/TR/TR_ESMC_8.0A.pkl', 
        '3_Graph_Data/TR/TR_ESMC_12.0A.pkl'
    ]
    
    # è¿è¡ŒGATå±‚æ•°å¯¹æ¯”è®­ç»ƒ
    comprehensive_report = train_gat_layer_comparison(
        train_paths=train_paths,
        base_save_dir="4_Results_TR_RS",
        random_seed=42,
        model_params={
            'hidden_dim': 128,
            'output_dim': 2,
            'drop': 0.3,
            'heads': 8,
            'k': 0.7,
            'add_self_loops': True
        },
        training_params={
            'lr': 0.0001,
            'batch_size': 64,
            'max_epochs': 100,
            'weight_decay': 0.01
        },
        n_folds=5,
        use_class_weights=True,
        show_plots=True
    )
    
    return comprehensive_report

if __name__ == "__main__":
    # ç›´æ¥è¿è¡ŒGATå±‚æ•°å¯¹æ¯”
    comprehensive_report = run_gat_comparison_notebook()