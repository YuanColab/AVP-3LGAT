import torch
import torch.nn.functional as F
import numpy as np
from torch_geometric.nn import GATConv, LayerNorm, TopKPooling
from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool
from torch.nn import Linear, Dropout, BatchNorm1d
import torch.nn as nn

# ==================== åŸºç¡€æ¨¡å‹ç±» ====================

class BaseGNNWithGradCAM(torch.nn.Module):
    """
    æ‰€æœ‰GNNæ¨¡å‹çš„åŸºç±»ï¼ŒåŒ…å«é€šç”¨åŠŸèƒ½å’ŒGrad-CAMæ”¯æŒ
    """
    
    def __init__(self, node_feature_dim, hidden_dim=128, output_dim=2, 
                 drop=0.3, k=0.8, add_self_loops=True):
        super(BaseGNNWithGradCAM, self).__init__()
        
        # åŸºç¡€å‚æ•°
        self.node_feature_dim = node_feature_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.drop = drop
        self.k = k
        self.add_self_loops = add_self_loops
        
        # Grad-CAMç›¸å…³å±æ€§
        self.gradients = None
        self.activations = None
        self.node_importance = None
        self.edge_importance = None
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = Linear(node_feature_dim, hidden_dim)
        self.input_bn = BatchNorm1d(hidden_dim)
        
        # TopKæ± åŒ–å±‚
        self.topk_pool = TopKPooling(hidden_dim, ratio=k)
        
        # åˆ†ç±»å™¨ - å¤šå°ºåº¦æ± åŒ–åçš„ç‰¹å¾ç»´åº¦: hidden_dim * 4
        classifier_input_dim = hidden_dim * 4
        self.classifier = nn.Sequential(
            Linear(classifier_input_dim, hidden_dim * 2),
            BatchNorm1d(hidden_dim * 2),
            nn.ReLU(inplace=True),
            Dropout(drop),
            Linear(hidden_dim * 2, hidden_dim),
            BatchNorm1d(hidden_dim),
            nn.ReLU(inplace=True),
            Dropout(drop * 0.5),
            Linear(hidden_dim, output_dim)
        )
        
        self._reset_parameters()
    
    def _reset_parameters(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, BatchNorm1d):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def activations_hook(self, grad):
        """Grad-CAMæ¢¯åº¦é’©å­"""
        self.gradients = grad
    
    def process_input(self, x):
        """å¤„ç†è¾“å…¥ç‰¹å¾"""
        x = self.input_projection(x)
        x = self.input_bn(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.drop, training=self.training)
        return x
    
    def apply_pooling(self, features, edge_index, batch):
        """åº”ç”¨å¤šå°ºåº¦æ± åŒ–"""
        # ä¿å­˜æ¿€æ´»å€¼ç”¨äºGrad-CAM
        self.activations = features
        if features.requires_grad:
            features.register_hook(self.activations_hook)
        
        # TopKæ± åŒ–
        selected_x, selected_edge_index, _, selected_batch, _, _ = self.topk_pool(
            features, edge_index, None, batch
        )
        
        # å¤šå°ºåº¦å…¨å±€æ± åŒ–
        pooled_max = global_max_pool(features, batch)
        pooled_mean = global_mean_pool(features, batch)
        pooled_add = global_add_pool(features, batch)
        pooled_topk = global_mean_pool(selected_x, selected_batch)
        
        # æ‹¼æ¥å››ç§æ± åŒ–æ–¹å¼çš„ç»“æœ
        graph_features = torch.cat([
            pooled_max, pooled_mean, pooled_add, pooled_topk
        ], dim=1)
        
        return graph_features
    
    def classify(self, graph_features):
        """åˆ†ç±»"""
        return self.classifier(graph_features)
    
    # ========== Grad-CAMç›¸å…³æ–¹æ³• ==========
    def get_activations_gradient(self):
        """è·å–æ¿€æ´»å€¼çš„æ¢¯åº¦"""
        return self.gradients
    
    def get_activations(self):
        """è·å–æ¿€æ´»å€¼"""
        return self.activations
    
    def compute_node_importance(self, target_class_idx=1):
        """è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§åˆ†æ•°"""
        if self.gradients is None or self.activations is None:
            return None
        
        alpha = torch.mean(self.gradients, dim=0)
        node_importance = []
        for i in range(self.activations.shape[0]):
            importance_score = F.relu(torch.sum(alpha * self.activations[i])).item()
            node_importance.append(importance_score)
        
        if max(node_importance) > 0:
            node_importance = [score/max(node_importance) for score in node_importance]
        
        self.node_importance = node_importance
        return node_importance
    
    def compute_edge_importance(self, edge_index, target_class_idx=1):
        """è®¡ç®—è¾¹é‡è¦æ€§åˆ†æ•°"""
        if self.gradients is None or self.activations is None:
            return None
        
        node_importance = self.compute_node_importance(target_class_idx)
        if node_importance is None:
            return None
        
        edge_importance = []
        for i in range(edge_index.size(1)):
            source_idx = edge_index[0, i].item()
            target_idx = edge_index[1, i].item()
            base_importance = node_importance[source_idx] * node_importance[target_idx]
            edge_importance.append(base_importance)
        
        self.edge_importance = edge_importance
        return edge_importance

# ==================== å¯é…ç½®å±‚æ•°çš„GATæ¨¡å‹ç±» ====================

class ConfigurableGATModel(BaseGNNWithGradCAM):
    """å¯é…ç½®å±‚æ•°çš„GATæ¨¡å‹ - æ”¯æŒ2å±‚å’Œ3å±‚å¯¹æ¯”"""
    
    def __init__(self, node_feature_dim, hidden_dim=128, output_dim=2, 
                 drop=0.3, heads=4, k=0.8, add_self_loops=True, num_layers=3):
        super(ConfigurableGATModel, self).__init__(node_feature_dim, hidden_dim, output_dim, drop, k, add_self_loops)
        
        self.heads = heads
        self.num_layers = num_layers
        
        # éªŒè¯å±‚æ•°
        if num_layers not in [2, 3]:
            raise ValueError(f"å½“å‰åªæ”¯æŒ2å±‚æˆ–3å±‚GATï¼Œå¾—åˆ°: {num_layers}")
        
        # åŠ¨æ€æ„å»ºGATå±‚
        self.gat_convs = nn.ModuleList()
        self.gat_norms = nn.ModuleList()
        
        for i in range(num_layers):
            if i == num_layers - 1:
                # æœ€åä¸€å±‚ï¼šè¾“å‡ºhidden_dimï¼Œå•å¤´ä¸æ‹¼æ¥
                gat_conv = GATConv(hidden_dim, hidden_dim, heads=1, 
                                  dropout=drop, add_self_loops=add_self_loops, concat=False)
            else:
                # å‰é¢çš„å±‚ï¼šå¤šå¤´æ‹¼æ¥
                gat_conv = GATConv(hidden_dim, hidden_dim // heads, heads=heads, 
                                  dropout=drop, add_self_loops=add_self_loops, concat=True)
            
            self.gat_convs.append(gat_conv)
            self.gat_norms.append(LayerNorm(hidden_dim))
    
    def forward(self, x, edge_index, batch, return_attention=False):
        x = self.process_input(x)
        
        all_attentions = []  # æ–°å¢ï¼šå­˜å‚¨å„å±‚æ³¨æ„åŠ›æƒé‡
        
        for i in range(self.num_layers):
            if return_attention:
                # ä¿®æ”¹ï¼šè°ƒç”¨GATConvæ—¶å¯ç”¨return_attention_weights
                x, attn = self.gat_convs[i](x, edge_index, return_attention_weights=True)
                all_attentions.append(attn)  # ä¿å­˜(edge_index, attention_values)
            else:
                x = self.gat_convs[i](x, edge_index)
            
            x = self.gat_norms[i](x, batch)
            x = F.relu(x)
            if i < self.num_layers - 1:
                x = F.dropout(x, p=self.drop, training=self.training)
        
        if return_attention:
            return all_attentions  # ä¿®æ”¹ï¼šç›´æ¥è¿”å›æ³¨æ„åŠ›æƒé‡åˆ—è¡¨
        else:
            features = x
            graph_features = self.apply_pooling(features, edge_index, batch)
            output = self.classify(graph_features)
            return output, features
    
    def get_model_info(self):
        return {
            'model_name': f'{self.num_layers}Layer_GAT',
            'node_feature_dim': self.node_feature_dim,
            'hidden_dim': self.hidden_dim,
            'heads': self.heads,
            'num_layers': self.num_layers,
            'total_parameters': sum(p.numel() for p in self.parameters())
        }

# ==================== ä¿æŒåŸæœ‰çš„å›ºå®šå±‚æ•°æ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰ ====================

class SingleGATModel(BaseGNNWithGradCAM):
    """åŸå§‹çš„3å±‚GATæ¨¡å‹ï¼ˆä¿æŒå‘åå…¼å®¹æ€§ï¼‰"""
    
    def __init__(self, node_feature_dim, hidden_dim=128, output_dim=2, 
                 drop=0.3, heads=4, k=0.8, add_self_loops=True):
        super(SingleGATModel, self).__init__(node_feature_dim, hidden_dim, output_dim, drop, k, add_self_loops)
        
        self.heads = heads
        
        # å›ºå®š3å±‚GAT
        self.gat_conv1 = GATConv(hidden_dim, hidden_dim // heads, heads=heads, 
                                dropout=drop, add_self_loops=add_self_loops, concat=True)
        self.gat_norm1 = LayerNorm(hidden_dim)
        
        self.gat_conv2 = GATConv(hidden_dim, hidden_dim // heads, heads=heads, 
                                dropout=drop, add_self_loops=add_self_loops, concat=True)
        self.gat_norm2 = LayerNorm(hidden_dim)
        
        self.gat_conv3 = GATConv(hidden_dim, hidden_dim, heads=1, 
                                dropout=drop, add_self_loops=add_self_loops, concat=False)
        self.gat_norm3 = LayerNorm(hidden_dim)
    
    def forward(self, x, edge_index, batch, return_attention=False):
        x = self.process_input(x)
        
        if return_attention:
            # ä¿®æ”¹ï¼šä¸‰å¤„GATConvè°ƒç”¨å‡å¯ç”¨return_attention_weights
            x, attn1 = self.gat_conv1(x, edge_index, return_attention_weights=True)
            x = self.gat_norm1(x, batch)
            x = F.relu(x)
            x = F.dropout(x, p=self.drop, training=self.training)
            
            x, attn2 = self.gat_conv2(x, edge_index, return_attention_weights=True)
            x = self.gat_norm2(x, batch)
            x = F.relu(x)
            x = F.dropout(x, p=self.drop, training=self.training)
            
            x, attn3 = self.gat_conv3(x, edge_index, return_attention_weights=True)
            x = self.gat_norm3(x, batch)
            features = F.relu(x)
            
            return [attn1, attn2, attn3]  # ä¿®æ”¹ï¼šè¿”å›ä¸‰å±‚æ³¨æ„åŠ›æƒé‡
        else:
            # å›ºå®š3å±‚GAT
            x = self.gat_conv1(x, edge_index)
            x = self.gat_norm1(x, batch)
            x = F.relu(x)
            x = F.dropout(x, p=self.drop, training=self.training)
            
            x = self.gat_conv2(x, edge_index)
            x = self.gat_norm2(x, batch)
            x = F.relu(x)
            x = F.dropout(x, p=self.drop, training=self.training)
            
            x = self.gat_conv3(x, edge_index)
            x = self.gat_norm3(x, batch)
            features = F.relu(x)
            
            # æ± åŒ–å’Œåˆ†ç±»
            graph_features = self.apply_pooling(features, edge_index, batch)
            output = self.classify(graph_features)
            
            return output, features
    
    def get_model_info(self):
        return {
            'model_name': 'Single_GAT_3Layer',
            'node_feature_dim': self.node_feature_dim,
            'hidden_dim': self.hidden_dim,
            'heads': self.heads,
            'total_parameters': sum(p.numel() for p in self.parameters())
        }

# ==================== æ¨¡å‹å·¥å‚å‡½æ•° ====================

def create_model(node_feature_dim, num_layers=3, **kwargs):
    """æ¨¡å‹å·¥å‚å‡½æ•° - åˆ›å»ºå¯é…ç½®å±‚æ•°çš„GATæ¨¡å‹"""
    
    # é»˜è®¤å‚æ•°
    default_params = {
        'hidden_dim': 128,
        'output_dim': 2,
        'drop': 0.3,
        'heads': 4,
        'k': 0.7,
        'add_self_loops': True,
        'num_layers': num_layers
    }
    
    # æ›´æ–°å‚æ•°
    default_params.update(kwargs)
    
    return ConfigurableGATModel(node_feature_dim, **default_params)

# ==================== ä¾¿åˆ©å‡½æ•° ====================

def create_2layer_gat(node_feature_dim, **kwargs):
    """åˆ›å»º2å±‚GATæ¨¡å‹"""
    return create_model(node_feature_dim, num_layers=2, **kwargs)

def create_3layer_gat(node_feature_dim, **kwargs):
    """åˆ›å»º3å±‚GATæ¨¡å‹"""
    return create_model(node_feature_dim, num_layers=3, **kwargs)

# ä¸ºäº†ä¿æŒå‘åå…¼å®¹æ€§ï¼Œä¿ç•™åŸå§‹å‡½æ•°å
def create_hybrid_model(node_feature_dim, **kwargs):
    """
    åˆ›å»ºGATæ¨¡å‹ï¼ˆä¿æŒå‘åå…¼å®¹æ€§ï¼Œé»˜è®¤3å±‚ï¼‰
    """
    return create_model(node_feature_dim, num_layers=3, **kwargs)

# ==================== å±‚æ•°å¯¹æ¯”æµ‹è¯•å‡½æ•° ====================

def compare_layer_configurations(node_feature_dim, **kwargs):
    """å¯¹æ¯”2å±‚å’Œ3å±‚GATçš„é…ç½®ä¿¡æ¯"""
    print("ğŸ”¬ GAT Layer Configuration Comparison")
    print("="*60)
    
    configs = [
        {'num_layers': 2, 'description': '2å±‚GAT - è½»é‡é«˜æ•ˆ'},
        {'num_layers': 3, 'description': '3å±‚GAT - æ·±åº¦è¡¨è¾¾'}
    ]
    
    results = {}
    
    for config in configs:
        num_layers = config['num_layers']
        description = config['description']
        
        print(f"\nğŸ“Š {description}")
        print("-" * 40)
        
        try:
            # åˆ›å»ºæ¨¡å‹
            model = create_model(node_feature_dim, num_layers=num_layers, **kwargs)
            
            # è·å–æ¨¡å‹ä¿¡æ¯
            info = model.get_model_info()
            
            print(f"   ğŸ—ï¸  Model: {info['model_name']}")
            print(f"   ğŸ“Š  Parameters: {info['total_parameters']:,}")
            print(f"   ğŸ§   Hidden dim: {info['hidden_dim']}")
            print(f"   ğŸ‘ï¸  Heads: {info['heads']}")
            print(f"   ğŸ”—  Layers: {info['num_layers']}")
            
            # è®¡ç®—æ¯æ ·æœ¬å‚æ•°æ•°
            params_per_sample = info['total_parameters'] / 6810  # ä½ çš„è®­ç»ƒé›†å¤§å°
            print(f"   ğŸ“ˆ  Params/Sample: {params_per_sample:.1f}")
            
            # ä¿å­˜ç»“æœ
            results[f'{num_layers}layer'] = info
            
        except Exception as e:
            print(f"   âŒ Error: {e}")
    
    print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print(f"   ğŸ¯ 2å±‚GAT: é€‚åˆå¿«é€Ÿå®éªŒï¼Œè®­ç»ƒé€Ÿåº¦å¿«ï¼Œä¸æ˜“è¿‡æ‹Ÿåˆ")
    print(f"   ğŸ¯ 3å±‚GAT: è¡¨è¾¾èƒ½åŠ›æ›´å¼ºï¼Œå¯èƒ½æ•è·æ›´å¤æ‚çš„æ¨¡å¼")
    print(f"")
    print(f"ğŸ”§ è°ƒç”¨æ–¹å¼:")
    print(f"   # 2å±‚GAT")
    print(f"   model_2layer = create_2layer_gat(node_feature_dim)")
    print(f"   # 3å±‚GAT") 
    print(f"   model_3layer = create_3layer_gat(node_feature_dim)")
    
    return results

# ==================== ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯• ====================

if __name__ == "__main__":
    print("ğŸš€ Configurable GAT Model Test")
    print("="*50)
    
    # æ¨¡æ‹Ÿå‚æ•°
    node_feature_dim = 1152  # ESMCç‰¹å¾ç»´åº¦
    
    # å¯¹æ¯”ä¸åŒå±‚æ•°é…ç½®
    print("ğŸ“Š Testing 2-layer vs 3-layer GAT:")
    results = compare_layer_configurations(
        node_feature_dim, 
        hidden_dim=128, 
        heads=8,
        drop=0.3
    )
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print(f"\nğŸ§ª Forward Pass Test:")
    print("-" * 30)
    
    for num_layers in [2, 3]:
        try:
            model = create_model(node_feature_dim, num_layers=num_layers, 
                               hidden_dim=128, heads=4)
            
            # æ¨¡æ‹Ÿæ•°æ®
            num_nodes = 50
            num_edges = 100
            x = torch.randn(num_nodes, node_feature_dim)
            edge_index = torch.randint(0, num_nodes, (2, num_edges))
            batch = torch.zeros(num_nodes, dtype=torch.long)
            batch[num_nodes//2:] = 1
            
            with torch.no_grad():
                output, features = model(x, edge_index, batch)
                print(f"   âœ… {num_layers}-layer: Output {output.shape}, Features {features.shape}")
                
        except Exception as e:
            print(f"   âŒ {num_layers}-layer error: {e}")
    
    print(f"\nâœ¨ Available Functions:")
    print(f"   create_2layer_gat() - åˆ›å»º2å±‚GAT")
    print(f"   create_3layer_gat() - åˆ›å»º3å±‚GAT")
    print(f"   create_model(num_layers=N) - åˆ›å»ºNå±‚GAT")
    print(f"   compare_layer_configurations() - å¯¹æ¯”ä¸åŒå±‚æ•°é…ç½®")