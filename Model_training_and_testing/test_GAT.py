import os
import sys
import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    roc_auc_score, matthews_corrcoef, confusion_matrix, roc_curve
)
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import json
warnings.filterwarnings('ignore')

# è®¾ç½®å­—ä½“å’Œåˆ†è¾¨ç‡
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 12
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['savefig.dpi'] = 1200
plt.rcParams['figure.dpi'] = 1200

# æ·»åŠ é¡¹ç›®è·¯å¾„
if os.getcwd() not in sys.path:
    sys.path.append(os.getcwd())

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from torch_geometric.loader import DataLoader
from Feature_extraction.utils import load_dataset
from Model_training_and_testing.GAT import create_model

class GATModelTestEvaluator:
    """GATæ¨¡å‹æµ‹è¯•è¯„ä¼°å™¨"""
    
    def __init__(self, model_name, distance, model_test_dir):
        self.model_name = model_name
        self.distance = distance
        self.model_test_dir = Path(model_test_dir)
        self.model_test_dir.mkdir(parents=True, exist_ok=True)
        
    def evaluate_and_visualize(self, y_true, y_pred, y_prob, test_data=None):
        """å®Œæ•´çš„è¯„ä¼°å’Œå¯è§†åŒ–"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ {self.model_name} on {self.distance} - Test Set Evaluation Results")
        print(f"{'='*80}")
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
        y_prob = np.array(y_prob)
        
        # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        metrics = self.calculate_all_metrics(y_true, y_pred, y_prob)
        
        # æ‰“å°ç»“æœ
        self.print_test_results(metrics, y_true, y_pred)
        
        # ä¿å­˜ç»“æœ
        self.save_results(y_true, y_pred, y_prob, metrics, test_data) 
               
        # ç”Ÿæˆå¯è§†åŒ–
        self.create_comprehensive_plots(y_true, y_pred, y_prob, metrics)
        
        return metrics
    
    def calculate_all_metrics(self, y_true, y_pred, y_prob):
        """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, average='binary'),
            'recall': recall_score(y_true, y_pred, average='binary'),
            'f1': f1_score(y_true, y_pred, average='binary'),
            'auc': roc_auc_score(y_true, y_prob),
            'mcc': matthews_corrcoef(y_true, y_pred)
        }
        
        # æ··æ·†çŸ©é˜µåˆ†è§£
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # é¢å¤–æŒ‡æ ‡
        metrics.update({
            'sensitivity': tp / (tp + fn) if (tp + fn) > 0 else 0,
            'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,
            'ppv': tp / (tp + fp) if (tp + fp) > 0 else 0,
            'npv': tn / (tn + fn) if (tn + fn) > 0 else 0,
            'true_positive': int(tp),
            'true_negative': int(tn),
            'false_positive': int(fp),
            'false_negative': int(fn)
        })
        
        return metrics
    
    def print_test_results(self, metrics, y_true, y_pred):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        total_samples = len(y_true)
        positive_samples = np.sum(y_true == 1)
        negative_samples = np.sum(y_true == 0)
        
        print(f"ğŸ“Š Dataset Statistics:")
        print(f"   Total samples: {total_samples}")
        print(f"   AVP samples: {positive_samples} ({positive_samples/total_samples*100:.1f}%)")
        print(f"   non_AVP samples: {negative_samples} ({negative_samples/total_samples*100:.1f}%)")
        
        pred_positive = np.sum(y_pred == 1)
        pred_negative = np.sum(y_pred == 0)
        
        print(f"\nğŸ”® Prediction Statistics:")
        print(f"   Predicted as AVP: {pred_positive} ({pred_positive/total_samples*100:.1f}%)")
        print(f"   Predicted as non_AVP: {pred_negative} ({pred_negative/total_samples*100:.1f}%)")
        
        print(f"\nğŸ“ˆ Performance Metrics:")
        print(f"   Accuracy:      {metrics['accuracy']:.3f}")
        print(f"   Specificity:   {metrics['specificity']:.3f}")
        print(f"   Sensitivity:   {metrics['sensitivity']:.3f}")
        print(f"   Recall:        {metrics['recall']:.3f}")
        print(f"   AUC:           {metrics['auc']:.3f}")
        print(f"   F1-Score:      {metrics['f1']:.3f}")
        print(f"   MCC:           {metrics['mcc']:.3f}")
        print(f"   Precision:     {metrics['precision']:.3f}")

    def create_comprehensive_plots(self, y_true, y_pred, y_prob, metrics):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾è¡¨"""
        # åˆ›å»º2x3å­å›¾å¸ƒå±€
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # ä¸»æ ‡é¢˜
        fig.suptitle(f'{self.model_name} on {self.distance} Dataset\nTest Set Evaluation', 
                     fontsize=18, fontweight='bold', y=0.96, ha='center')
        
        # å®šä¹‰å­å›¾æ ‡ç­¾
        subplot_labels = ['(A)', '(B)', '(C)', '(D)', '(E)', '(F)']
        
        # ç¬¬ä¸€è¡Œï¼šæ··æ·†çŸ©é˜µã€å½’ä¸€åŒ–æ··æ·†çŸ©é˜µã€é¥¼å›¾
        self.plot_confusion_matrix_with_accuracy(y_true, y_pred, metrics['accuracy'], axes[0, 0])
        axes[0, 0].text(-0.15, 1.05, subplot_labels[0], transform=axes[0, 0].transAxes, 
                        fontsize=16, fontweight='bold', va='top', ha='right')
        
        self.plot_normalized_confusion_matrix(y_true, y_pred, axes[0, 1])
        axes[0, 1].text(-0.15, 1.05, subplot_labels[1], transform=axes[0, 1].transAxes, 
                        fontsize=16, fontweight='bold', va='top', ha='right')
        
        self.plot_confusion_pie_chart(y_true, y_pred, axes[0, 2])
        axes[0, 2].text(-0.15, 1.05, subplot_labels[2], transform=axes[0, 2].transAxes, 
                        fontsize=16, fontweight='bold', va='top', ha='right')
        
        # ç¬¬äºŒè¡Œï¼šæ¦‚ç‡åˆ†å¸ƒã€æ ‡ç­¾åˆ†å¸ƒã€ROCæ›²çº¿
        self.plot_probability_distribution(y_true, y_prob, axes[1, 0])
        axes[1, 0].text(-0.15, 1.05, subplot_labels[3], transform=axes[1, 0].transAxes, 
                        fontsize=16, fontweight='bold', va='top', ha='right')
        
        self.plot_label_distribution(y_true, y_pred, axes[1, 1])
        axes[1, 1].text(-0.15, 1.05, subplot_labels[4], transform=axes[1, 1].transAxes, 
                        fontsize=16, fontweight='bold', va='top', ha='right')
        
        self.plot_roc_curve_in_subplot(y_true, y_prob, axes[1, 2])
        axes[1, 2].text(-0.15, 1.05, subplot_labels[5], transform=axes[1, 2].transAxes, 
                        fontsize=16, fontweight='bold', va='top', ha='right')
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout(rect=[0, 0, 1, 0.92])
        
        plt.savefig(self.model_test_dir / 'test_evaluation.png', 
                   dpi=1200, bbox_inches='tight')
        plt.show()
    
    def plot_confusion_matrix_with_accuracy(self, y_true, y_pred, accuracy, ax):
        """ç»˜åˆ¶å¸¦å‡†ç¡®åº¦çš„æ··æ·†çŸ©é˜µ"""
        cm = confusion_matrix(y_true, y_pred)
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                   xticklabels=['non_AVP', 'AVP'],
                   yticklabels=['non_AVP', 'AVP'],
                   cbar_kws={'label': 'Count'},
                   annot_kws={'size': 14, 'weight': 'bold'})
        
        ax.set_title(f'Confusion Matrix (Counts)\nAccuracy: {accuracy:.3f}', 
                    fontweight='bold', fontsize=14)
        ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')
        ax.set_ylabel('True Label', fontsize=12, fontweight='bold')
    
    def plot_normalized_confusion_matrix(self, y_true, y_pred, ax):
        """ç»˜åˆ¶å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ"""
        cm = confusion_matrix(y_true, y_pred)
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues', ax=ax,
                   xticklabels=['non_AVP', 'AVP'],
                   yticklabels=['non_AVP', 'AVP'],
                   cbar_kws={'label': 'Proportion'},
                   annot_kws={'size': 14, 'weight': 'bold'})
        
        ax.set_title('Confusion Matrix (Normalized)', fontweight='bold', fontsize=14)
        ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')
        ax.set_ylabel('True Label', fontsize=12, fontweight='bold')
    
    def plot_confusion_pie_chart(self, y_true, y_pred, ax):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µé¥¼å›¾"""
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        sizes = [tp, tn, fp, fn]
        labels = ['TP', 'TN', 'FP', 'FN']
        colors = ['#90EE90', '#87CEEB', '#FFB6C1', '#F0E68C']
        
        wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',
                                         startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})
        
        ax.set_title('Confusion Matrix Distribution', fontweight='bold', fontsize=14)
        
        # æ·»åŠ å›¾ä¾‹
        legend = ax.legend(wedges, [f'{label}: {size}' for label, size in zip(labels, sizes)],
                          title="Counts", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))
        legend.get_title().set_fontweight('bold')
        for text in legend.get_texts():
            text.set_fontweight('bold')
    
    def plot_probability_distribution(self, y_true, y_prob, ax):
        """ç»˜åˆ¶é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ"""
        y_true = np.array(y_true)
        y_prob = np.array(y_prob)
        
        avp_probs = y_prob[y_true == 1]
        non_avp_probs = y_prob[y_true == 0]
        
        if len(non_avp_probs) > 0:
            ax.hist(non_avp_probs, bins=30, alpha=0.6, label='True non_AVP', 
                   color='red', density=True, edgecolor='black', linewidth=0.5)
        
        if len(avp_probs) > 0:
            ax.hist(avp_probs, bins=30, alpha=0.6, label='True AVP', 
                   color='blue', density=True, edgecolor='black', linewidth=0.5)
        
        ax.axvline(x=0.5, color='black', linestyle='--', linewidth=2,
                  label='Classification Threshold (0.5)')
        
        ax.set_xlabel('Prediction Probability', fontsize=12, fontweight='bold')
        ax.set_ylabel('Density', fontsize=12, fontweight='bold')
        ax.set_title('Probability Distribution', fontweight='bold', fontsize=14)
        legend = ax.legend()
        for text in legend.get_texts():
            text.set_fontweight('bold')
        ax.grid(True, alpha=0.3)
    
    def plot_label_distribution(self, y_true, y_pred, ax):
        """ç»˜åˆ¶æ ‡ç­¾åˆ†å¸ƒå¯¹æ¯”"""
        true_avp = np.sum(y_true == 1)
        true_non_avp = np.sum(y_true == 0)
        pred_avp = np.sum(y_pred == 1)
        pred_non_avp = np.sum(y_pred == 0)
        
        labels = ['non_AVP', 'AVP']
        true_counts = [true_non_avp, true_avp]
        pred_counts = [pred_non_avp, pred_avp]
        
        x = np.arange(len(labels))
        width = 0.35
        
        bars1 = ax.bar(x - width/2, true_counts, width, label='True Labels', 
                      color='steelblue', alpha=0.8, edgecolor='black', linewidth=1)
        bars2 = ax.bar(x + width/2, pred_counts, width, label='Predicted Labels', 
                      color='orange', alpha=0.8, edgecolor='black', linewidth=1)
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars1, true_counts):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(true_counts + pred_counts)*0.01,
                   f'{int(value)}', ha='center', va='bottom', fontweight='bold', fontsize=10)
        
        for bar, value in zip(bars2, pred_counts):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(true_counts + pred_counts)*0.01,
                   f'{int(value)}', ha='center', va='bottom', fontweight='bold', fontsize=10)
        
        ax.set_xlabel('Labels', fontsize=12, fontweight='bold')
        ax.set_ylabel('Count', fontsize=12, fontweight='bold')
        ax.set_title('Label Distribution Comparison', fontweight='bold', fontsize=14)
        ax.set_xticks(x)
        ax.set_xticklabels(labels)
        legend = ax.legend()
        for text in legend.get_texts():
            text.set_fontweight('bold')
        ax.grid(True, alpha=0.3)
    
    def plot_roc_curve_in_subplot(self, y_true, y_prob, ax):
        """åœ¨å­å›¾ä¸­ç»˜åˆ¶ROCæ›²çº¿"""
        fpr, tpr, thresholds = roc_curve(y_true, y_prob)
        auc_score = roc_auc_score(y_true, y_prob)
        
        ax.plot(fpr, tpr, color='darkorange', linewidth=3, 
                label=f'ROC Curve (AUC = {auc_score:.3f})')
        ax.plot([0, 1], [0, 1], color='navy', linewidth=2, linestyle='--', 
                label='Random Classifier')
        
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate (FPR)', fontsize=12, fontweight='bold')
        ax.set_ylabel('True Positive Rate (TPR)', fontsize=12, fontweight='bold')
        ax.set_title('ROC Curve', fontweight='bold', fontsize=14)
        legend = ax.legend(loc="lower right", fontsize=10)
        for text in legend.get_texts():
            text.set_fontweight('bold')
        ax.grid(True, alpha=0.3)
    
    def save_results(self, y_true, y_pred, y_prob, metrics, test_data=None):
        """ä¿å­˜è¯¦ç»†ç»“æœ - ä¿å­˜ä¸º3ä½æœ‰æ•ˆæ•°å­—"""
        # æ„å»ºç»“æœDataFrame
        result_dict = {
            'sample_id': range(len(y_true)),
            'true_label': y_true,
            'predicted_label': y_pred,
            'prediction_probability': np.round(y_prob, 3),  # 3ä½å°æ•°
            'correct_prediction': (y_true == y_pred)
        }
        
        results_df = pd.DataFrame(result_dict)
        results_df.to_csv(self.model_test_dir / 'test_predictions.csv', index=False)
        
        # ä¿å­˜æŒ‡æ ‡ - è½¬æ¢ä¸º3ä½æœ‰æ•ˆæ•°å­—
        metrics_3sf = {}
        for key, value in metrics.items():
            if isinstance(value, (int, np.integer)):
                metrics_3sf[key] = int(value)
            elif isinstance(value, (float, np.floating)):
                metrics_3sf[key] = round(float(value), 3)
            else:
                metrics_3sf[key] = value
        
        metrics_df = pd.DataFrame([metrics_3sf])
        metrics_df.to_csv(self.model_test_dir / 'test_metrics.csv', index=False)
        
        # ä¿å­˜æµ‹è¯•æ‘˜è¦ - è½¬æ¢ä¸º3ä½æœ‰æ•ˆæ•°å­—
        test_summary = {
            'model_name': self.model_name,
            'distance': self.distance,
            'test_performance': metrics_3sf,
            'test_info': {
                'total_samples': len(y_true),
                'avp_samples': int(np.sum(y_true == 1)),
                'non_avp_samples': int(np.sum(y_true == 0))
            }
        }
        
        with open(self.model_test_dir / 'test_summary.json', 'w') as f:
            json.dump(test_summary, f, indent=2)
        
        print(f"\nğŸ’¾ {self.model_name} test results saved to: {self.model_test_dir}")

class GATLayerComparisonTester:
    """GATå±‚æ•°å¯¹æ¯”æµ‹è¯•å™¨ - ä¸“é—¨æµ‹è¯•è®­ç»ƒå¥½çš„GATæ¨¡å‹"""
    
    def __init__(self, train_results_dir="4_Results_TR_RS", test_results_dir="4_Results_TS_RS", device='cuda'):
        self.train_results_dir = Path(train_results_dir)
        self.test_results_dir = Path(test_results_dir)
        self.test_results_dir.mkdir(parents=True, exist_ok=True)
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        # GATé…ç½®æ˜ å°„
        self.gat_configs = {
            '2Layer_GAT': {'num_layers': 2, 'name': '2-Layer GAT'},
            '3Layer_GAT': {'num_layers': 3, 'name': '3-Layer GAT'}
        }
        
        # è·ç¦»é˜ˆå€¼æ˜ å°„
        self.distance_mapping = {
            '4.0A': '4.0A',
            '8.0A': '8.0A', 
            '12.0A': '12.0A'
        }
    
    def extract_distance_from_path(self, test_path):
        """ä»æµ‹è¯•æ•°æ®è·¯å¾„æå–è·ç¦»é˜ˆå€¼"""
        path_str = str(test_path)
        for distance_key in self.distance_mapping.keys():
            if distance_key in path_str:
                return distance_key
        return 'unknown'
    
    def find_available_models(self):
        """æŸ¥æ‰¾å¯ç”¨çš„è®­ç»ƒæ¨¡å‹"""
        available_models = {}
        
        if not self.train_results_dir.exists():
            print(f"âŒ Training results directory not found: {self.train_results_dir}")
            return available_models
        
        # æŸ¥æ‰¾æ¯ä¸ªè·ç¦»é˜ˆå€¼ä¸‹çš„GATæ¨¡å‹
        for distance in self.distance_mapping.keys():
            distance_dir = self.train_results_dir / f"GAT_Comparison_{distance}"
            if distance_dir.exists():
                distance_models = {}
                
                for gat_type in self.gat_configs.keys():
                    gat_dir = distance_dir / gat_type
                    models_dir = gat_dir / 'models'
                    cv_summary_file = gat_dir / 'train_results' / 'cv_summary.json'
                    
                    if models_dir.exists() and cv_summary_file.exists():
                        model_files = list(models_dir.glob("best_model_fold_*.pth"))
                        if model_files:
                            distance_models[gat_type] = {
                                'models_dir': models_dir,
                                'cv_summary_file': cv_summary_file,
                                'model_files': model_files
                            }
                            print(f"âœ… Found {distance} {gat_type}: {len(model_files)} fold models")
                
                if distance_models:
                    available_models[distance] = distance_models
        
        return available_models
    
    def safe_torch_load(self, path, map_location=None):
        """å®‰å…¨çš„torch.loadåŒ…è£…å™¨"""
        try:
            return torch.load(path, map_location=map_location, weights_only=False)
        except TypeError:
            return torch.load(path, map_location=map_location)
    
    def test_single_gat_model(self, distance, gat_type, model_info, test_data):
        """æµ‹è¯•å•ä¸ªGATæ¨¡å‹"""
        gat_name = self.gat_configs[gat_type]['name']
        print(f"\n{'='*80}")
        print(f"ğŸ§ª Testing {gat_name} on {distance} Dataset")
        print(f"{'='*80}")
        
        # åˆ›å»ºæµ‹è¯•ç»“æœç›®å½•
        test_distance_dir = self.test_results_dir / f"GAT_Test_{distance}"
        test_model_dir = test_distance_dir / gat_type
        test_model_dir.mkdir(parents=True, exist_ok=True)
        
        # è·å–æ¨¡å‹æ–‡ä»¶
        model_files = model_info['model_files']
        model_files.sort()
        
        print(f"ğŸ“¦ Found {len(model_files)} fold models")
        
        test_loader = DataLoader(test_data, batch_size=64, shuffle=False)
        all_predictions = []
        
        # å¯¹æ¯ä¸ªfoldæ¨¡å‹è¿›è¡Œé¢„æµ‹
        for i, model_path in enumerate(model_files):
            try:
                print(f"Loading model from fold {i+1}: {model_path.name}")
                
                checkpoint = self.safe_torch_load(model_path, map_location=self.device)
                
                # ä»checkpointä¸­è¯»å–æ¨¡å‹å‚æ•°
                model_params = checkpoint['model_params']
                num_layers = checkpoint['num_layers']
                
                # åˆ›å»ºæ¨¡å‹ - ä½¿ç”¨ä½ çš„GATæ¨¡å‹
                model = create_model(
                    model_params['node_feature_dim'], 
                    num_layers=num_layers,
                    **{k: v for k, v in model_params.items() if k != 'node_feature_dim'}
                ).to(self.device)
                
                model.load_state_dict(checkpoint['model_state_dict'])
                model.eval()
                
                fold_probs = []
                with torch.no_grad():
                    for batch in test_loader:
                        batch = batch.to(self.device)
                        # GATæ¨¡å‹çš„forwardæ¥å£
                        outputs, _ = model(batch.x, batch.edge_index, batch.batch)
                        probs = torch.softmax(outputs, dim=1)[:, 1]
                        fold_probs.extend(probs.cpu().numpy())
                
                all_predictions.append(fold_probs)
                print(f"âœ… Successfully tested fold {i+1}")
                
            except Exception as e:
                print(f"âŒ Error loading model from {model_path}: {e}")
                continue
        
        if not all_predictions:
            print(f"âŒ No valid models could be loaded for {gat_name} on {distance}!")
            return None
        
        # é›†æˆé¢„æµ‹ï¼ˆå¹³å‡æ¦‚ç‡ï¼‰
        ensemble_probs = np.mean(all_predictions, axis=0)
        ensemble_preds = (ensemble_probs > 0.5).astype(int)
        
        # çœŸå®æ ‡ç­¾
        true_labels = [graph.y.item() for graph in test_data]
        
        # è®¡ç®—æŒ‡æ ‡é¢„è§ˆ
        test_metrics_preview = {
            'accuracy': accuracy_score(true_labels, ensemble_preds),
            'recall': recall_score(true_labels, ensemble_preds, average='binary'),
            'f1': f1_score(true_labels, ensemble_preds, average='binary'),
            'auc': roc_auc_score(true_labels, ensemble_probs),
            'mcc': matthews_corrcoef(true_labels, ensemble_preds)
        }
        
        print(f"\nğŸ¯ {gat_name} on {distance} Test Results Preview:")
        print(f"   Accuracy: {test_metrics_preview['accuracy']:.3f}")
        print(f"   AUC:      {test_metrics_preview['auc']:.3f}")
        print(f"   F1-Score: {test_metrics_preview['f1']:.3f}")
        print(f"   MCC:      {test_metrics_preview['mcc']:.3f}")
        
        # åˆ›å»ºGATæ¨¡å‹è¯„ä¼°å™¨å¹¶è¿›è¡Œè¯¦ç»†è¯„ä¼°
        evaluator = GATModelTestEvaluator(gat_name, distance, test_model_dir)
        final_metrics = evaluator.evaluate_and_visualize(
            true_labels, ensemble_preds, ensemble_probs, test_data
        )
        
        print(f"âœ… {gat_name} on {distance} testing completed!")
        
        return final_metrics
    
    def test_distance_dataset(self, test_path, available_models):
        """æµ‹è¯•å•ä¸ªè·ç¦»æ•°æ®é›†çš„æ‰€æœ‰GATæ¨¡å‹"""
        distance = self.extract_distance_from_path(test_path)
        
        print(f"\n{'='*100}")
        print(f"ğŸ§¬ Testing GAT Models on {distance} Test Dataset")
        print(f"ğŸ“ Test data path: {test_path}")
        print(f"{'='*100}")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data = load_dataset(test_path)
        print(f"âœ… Loaded {len(test_data)} test graphs")
        
        # åˆ†ææµ‹è¯•é›†åˆ†å¸ƒ
        test_labels = [graph.y.item() for graph in test_data]
        test_pos = sum(test_labels)
        test_neg = len(test_labels) - test_pos
        
        print(f"\nğŸ“Š Test Set Distribution:")
        print(f"   Total samples: {len(test_labels)}")
        print(f"   AVP samples: {test_pos} ({test_pos/len(test_labels)*100:.1f}%)")
        print(f"   non_AVP samples: {test_neg} ({test_neg/len(test_labels)*100:.1f}%)")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„è®­ç»ƒæ¨¡å‹
        if distance not in available_models:
            print(f"âŒ No trained models found for {distance} dataset!")
            return {}
        
        distance_models = available_models[distance]
        distance_results = {}
        
        # æµ‹è¯•æ¯ç§GATé…ç½®
        for gat_type, model_info in distance_models.items():
            try:
                test_results = self.test_single_gat_model(
                    distance, gat_type, model_info, test_data
                )
                if test_results:
                    distance_results[gat_type] = test_results
            except Exception as e:
                print(f"âŒ Error testing {gat_type} on {distance}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # ç”Ÿæˆè·ç¦»ç‰¹å®šçš„æ¯”è¾ƒæŠ¥å‘Š
        if len(distance_results) > 1:
            self.create_distance_comparison_report(distance, distance_results)
        
        return distance_results
    
    def plot_distance_comparison(self, comparison_data, comparison_dir, distance):
        """ç»˜åˆ¶è·ç¦»ç‰¹å®šçš„GATæ¨¡å‹æ¯”è¾ƒå›¾"""
        metrics = ['accuracy', 'sensitivity', 'specificity', 'f1', 'mcc', 'auc']
        metric_labels = ['Accuracy', 'Sensitivity', 'Specificity', 'F1 Score', 'MCC', 'AUC']
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.ravel()
        
        gat_models = [row['GAT_Model'] for row in comparison_data]
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(gat_models)]
        
        for i, (metric, label) in enumerate(zip(metrics, metric_labels)):
            values = [row[metric] for row in comparison_data]
            
            bars = axes[i].bar(gat_models, values, color=colors, alpha=0.8)
            axes[i].set_title(f'{label}', fontweight='bold', fontsize=14)
            axes[i].set_ylabel(label, fontsize=12)
            axes[i].grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾ - 3ä½æœ‰æ•ˆæ•°å­—
            for bar, value in zip(bars, values):
                axes[i].text(bar.get_x() + bar.get_width()/2, 
                            bar.get_height() + max(values) * 0.01,
                            f'{value:.3f}', ha='center', va='bottom', 
                            fontsize=10, fontweight='bold')
            
            # æ ‡è®°æœ€ä½³ç»“æœ
            best_idx = values.index(max(values))
            bars[best_idx].set_edgecolor('green')
            bars[best_idx].set_linewidth(2)
        
        plt.suptitle(f'GAT Models Test Performance Comparison on {distance} Dataset', 
                    fontsize=16, fontweight='bold', y=0.98)
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        save_path = comparison_dir / f'{distance}_gat_test_comparison.png'
        plt.savefig(save_path, dpi=1200, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š {distance} comparison plot saved: {save_path}")
        
    def create_distance_comparison_report(self, distance, distance_results):
        """åˆ›å»ºè·ç¦»ç‰¹å®šçš„GATæ¨¡å‹æ¯”è¾ƒæŠ¥å‘Š"""
        print(f"\nğŸ“Š Creating {distance} GAT Models Comparison Report")
        print("="*60)
        
        # åˆ›å»ºæ¯”è¾ƒç›®å½•
        comparison_dir = self.test_results_dir / f"GAT_Test_{distance}"
        
        # å‡†å¤‡æ¯”è¾ƒæ•°æ® - è½¬æ¢ä¸º3ä½æœ‰æ•ˆæ•°å­—
        comparison_data = []
        metrics_keys = ['accuracy', 'sensitivity', 'specificity', 'f1', 'mcc', 'auc', 'precision']
        
        for gat_type, results in distance_results.items():
            gat_name = self.gat_configs[gat_type]['name']
            row = {'GAT_Model': gat_name, 'Layers': self.gat_configs[gat_type]['num_layers']}
            for metric in metrics_keys:
                if metric in results:
                    row[metric] = round(results[metric], 3)
                else:
                    row[metric] = 0.000
            comparison_data.append(row)
        
        # ä¿å­˜æ¯”è¾ƒè¡¨æ ¼
        comparison_df = pd.DataFrame(comparison_data)
        comparison_df.to_csv(comparison_dir / f'{distance}_gat_comparison.csv', index=False)
        
        # ç”Ÿæˆè·ç¦»æ¯”è¾ƒå›¾è¡¨
        if len(comparison_data) > 1:
            self.plot_distance_comparison(comparison_data, comparison_dir, distance)   
                 
        # æ‰“å°æ¯”è¾ƒç»“æœ
        print(f"\nğŸ“Š {distance} GAT Models Test Comparison:")
        print("="*80)
        print(f"{'Model':<15} {'Layers':<8} {'Accuracy':<10} {'Sensitivity':<12} {'Specificity':<12} {'F1 Score':<10} {'MCC':<8} {'AUC':<8}")
        print("="*80)
        
        for row in comparison_data:
            print(f"{row['GAT_Model']:<15} {row['Layers']:<8} {row['accuracy']:<10.3f} {row['sensitivity']:<12.3f} {row['specificity']:<12.3f} {row['f1']:<10.3f} {row['mcc']:<8.3f} {row['auc']:<8.3f}")
        
        print("="*80)
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_models = {}
        for metric in metrics_keys:
            best_score = 0
            best_model = None
            for row in comparison_data:
                if row[metric] > best_score:
                    best_score = row[metric]
                    best_model = row['GAT_Model']
            if best_model:
                best_models[metric] = (best_model, best_score)
        
        print(f"\nğŸ† Best {distance} GAT Models by Metric:")
        for metric, (model_name, score) in best_models.items():
            print(f"  {metric}: {model_name} ({score:.3f})")
        
        # ä¿å­˜æ¯”è¾ƒæŠ¥å‘Š - è½¬æ¢ä¸º3ä½æœ‰æ•ˆæ•°å­—
        comparison_report = {
            'distance': distance,
            'test_comparison_table': comparison_data,
            'best_models': {k: (v[0], round(v[1], 3)) for k, v in best_models.items()},
            'total_models_tested': len(distance_results),
            'timestamp': pd.Timestamp.now().isoformat()
        }
        
        with open(comparison_dir / f'{distance}_comparison_report.json', 'w') as f:
            json.dump(comparison_report, f, indent=2)
        
        print(f"\nâœ… {distance} comparison report saved to: {comparison_dir}")
    
    def test_all_datasets(self, test_paths):
        """æµ‹è¯•æ‰€æœ‰æ•°æ®é›†çš„GATæ¨¡å‹"""
        print(f"\nğŸ­ GAT Layer Comparison Testing System")
        print(f"{'='*80}")
        
        # æŸ¥æ‰¾å¯ç”¨æ¨¡å‹
        available_models = self.find_available_models()
        
        if not available_models:
            print("âŒ No trained GAT models found!")
            return {}
        
        print(f"\nğŸ¯ Available trained models:")
        for distance, models in available_models.items():
            print(f"  {distance}: {list(models.keys())}")
        
        # æµ‹è¯•æ‰€æœ‰æ•°æ®é›†
        all_test_results = {}
        
        for test_path in test_paths:
            try:
                distance_results = self.test_distance_dataset(test_path, available_models)
                if distance_results:
                    distance = self.extract_distance_from_path(test_path)
                    all_test_results[distance] = distance_results
            except Exception as e:
                print(f"âŒ Error testing {test_path}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # ç”Ÿæˆç»¼åˆæ¯”è¾ƒæŠ¥å‘Š
        if len(all_test_results) > 0:
            self.create_comprehensive_test_report(all_test_results)
        
        return all_test_results
    
    def create_comprehensive_test_report(self, all_test_results):
        """åˆ›å»ºç»¼åˆæµ‹è¯•æ¯”è¾ƒæŠ¥å‘Š"""
        print(f"\nğŸ“Š Creating Comprehensive GAT Test Comparison Report")
        print("="*80)
        
        # åˆ›å»ºç»¼åˆæ¯”è¾ƒç›®å½•
        comprehensive_dir = self.test_results_dir / "Comprehensive_Test_Comparison"
        comprehensive_dir.mkdir(parents=True, exist_ok=True)
        
        # å‡†å¤‡ç»¼åˆæ•°æ® - ä¿®æ”¹ä¸ºä¸è®­ç»ƒé›†ä¸€è‡´çš„æ ¼å¼ï¼Œè½¬æ¢ä¸º3ä½æœ‰æ•ˆæ•°å­—
        comprehensive_data = []
        metrics = ['accuracy', 'sensitivity', 'specificity', 'f1', 'mcc', 'auc']
        
        for distance, distance_results in all_test_results.items():
            for gat_type, results in distance_results.items():
                gat_name = self.gat_configs[gat_type]['name']
                row = {
                    'Distance': distance,
                    'GAT_Model': gat_name,
                    'Layers': self.gat_configs[gat_type]['num_layers']
                }
                
                for metric in metrics:
                    value = round(results.get(metric, 0), 3)
                    row[metric] = f"{value:.3f}"  
                comprehensive_data.append(row)
        
        # ä¿å­˜ç»¼åˆæ¯”è¾ƒè¡¨æ ¼
        comprehensive_df = pd.DataFrame(comprehensive_data)
        comprehensive_df.to_csv(comprehensive_dir / 'comprehensive_test_comparison.csv', index=False)
        
        # ç”Ÿæˆç»¼åˆæ¯”è¾ƒå›¾
        self.plot_comprehensive_test_comparison(comprehensive_data, comprehensive_dir)
        
        # åˆ†ææœ€ä½³é…ç½®
        best_overall_configs = self.analyze_best_test_configurations(comprehensive_data)
        
        # ä¿®æ”¹ï¼šä¿å­˜å®Œæ•´æµ‹è¯•æŠ¥å‘Šï¼Œæ ¼å¼ä¸è®­ç»ƒé›†ä¸€è‡´ï¼Œè½¬æ¢ä¸º3ä½æœ‰æ•ˆæ•°å­—
        test_report = {
            'comparison_data': comprehensive_data,
            'best_configurations': best_overall_configs,
            'summary': {
                'total_test_experiments': len(comprehensive_data),
                'tested_distances': list(all_test_results.keys()),
                'tested_gat_models': list(set([row['GAT_Model'] for row in comprehensive_data]))
            },
            # æ·»åŠ æ ¼å¼åŒ–çš„æœ€ä½³é…ç½®æ‘˜è¦
            'best_configurations_formatted': {},
            # æ·»åŠ æ ¼å¼åŒ–çš„ç»“æœ
            'formatted_results': {}
        }
        
        # ä¸ºæœ€ä½³é…ç½®æ·»åŠ æ ¼å¼åŒ–ç‰ˆæœ¬
        for metric_name, config in best_overall_configs.items():
            if config:
                test_report['best_configurations_formatted'][metric_name] = {
                    'config': f"{config['gat_model']} on {config['distance']}",
                    'performance': f"{config['score']:.3f}"
                }
        
        # ä¸ºæ¯ä¸ªè·ç¦»å’ŒGATé…ç½®æ·»åŠ æ ¼å¼åŒ–ç‰ˆæœ¬
        for distance, distance_results in all_test_results.items():
            test_report['formatted_results'][distance] = {}
            for gat_type, results in distance_results.items():
                gat_name = self.gat_configs[gat_type]['name']
                
                formatted_metrics = {}
                for metric in metrics:
                    value = round(results.get(metric, 0), 3)
                    formatted_metrics[metric] = f"{value:.3f}"
                
                test_report['formatted_results'][distance][gat_name] = formatted_metrics
        
        # ä¿å­˜JSONæŠ¥å‘Šï¼Œä½¿ç”¨UTF-8ç¼–ç 
        with open(comprehensive_dir / 'comprehensive_test_report.json', 'w', encoding='utf-8') as f:
            json.dump(test_report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Comprehensive test comparison complete! Results saved to: {comprehensive_dir}")
        
        return test_report
    
    def plot_comprehensive_test_comparison(self, comprehensive_data, comprehensive_dir):
        """ç»˜åˆ¶ç»¼åˆæµ‹è¯•æ¯”è¾ƒå›¾è¡¨"""
        metrics = ['accuracy', 'sensitivity', 'specificity', 'f1', 'mcc', 'auc']
        metric_labels = ['Accuracy', 'Sensitivity/SN/Recall', 'Specificity/SP', 'F1 Score', 'MCC', 'AUC']
        
        # åˆ›å»ºå¤§å‹ç»¼åˆæ¯”è¾ƒå›¾
        fig, axes = plt.subplots(2, 3, figsize=(24, 16))
        axes = axes.ravel()
        
        distances = ['4.0A', '8.0A', '12.0A']
        gat_models = ['2-Layer GAT', '3-Layer GAT']
        colors = {'2-Layer GAT': '#1f77b4', '3-Layer GAT': '#ff7f0e'}
        
        for i, (metric, label) in enumerate(zip(metrics, metric_labels)):
            # å‡†å¤‡æ•°æ® - ä»æ ¼å¼åŒ–å­—ç¬¦ä¸²ä¸­æå–æ•°å€¼
            data_2layer = []
            data_3layer = []
            
            for distance in distances:
                val_2layer = 0
                val_3layer = 0
                
                for row in comprehensive_data:
                    if row['Distance'] == distance:
                        # ä»æ ¼å¼åŒ–å­—ç¬¦ä¸²ä¸­æå–æ•°å€¼
                        metric_str = row[metric]  
                        parts = metric_str.split(' Â± ')
                        mean_val = float(parts[0])
                        
                        if row['GAT_Model'] == '2-Layer GAT':
                            val_2layer = mean_val
                        elif row['GAT_Model'] == '3-Layer GAT':
                            val_3layer = mean_val
                
                data_2layer.append(val_2layer)
                data_3layer.append(val_3layer)
            
            # ç»˜åˆ¶åˆ†ç»„æŸ±çŠ¶å›¾ï¼ˆæµ‹è¯•é›†æ²¡æœ‰è¯¯å·®æ£’ï¼‰
            x = np.arange(len(distances))
            width = 0.35
            
            bars1 = axes[i].bar(x - width/2, data_2layer, width, 
                            label='2-Layer GAT', color=colors['2-Layer GAT'], alpha=0.8)
            bars2 = axes[i].bar(x + width/2, data_3layer, width,
                            label='3-Layer GAT', color=colors['3-Layer GAT'], alpha=0.8)
            
            axes[i].set_title(f'{label}', fontweight='bold', fontsize=14)
            axes[i].set_xlabel('Distance Threshold', fontsize=12, fontweight='bold')
            axes[i].set_ylabel(label, fontsize=12, fontweight='bold')
            axes[i].set_xticks(x)
            axes[i].set_xticklabels(distances)
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾ - 3ä½æœ‰æ•ˆæ•°å­—
            for j, (bar1, bar2, v1, v2) in enumerate(zip(bars1, bars2, data_2layer, data_3layer)):
                if v1 > 0:
                    axes[i].text(bar1.get_x() + bar1.get_width()/2, 
                            bar1.get_height() + max(data_2layer + data_3layer) * 0.01, 
                            f'{v1:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
                if v2 > 0:
                    axes[i].text(bar2.get_x() + bar2.get_width()/2, 
                            bar2.get_height() + max(data_2layer + data_3layer) * 0.01,
                            f'{v2:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
                
                # æ ‡è®°æ›´å¥½çš„ç»“æœ
                if v1 > v2 and v1 > 0:
                    bar1.set_edgecolor('green')
                    bar1.set_linewidth(2)
                elif v2 > v1 and v2 > 0:
                    bar2.set_edgecolor('green')
                    bar2.set_linewidth(2)
        
        # æ·»åŠ ä¸»æ ‡é¢˜
        plt.suptitle('Comprehensive GAT Test Performance Comparison Across Distance Thresholds', 
                    fontsize=18, fontweight='bold', y=0.98)
        plt.tight_layout()
        
        # ä¿®æ”¹ï¼šä¿å­˜å›¾ç‰‡
        save_path = comprehensive_dir / 'comprehensive_test_comparison.png'
        plt.savefig(save_path, dpi=1200, bbox_inches='tight')
        
        # ä¿®æ”¹ï¼šæ§åˆ¶æ˜¯å¦æ˜¾ç¤ºå›¾ç‰‡
        plt.show()
        
        print(f"ğŸ“Š Comprehensive test comparison plot saved: {save_path}")
    
    def analyze_best_test_configurations(self, comprehensive_data):
        """åˆ†ææœ€ä½³æµ‹è¯•é…ç½®"""
        metrics = ['accuracy', 'sensitivity', 'specificity', 'f1', 'mcc', 'auc']
        metric_labels = ['Accuracy', 'Sensitivity', 'Specificity', 'F1 Score', 'MCC', 'AUC']
        
        best_configs = {}
        
        for metric, label in zip(metrics, metric_labels):
            best_score = 0
            best_config = None
            
            for row in comprehensive_data:           
                metric_str = row[metric]  
                parts = metric_str.split(' Â± ')
                score = float(parts[0])
                
                if score > best_score:
                    best_score = score
                    best_config = {
                        'distance': row['Distance'],
                        'gat_model': row['GAT_Model'],
                        'layers': row['Layers'],
                        'score': round(score, 3)
                    }
            
            best_configs[label] = best_config
        
        # æ‰“å°æœ€ä½³é…ç½®
        print(f"\nğŸ† Best Test Configurations by Metric:")
        print("="*80)
        for metric_label, config in best_configs.items():
            if config:
                print(f"{metric_label}: {config['gat_model']} on {config['distance']} ({config['score']:.3f})")
        
        return best_configs

def main_gat_testing(
    test_paths=[
        '3_Graph_Data/TS/TS_ESMC_4.0A.pkl',
        '3_Graph_Data/TS/TS_ESMC_8.0A.pkl',
        '3_Graph_Data/TS/TS_ESMC_12.0A.pkl'
    ],
    train_results_dir="4_Results_TR_RS"
):
    """
    ä¸»GATæµ‹è¯•å‡½æ•° - æµ‹è¯•æ‰€æœ‰è®­ç»ƒè¿‡çš„GATæ¨¡å‹
    
    å‚æ•°:
        test_paths (list): æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        train_results_dir (str): è®­ç»ƒç»“æœç›®å½•
    """
    print("ğŸ§ª GAT Layer Comparison Testing System")
    print("="*80)
    
    # é…ç½®å‚æ•°
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ Using device: {device}")
    
    print(f"ğŸ“Š Test datasets:")
    for path in test_paths:
        print(f"  - {path}")
    
    print(f"ğŸ“ Training results directory: {train_results_dir}")
    
    # æ£€æŸ¥è®­ç»ƒç»“æœç›®å½•
    if not Path(train_results_dir).exists():
        print(f"âŒ Training results directory not found: {train_results_dir}")
        print("Please run the GAT training code first!")
        return None
    
    # åˆ›å»ºGATæµ‹è¯•å™¨
    tester = GATLayerComparisonTester(
        train_results_dir=train_results_dir,
        test_results_dir="4_Results_TS_RS",
        device=device
    )
    
    # æ‰§è¡Œæµ‹è¯•
    try:
        all_test_results = tester.test_all_datasets(test_paths)
        
        if all_test_results:
            print(f"\nğŸ‰ GAT testing completed successfully!")
            print(f"ğŸ“Š Tested configurations:")
            for distance, results in all_test_results.items():
                for gat_type, metrics in results.items():
                    gat_name = tester.gat_configs[gat_type]['name']
                    print(f"  âœ… {gat_name} on {distance}: AUC = {metrics['auc']:.3f}")
        else:
            print(f"\nâŒ No GAT models were successfully tested!")
        
        return all_test_results
        
    except Exception as e:
        print(f"âŒ Error during GAT testing: {e}")
        import traceback
        traceback.print_exc()
        return None

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================
if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    results = main_gat_testing(
        test_paths=[
            '3_Graph_Data/TS/TS_ESMC_4.0A.pkl',
            '3_Graph_Data/TS/TS_ESMC_8.0A.pkl',
            '3_Graph_Data/TS/TS_ESMC_12.0A.pkl'
        ],
        train_results_dir="4_Results_TR_RS"
    )
    
    if results:
        print("\nğŸ‰ GAT multi-model testing completed successfully!")
        print(f"ğŸ“Š Total configurations tested: {sum(len(r) for r in results.values())}")
    else:
        print("\nâŒ GAT testing failed!")